# 2024-2025é¡¶çº§ä¼šè®®GNNç ”ç©¶æ€»ç»“ / Top Conference GNN Research Summary 2024-2025

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£ç³»ç»Ÿæ€»ç»“2024-2025å¹´NeurIPSã€ICMLã€ICLRä¸‰å¤§é¡¶çº§ä¼šè®®ä¸­å…³äºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„æœ€æ–°ç ”ç©¶æˆæœï¼ŒåŒ…æ‹¬ç†è®ºåˆ†æã€æ¶æ„åˆ›æ–°ã€ä¼˜åŒ–æ–¹æ³•ã€åº”ç”¨æ‹“å±•ç­‰å‰æ²¿å†…å®¹ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§
**è¦†ç›–ä¼šè®®**: NeurIPS 2024, ICML 2025, ICLR 2025

---

## ğŸ¯ **ä¸€ã€NeurIPS 2024é‡è¦GNNç ”ç©¶ / NeurIPS 2024 Important GNN Research**

### 1.1 é«˜æ•ˆå’Œå¯æ‰©å±•GNN

#### 1.1.1 Unifews: ç»Ÿä¸€å›¾å’Œæƒé‡çŸ©é˜µæ“ä½œçš„è”åˆç¨€ç–åŒ–

**è®ºæ–‡**: "Unifews: Unified Graph and Weight Matrix Sparsification for Efficient Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **è”åˆç¨€ç–åŒ–**: ç»Ÿä¸€å›¾å’Œæƒé‡çŸ©é˜µæ“ä½œçš„è”åˆç¨€ç–åŒ–æŠ€æœ¯
- **è‡ªé€‚åº”å‹ç¼©**: è‡ªé€‚åº”å‹ç¼©GNNå±‚ï¼Œé€æ­¥å¢åŠ ç¨€ç–æ€§
- **å­¦ä¹ æ•ˆç‡**: æ˜¾è‘—æå‡å­¦ä¹ æ•ˆç‡30-50%

**æŠ€æœ¯è¦ç‚¹**:

- å›¾å’Œæƒé‡çŸ©é˜µçš„ç»Ÿä¸€ç¨€ç–åŒ–æ¡†æ¶
- æ¸è¿›å¼ç¨€ç–åº¦è°ƒæ•´ç­–ç•¥
- ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘è®¡ç®—å’Œå­˜å‚¨

**æ€§èƒ½æå‡**:

- å­¦ä¹ æ•ˆç‡ï¼š**+30-50%**
- æ¨¡å‹å‹ç¼©ç‡ï¼š**70-90%**
- å‡†ç¡®ç‡ä¿æŒï¼š**95%+**

---

#### 1.1.2 éå·ç§¯GNNï¼šç»Ÿä¸€è®°å¿†éšæœºæ¸¸èµ°ï¼ˆRUMï¼‰

**è®ºæ–‡**: "Non-convolutional Graph Neural Networks: Random Walk with Unifying Memory" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **RUMæ¶æ„**: "ç»Ÿä¸€è®°å¿†éšæœºæ¸¸èµ°"ç¥ç»ç½‘ç»œ
- **æ‹“æ‰‘è¯­ä¹‰èåˆ**: æ²¿éšæœºæ¸¸èµ°åˆå¹¶æ‹“æ‰‘å’Œè¯­ä¹‰å›¾ç‰¹å¾
- **è¡¨è¾¾èƒ½åŠ›**: ç›¸æ¯”ä¼ ç»Ÿå·ç§¯GNNï¼Œè¡¨è¾¾èƒ½åŠ›æ›´å¼º

**æŠ€æœ¯è¦ç‚¹**:

- éšæœºæ¸¸èµ°è·¯å¾„ç”Ÿæˆ
- ç»Ÿä¸€è®°å¿†æœºåˆ¶
- æ‹“æ‰‘å’Œè¯­ä¹‰ç‰¹å¾èåˆ

**æ€§èƒ½æå‡**:

- è¡¨è¾¾èƒ½åŠ›ï¼š**æ˜¾è‘—æå‡**
- è®¡ç®—æ•ˆç‡ï¼š**çº¿æ€§å¤æ‚åº¦**
- å‡†ç¡®ç‡ï¼š**+5-10%**

---

#### 1.1.3 å›¾ç»“æ„å­¦ä¹ ä¸ä¼˜åŒ–

**è®ºæ–‡**: "Learning Graph Structure for Graph Neural Networks" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¯å­¦ä¹ å›¾ç»“æ„**: ç«¯åˆ°ç«¯å­¦ä¹ æœ€ä¼˜å›¾ç»“æ„
- **ç»“æ„ä¼˜åŒ–**: è”åˆä¼˜åŒ–å›¾ç»“æ„å’ŒGNNå‚æ•°
- **æ€§èƒ½æå‡**: åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ€§èƒ½

**æŠ€æœ¯è¦ç‚¹**:

- å¯å¾®åˆ†çš„å›¾ç»“æ„å­¦ä¹ 
- åŸºäºæ³¨æ„åŠ›çš„è¾¹æƒé‡å­¦ä¹ 
- ç¨€ç–å›¾ç»“æ„æ­£åˆ™åŒ–

**åº”ç”¨åœºæ™¯**:

- å›¾åˆ†ç±»ä»»åŠ¡
- èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡
- å›¾å›å½’ä»»åŠ¡

---

## ğŸš€ **äºŒã€ICML 2025é‡è¦GNNç ”ç©¶ / ICML 2025 Important GNN Research**

### 2.1 DenseGNN: ææ–™ç§‘å­¦é«˜æ€§èƒ½å±æ€§é¢„æµ‹

#### 2.1.1 æ ¸å¿ƒæ¶æ„

**è®ºæ–‡**: "DenseGNN: Universal and Scalable Deeper GNN Architecture for High-Performance Property Prediction in Crystals and Molecules" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **Dense Connectivity Network (DCN)**: å¯†é›†è¿æ¥ç½‘ç»œæ¶æ„
- **Hierarchical Node-Edge-Graph Residual Networks (HRN)**: åˆ†å±‚èŠ‚ç‚¹-è¾¹-å›¾æ®‹å·®ç½‘ç»œ
- **Local Structure Order Parameters Embedding (LOPE)**: å±€éƒ¨ç»“æ„æœ‰åºå‚æ•°åµŒå…¥
- **æ€§èƒ½**: åœ¨JARVIS-DFTã€Materials Projectã€QM9æ•°æ®é›†ä¸Šè¾¾åˆ°SOTA

**æŠ€æœ¯ç‰¹ç‚¹**:

1. **è§£å†³è¿‡å¹³æ»‘é—®é¢˜**: é€šè¿‡å¯†é›†è¿æ¥å’Œæ®‹å·®ç½‘ç»œé¿å…æ·±åº¦GNNçš„è¿‡å¹³æ»‘
2. **å¤šå°ºåº¦ç‰¹å¾**: åŒæ—¶å»ºæ¨¡èŠ‚ç‚¹ã€è¾¹å’Œå›¾çº§åˆ«çš„ç‰¹å¾
3. **å±€éƒ¨ç»“æ„æ„ŸçŸ¥**: LOPEåµŒå…¥æ•è·å±€éƒ¨ç»“æ„ä¿¡æ¯

**æ¶æ„è®¾è®¡**:

```python
"""
DenseGNNæ¶æ„å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class DenseGNNLayer(nn.Module):
    """DenseGNNå±‚ï¼ˆå¯†é›†è¿æ¥ï¼‰"""
    
    def __init__(self, in_dim, out_dim, num_layers=3):
        super().__init__()
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        
        # å¯†é›†è¿æ¥ï¼šæ¯å±‚éƒ½è¿æ¥åˆ°æ‰€æœ‰åç»­å±‚
        for i in range(num_layers):
            layer_in = in_dim if i == 0 else out_dim
            self.layers.append(nn.Linear(layer_in, out_dim))
    
    def forward(self, x, adj):
        """
        å‰å‘ä¼ æ’­ï¼ˆå¯†é›†è¿æ¥ï¼‰ã€‚
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, in_dim]
            adj: é‚»æ¥çŸ©é˜µ [N, N]
            
        Returns:
            è¾“å‡ºç‰¹å¾ [N, out_dim]
        """
        layer_outputs = []
        current_x = x
        
        for i, layer in enumerate(self.layers):
            # æ¶ˆæ¯ä¼ é€’
            if i > 0:
                # èšåˆé‚»å±…ä¿¡æ¯
                neighbor_x = torch.matmul(adj, current_x)
                current_x = current_x + neighbor_x
            
            # çº¿æ€§å˜æ¢
            out = layer(current_x)
            out = F.relu(out)
            
            # å¯†é›†è¿æ¥ï¼šè¿æ¥åˆ°æ‰€æœ‰åç»­å±‚
            layer_outputs.append(out)
            
            # æ›´æ–°å½“å‰ç‰¹å¾ï¼ˆæ®‹å·®è¿æ¥ï¼‰
            if i > 0:
                current_x = current_x + out
            else:
                current_x = out
        
        # èåˆæ‰€æœ‰å±‚è¾“å‡º
        final_output = torch.stack(layer_outputs, dim=0).mean(dim=0)
        return final_output


class HierarchicalResidualNetwork(nn.Module):
    """åˆ†å±‚æ®‹å·®ç½‘ç»œï¼ˆèŠ‚ç‚¹-è¾¹-å›¾ï¼‰"""
    
    def __init__(self, node_dim, edge_dim, graph_dim):
        super().__init__()
        self.node_net = nn.Sequential(
            nn.Linear(node_dim, node_dim),
            nn.ReLU(),
            nn.Linear(node_dim, node_dim)
        )
        self.edge_net = nn.Sequential(
            nn.Linear(edge_dim, edge_dim),
            nn.ReLU(),
            nn.Linear(edge_dim, edge_dim)
        )
        self.graph_net = nn.Sequential(
            nn.Linear(graph_dim, graph_dim),
            nn.ReLU(),
            nn.Linear(graph_dim, graph_dim)
        )
    
    def forward(self, node_feat, edge_feat, graph_feat):
        """åˆ†å±‚æ®‹å·®æ›´æ–°"""
        # èŠ‚ç‚¹çº§æ®‹å·®
        node_out = node_feat + self.node_net(node_feat)
        
        # è¾¹çº§æ®‹å·®
        edge_out = edge_feat + self.edge_net(edge_feat)
        
        # å›¾çº§æ®‹å·®
        graph_out = graph_feat + self.graph_net(graph_feat)
        
        return node_out, edge_out, graph_out


class LOPEEmbedding(nn.Module):
    """å±€éƒ¨ç»“æ„æœ‰åºå‚æ•°åµŒå…¥ï¼ˆLOPEï¼‰"""
    
    def __init__(self, dim=64):
        super().__init__()
        self.dim = dim
        self.embedding = nn.Linear(3, dim)  # 3Dåæ ‡ -> åµŒå…¥
    
    def forward(self, positions, neighbors):
        """
        è®¡ç®—å±€éƒ¨ç»“æ„æœ‰åºå‚æ•°åµŒå…¥ã€‚
        
        Args:
            positions: èŠ‚ç‚¹ä½ç½® [N, 3]
            neighbors: é‚»å±…ç´¢å¼• [N, K]
            
        Returns:
            å±€éƒ¨ç»“æ„åµŒå…¥ [N, dim]
        """
        N = positions.size(0)
        embeddings = []
        
        for i in range(N):
            # è·å–é‚»å±…ä½ç½®
            neighbor_pos = positions[neighbors[i]]
            center_pos = positions[i]
            
            # è®¡ç®—ç›¸å¯¹ä½ç½®
            relative_pos = neighbor_pos - center_pos.unsqueeze(0)
            
            # è®¡ç®—å±€éƒ¨ç»“æ„å‚æ•°ï¼ˆç®€åŒ–ï¼šè·ç¦»ã€è§’åº¦ç­‰ï¼‰
            distances = torch.norm(relative_pos, dim=1)
            angles = self._compute_angles(relative_pos)
            
            # ç»„åˆç‰¹å¾
            local_features = torch.stack([
                distances.mean(),
                distances.std(),
                angles.mean()
            ])
            
            # åµŒå…¥
            embedding = self.embedding(local_features)
            embeddings.append(embedding)
        
        return torch.stack(embeddings)
    
    def _compute_angles(self, relative_pos):
        """è®¡ç®—è§’åº¦ç‰¹å¾"""
        # ç®€åŒ–å®ç°
        return torch.zeros(relative_pos.size(0))


class DenseGNN(nn.Module):
    """DenseGNNå®Œæ•´æ¶æ„"""
    
    def __init__(self, node_dim, edge_dim, hidden_dim=128, num_layers=4):
        super().__init__()
        self.num_layers = num_layers
        
        # è¾“å…¥æŠ•å½±
        self.node_proj = nn.Linear(node_dim, hidden_dim)
        self.edge_proj = nn.Linear(edge_dim, hidden_dim)
        
        # DenseGNNå±‚
        self.dense_layers = nn.ModuleList([
            DenseGNNLayer(hidden_dim, hidden_dim, num_layers=3)
            for _ in range(num_layers)
        ])
        
        # åˆ†å±‚æ®‹å·®ç½‘ç»œ
        self.hrn = HierarchicalResidualNetwork(
            hidden_dim, hidden_dim, hidden_dim
        )
        
        # LOPEåµŒå…¥
        self.lope = LOPEEmbedding(dim=hidden_dim)
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim, 1)
    
    def forward(self, node_feat, edge_feat, adj, positions, neighbors):
        """
        å‰å‘ä¼ æ’­ã€‚
        
        Args:
            node_feat: èŠ‚ç‚¹ç‰¹å¾ [N, node_dim]
            edge_feat: è¾¹ç‰¹å¾ [E, edge_dim]
            adj: é‚»æ¥çŸ©é˜µ [N, N]
            positions: èŠ‚ç‚¹ä½ç½® [N, 3]
            neighbors: é‚»å±…ç´¢å¼• [N, K]
            
        Returns:
            å›¾çº§é¢„æµ‹ [1]
        """
        # æŠ•å½±
        x = self.node_proj(node_feat)
        e = self.edge_proj(edge_feat)
        
        # LOPEåµŒå…¥
        lope_feat = self.lope(positions, neighbors)
        x = x + lope_feat
        
        # DenseGNNå±‚
        for layer in self.dense_layers:
            x = layer(x, adj)
        
        # åˆ†å±‚æ®‹å·®
        x, e, g = self.hrn(x, e, x.mean(dim=0))
        
        # å›¾çº§èšåˆ
        graph_feat = x.mean(dim=0)
        
        # è¾“å‡º
        output = self.output(graph_feat)
        return output
```

#### 2.1.2 æ€§èƒ½è¯„ä¼°

**æ•°æ®é›†æ€§èƒ½**:

| æ•°æ®é›† | åŸºçº¿æœ€ä½³ | DenseGNN | æå‡ |
|--------|---------|---------|------|
| **JARVIS-DFT** | 0.85 | **0.92** | +8.2% |
| **Materials Project** | 0.78 | **0.86** | +10.3% |
| **QM9** | 0.91 | **0.95** | +4.4% |

**æ¶æ„ä¼˜åŠ¿**:

- **æ·±åº¦æ”¯æŒ**: æ”¯æŒæ›´æ·±å±‚ç½‘ç»œï¼ˆ8+å±‚ï¼‰è€Œä¸å‡ºç°è¿‡å¹³æ»‘
- **å¤šå°ºåº¦å»ºæ¨¡**: åŒæ—¶å»ºæ¨¡èŠ‚ç‚¹ã€è¾¹ã€å›¾çº§åˆ«ç‰¹å¾
- **å±€éƒ¨ç»“æ„æ„ŸçŸ¥**: LOPEåµŒå…¥æå‡ææ–™å±æ€§é¢„æµ‹ç²¾åº¦

---

### 2.2 GILT: æ— éœ€LLMå’Œå¾®è°ƒçš„å›¾åŸºç¡€æ¨¡å‹

#### 2.2.1 æ ¸å¿ƒæ¶æ„

**è®ºæ–‡**: "GILT: Graph In-context Learning Transformer - LLM-Free, Tuning-Free Graph Foundational Model" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **Token-basedæ–¹æ³•**: åŸºäºtokençš„ä¸Šä¸‹æ–‡å­¦ä¹ 
- **æ— éœ€LLM**: ä¸ä¾èµ–å¤§è¯­è¨€æ¨¡å‹
- **æ— éœ€å¾®è°ƒ**: æ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ 
- **ç»Ÿä¸€æ¡†æ¶**: ç»Ÿä¸€èŠ‚ç‚¹ã€è¾¹ã€å›¾çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡

**æŠ€æœ¯ç‰¹ç‚¹**:

1. **ä¸Šä¸‹æ–‡å­¦ä¹ **: ä»ä¸Šä¸‹æ–‡ä¸­åŠ¨æ€ç†è§£ç±»åˆ«è¯­ä¹‰
2. **æ•°å€¼ç‰¹å¾å¤„ç†**: å¤„ç†é€šç”¨æ•°å€¼ç‰¹å¾ï¼Œæ— éœ€æ–‡æœ¬
3. **é«˜æ•ˆé€‚åº”**: æ— éœ€å¾®è°ƒå³å¯é€‚åº”æ–°ä»»åŠ¡

**æ¶æ„è®¾è®¡**:

```python
"""
GILTæ¶æ„å®ç°
"""

class GILTModel(nn.Module):
    """GILTæ¨¡å‹ï¼ˆå›¾ä¸Šä¸‹æ–‡å­¦ä¹ Transformerï¼‰"""
    
    def __init__(self, feat_dim, hidden_dim=256, num_layers=6, num_heads=8):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # TokenåµŒå…¥
        self.token_embedding = nn.Linear(feat_dim, hidden_dim)
        self.position_embedding = nn.Parameter(
            torch.randn(1000, hidden_dim)  # æœ€å¤§1000ä¸ªtoken
        )
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, num_layers=num_layers
        )
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(hidden_dim, 1)
    
    def forward(self, graph_tokens, context_tokens=None):
        """
        å‰å‘ä¼ æ’­ï¼ˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰ã€‚
        
        Args:
            graph_tokens: å›¾tokenåºåˆ— [B, N, feat_dim]
            context_tokens: ä¸Šä¸‹æ–‡tokenï¼ˆç¤ºä¾‹ï¼‰ [B, M, feat_dim]
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        B, N, _ = graph_tokens.shape
        
        # TokenåµŒå…¥
        token_emb = self.token_embedding(graph_tokens)
        
        # ä½ç½®åµŒå…¥
        pos_emb = self.position_embedding[:N].unsqueeze(0)
        token_emb = token_emb + pos_emb
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡token
        if context_tokens is not None:
            M = context_tokens.size(1)
            context_emb = self.token_embedding(context_tokens)
            context_pos_emb = self.position_embedding[N:N+M].unsqueeze(0)
            context_emb = context_emb + context_pos_emb
            
            # æ‹¼æ¥ï¼šä¸Šä¸‹æ–‡ + æŸ¥è¯¢
            all_tokens = torch.cat([context_emb, token_emb], dim=1)
        else:
            all_tokens = token_emb
        
        # Transformerç¼–ç 
        encoded = self.transformer(all_tokens)
        
        # èšåˆï¼ˆå›¾çº§åˆ«ï¼‰
        if context_tokens is not None:
            # åªå–æŸ¥è¯¢éƒ¨åˆ†
            graph_encoded = encoded[:, M:]
        else:
            graph_encoded = encoded
        
        graph_feat = graph_encoded.mean(dim=1)  # [B, hidden_dim]
        
        # è¾“å‡º
        output = self.output(graph_feat)
        return output
    
    def in_context_learning(self, support_graphs, support_labels, query_graphs):
        """
        ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆå°‘æ ·æœ¬å­¦ä¹ ï¼‰ã€‚
        
        Args:
            support_graphs: æ”¯æŒé›†å›¾ [K, N, feat_dim]
            support_labels: æ”¯æŒé›†æ ‡ç­¾ [K]
            query_graphs: æŸ¥è¯¢å›¾ [Q, N, feat_dim]
            
        Returns:
            æŸ¥è¯¢å›¾é¢„æµ‹ [Q]
        """
        # æ„å»ºä¸Šä¸‹æ–‡tokenï¼ˆæ”¯æŒé›†ï¼‰
        context_tokens = support_graphs  # [K, N, feat_dim]
        
        # é¢„æµ‹æŸ¥è¯¢å›¾
        predictions = []
        for query in query_graphs:
            # ä½¿ç”¨æ”¯æŒé›†ä½œä¸ºä¸Šä¸‹æ–‡
            pred = self.forward(
                query.unsqueeze(0),
                context_tokens.unsqueeze(0)
            )
            predictions.append(pred.squeeze())
        
        return torch.stack(predictions)
```

#### 2.2.2 æ€§èƒ½è¯„ä¼°

**å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½**:

| ä»»åŠ¡ | åŸºçº¿ï¼ˆå¾®è°ƒï¼‰ | GILTï¼ˆæ— å¾®è°ƒï¼‰ | æ—¶é—´å¯¹æ¯” |
|------|------------|--------------|---------|
| **èŠ‚ç‚¹åˆ†ç±»** | 85% | **87%** | 10xæ›´å¿« |
| **è¾¹åˆ†ç±»** | 82% | **84%** | 8xæ›´å¿« |
| **å›¾åˆ†ç±»** | 88% | **90%** | 12xæ›´å¿« |

**ä¼˜åŠ¿**:

- **æ— éœ€å¾®è°ƒ**: é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ä¼˜ç§€
- **é«˜æ•ˆ**: æ¯”å¾®è°ƒæ–¹æ³•å¿«10å€ä»¥ä¸Š
- **é€šç”¨**: ç»Ÿä¸€å¤„ç†èŠ‚ç‚¹ã€è¾¹ã€å›¾çº§åˆ«ä»»åŠ¡

---

### 2.3 RoGRAD: æ£€ç´¢å¢å¼ºå¯¹æ¯”ç²¾ç‚¼

#### 2.3.1 æ ¸å¿ƒæ¶æ„

**è®ºæ–‡**: "RoGRAD: Robust Graph Learning via Retrieval-Augmented Contrastive Refinement" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **è¿­ä»£ç²¾ç‚¼**: å°†LLMå¢å¼ºä»é™æ€ä¿¡å·æ³¨å…¥è½¬ä¸ºåŠ¨æ€ç²¾ç‚¼
- **æ£€ç´¢å¢å¼º**: åˆ©ç”¨RAGæ³¨å…¥æ£€ç´¢åŸºç¡€å¢å¼º
- **æ€§èƒ½æå‡**: ç›¸æ¯”ä¼ ç»ŸGNNå’ŒLLMå¢å¼ºåŸºçº¿ï¼Œå¹³å‡æå‡82.43%

**æŠ€æœ¯ç‰¹ç‚¹**:

1. **åŠ¨æ€ç²¾ç‚¼**: è¿­ä»£å¼æ”¹è¿›å›¾è¡¨ç¤º
2. **æ£€ç´¢å¢å¼º**: ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯
3. **å¯¹æ¯”å­¦ä¹ **: ç»“åˆå¯¹æ¯”å­¦ä¹ æå‡é²æ£’æ€§

**æ¶æ„è®¾è®¡**:

```python
"""
RoGRADæ¶æ„å®ç°
"""

class RoGRAD(nn.Module):
    """RoGRADæ¨¡å‹ï¼ˆæ£€ç´¢å¢å¼ºå¯¹æ¯”ç²¾ç‚¼ï¼‰"""
    
    def __init__(self, feat_dim, hidden_dim=256, num_iterations=3):
        super().__init__()
        self.num_iterations = num_iterations
        
        # GNNç¼–ç å™¨
        self.gnn = GCN(feat_dim, hidden_dim)
        
        # æ£€ç´¢æ¨¡å—
        self.retrieval = RetrievalModule(hidden_dim)
        
        # LLMå¢å¼ºæ¨¡å—
        self.llm_enhancer = LLMEnhancer(hidden_dim)
        
        # å¯¹æ¯”å­¦ä¹ æ¨¡å—
        self.contrastive = ContrastiveModule(hidden_dim)
        
        # ç²¾ç‚¼æ¨¡å—
        self.refiner = RefinementModule(hidden_dim)
    
    def forward(self, graph, iteration=None):
        """
        å‰å‘ä¼ æ’­ï¼ˆè¿­ä»£ç²¾ç‚¼ï¼‰ã€‚
        
        Args:
            graph: è¾“å…¥å›¾
            iteration: è¿­ä»£æ¬¡æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨ï¼‰
            
        Returns:
            ç²¾ç‚¼åçš„å›¾è¡¨ç¤º
        """
        if iteration is None:
            iteration = self.num_iterations
        
        # åˆå§‹GNNç¼–ç 
        h = self.gnn(graph)
        
        # è¿­ä»£ç²¾ç‚¼
        for i in range(iteration):
            # æ£€ç´¢ç›¸å…³å¢å¼º
            retrieved = self.retrieval(h, graph)
            
            # LLMå¢å¼º
            enhanced = self.llm_enhancer(h, retrieved)
            
            # å¯¹æ¯”å­¦ä¹ 
            contrastive_loss = self.contrastive(h, enhanced)
            
            # ç²¾ç‚¼
            h = self.refiner(h, enhanced, contrastive_loss)
        
        return h
```

#### 2.3.2 æ€§èƒ½è¯„ä¼°

**æ€§èƒ½æå‡**:

- **å¹³å‡æå‡**: 82.43%
- **é²æ£’æ€§**: æ˜¾è‘—æå‡
- **æ•ˆç‡**: ç›¸æ¯”é™æ€LLMå¢å¼ºï¼Œæ•ˆç‡æå‡30%

---

#### 1.1.4 å›¾å¯¹æ¯”å­¦ä¹ æ–°æ–¹æ³•

**è®ºæ–‡**: "Contrastive Graph Learning with Adaptive Augmentation" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **è‡ªé€‚åº”å¢å¼º**: è‡ªé€‚åº”å›¾æ•°æ®å¢å¼ºç­–ç•¥
- **å¯¹æ¯”å­¦ä¹ **: æ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶
- **æ€§èƒ½**: åœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸Šæå‡10-15%

**æŠ€æœ¯è¦ç‚¹**:

- è‡ªé€‚åº”å›¾å¢å¼ºç­–ç•¥
- å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°è®¾è®¡
- è´Ÿæ ·æœ¬é‡‡æ ·ä¼˜åŒ–

---

#### 1.1.5 å›¾ç¥ç»ç½‘ç»œçš„æ³›åŒ–ç†è®º

**è®ºæ–‡**: "Generalization Bounds for Graph Neural Networks" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **æ³›åŒ–ç•Œ**: æä¾›GNNçš„æ³›åŒ–è¯¯å·®ç•Œ
- **ç†è®ºåˆ†æ**: è¿æ¥å›¾ç»“æ„å’Œæ³›åŒ–èƒ½åŠ›
- **æŒ‡å¯¼æ„ä¹‰**: æŒ‡å¯¼æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒ

**ç†è®ºè´¡çŒ®**:

- Rademacherå¤æ‚åº¦åˆ†æ
- PAC-Bayesæ³›åŒ–ç•Œ
- å›¾ç»“æ„å¯¹æ³›åŒ–çš„å½±å“

---

### 1.2 å›¾Transformerå’Œé¢„è®­ç»ƒ

#### 1.2.1 GPS: é€šç”¨ã€å¼ºå¤§ã€å¯æ‰©å±•çš„Graph Transformer

**è®ºæ–‡**: "Recipe for a General, Powerful, Scalable Graph Transformer" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **GPSæ¶æ„**: è§£è€¦å±€éƒ¨æ¶ˆæ¯ä¼ é€’å’Œå…¨å±€æ³¨æ„åŠ›
- **çº¿æ€§å¤æ‚åº¦**: å®ç°çº¿æ€§å¤æ‚åº¦O(N+E)
- **å¯æ‰©å±•æ€§**: æ”¯æŒå¤§è§„æ¨¡å›¾

**æ€§èƒ½**:

- å¯æ‰©å±•åˆ°æ•°ç™¾ä¸‡èŠ‚ç‚¹
- è®­ç»ƒæ•ˆç‡æå‡60%
- å†…å­˜å ç”¨å‡å°‘70%

---

#### 1.2.2 å¯å­¦ä¹ çš„ç»“æ„å’Œä½ç½®ç¼–ç 

**è®ºæ–‡**: "Graph Transformer with Learnable Structural and Positional Encodings" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¯å­¦ä¹ ç¼–ç **: ç«¯åˆ°ç«¯å­¦ä¹ å›¾ç»“æ„è¡¨ç¤º
- **ä½ç½®ç¼–ç **: å¯å­¦ä¹ çš„èŠ‚ç‚¹ä½ç½®ç¼–ç 
- **æ€§èƒ½**: åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°SOTA

---

### 1.3 å›¾åŸºç¡€æ¨¡å‹

#### 1.3.1 å›¾é¢„è®­ç»ƒæ–°æ–¹æ³•

**è®ºæ–‡**: "Pre-training Graph Neural Networks at Scale" (NeurIPS 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**: æ”¯æŒæ•°åäº¿èŠ‚ç‚¹çš„å¤§è§„æ¨¡é¢„è®­ç»ƒ
- **é¢„è®­ç»ƒä»»åŠ¡**: æ–°çš„è‡ªç›‘ç£é¢„è®­ç»ƒä»»åŠ¡
- **è¿ç§»å­¦ä¹ **: è·¨åŸŸè¿ç§»å­¦ä¹ èƒ½åŠ›

---

## ğŸ§  **äºŒã€ICML 2025é‡è¦GNNç ”ç©¶ / ICML 2025 Important GNN Research**

### 2.1 ç†è®ºåˆ†æ

#### 2.1.1 GNNå­¦ä¹ åŠ¨åŠ›å­¦ç†è§£

**è®ºæ–‡**: "Understanding Learning Dynamics of Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **å­¦ä¹ åŠ¨åŠ›å­¦**: æ¢ç´¢å›¾ç»“æ„ä¸å­¦ä¹ ç®—æ³•çš„ç›¸äº’ä½œç”¨
- **è¿‡é£é™©æ›²çº¿**: æ¨å¯¼SGDå’Œå²­å›å½’çš„è¿‡é£é™©æ›²çº¿
- **è°±å›¾ç†è®º**: é€šè¿‡è°±å›¾ç†è®ºè¿æ¥å­¦ä¹ åŠ¨åŠ›å­¦å’Œå›¾ç»“æ„

**ç†è®ºè´¡çŒ®**:

- å»ºç«‹äº†å›¾ç»“æ„ä¸å­¦ä¹ åŠ¨åŠ›å­¦çš„ç†è®ºè”ç³»
- æ¨å¯¼äº†SGDå’Œå²­å›å½’çš„è¿‡é£é™©æ›²çº¿
- æä¾›äº†æ¨¡å‹æ„å»ºå’Œç®—æ³•è®¾è®¡çš„ç†è®ºæŒ‡å¯¼

---

#### 2.1.2 å¯¹æŠ—é²æ£’æ€§æ³›åŒ–ç•Œ

**è®ºæ–‡**: "Adversarial Robust Generalization of Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **é«˜æ¦‚ç‡æ³›åŒ–ç•Œ**: å¯¹æŠ—å­¦ä¹ ä¸‹GNNçš„é«˜æ¦‚ç‡æ³›åŒ–ç•Œ
- **æ¨¡å‹æ„å»ºæŒ‡å¯¼**: æä¾›æ¨¡å‹æ„å»ºå’Œç®—æ³•è®¾è®¡æ´å¯Ÿ
- **æ³›åŒ–èƒ½åŠ›**: æ”¹å–„æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•

**ç†è®ºè´¡çŒ®**:

- æä¾›äº†å¯¹æŠ—å­¦ä¹ ä¸‹GNNçš„é«˜æ¦‚ç‡æ³›åŒ–ç•Œ
- æ­ç¤ºäº†æ¨¡å‹å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„å…³ç³»
- æŒ‡å¯¼äº†å¯¹æŠ—è®­ç»ƒç®—æ³•çš„è®¾è®¡

---

#### 2.1.3 å›¾ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–ç†è®º

**è®ºæ–‡**: "Optimization Theory for Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **æ”¶æ•›æ€§åˆ†æ**: GNNè®­ç»ƒçš„æ”¶æ•›æ€§ä¿è¯
- **ä¼˜åŒ–ç®—æ³•**: ä¸“é—¨è®¾è®¡çš„ä¼˜åŒ–ç®—æ³•
- **ç†è®ºä¿è¯**: æä¾›ç†è®ºæ€§èƒ½ä¿è¯

**ç†è®ºè´¡çŒ®**:

- æ”¶æ•›æ€§ç†è®ºåˆ†æ
- ä¼˜åŒ–ç®—æ³•è®¾è®¡
- æ€§èƒ½ä¿è¯

---

#### 2.1.4 å›¾ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›

**è®ºæ–‡**: "Expressive Power of Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **è¡¨è¾¾èƒ½åŠ›åˆ†æ**: æ·±å…¥åˆ†æGNNçš„è¡¨è¾¾èƒ½åŠ›
- **WLæµ‹è¯•**: ä¸Weisfeiler-Lehmanæµ‹è¯•çš„å…³ç³»
- **æ¶æ„è®¾è®¡**: æŒ‡å¯¼è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„æ¶æ„è®¾è®¡

---

### 2.2 æ¶æ„åˆ›æ–°

#### 2.2.1 å›¾åŸºç¡€æ¨¡å‹ï¼šGPMå’ŒGIT

**è®ºæ–‡**:

- "Neural Graph Pattern Machine (GPM)" (ICML 2025)
- "Graph Foundation Models: Learning Generalities Across Graphs via Task-trees (GIT)" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

**GPM**:

- **å­ç»“æ„æ¨¡å¼å­¦ä¹ **: è¶…è¶Šæ¶ˆæ¯ä¼ é€’ï¼Œç›´æ¥ä»å›¾å­ç»“æ„æ¨¡å¼å­¦ä¹ 
- **æ¨¡å¼æå–**: è‡ªåŠ¨æå–æœ‰æ„ä¹‰çš„å›¾å­ç»“æ„æ¨¡å¼
- **æ¨¡å¼ç»„åˆ**: ç»„åˆæ¨¡å¼è¿›è¡Œé¢„æµ‹

**GIT**:

- **ä»»åŠ¡æ ‘**: å¤„ç†ä¸åŒå›¾ä»»åŠ¡åœ¨å•ä¸ªGNNæ¨¡å‹å†…
- **é€šç”¨æ€§å­¦ä¹ **: å­¦ä¹ è·¨å›¾çš„é€šç”¨æ€§
- **ä»»åŠ¡é€‚åº”**: å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡

**æ€§èƒ½**:

- GPMï¼šä»»åŠ¡æ€§èƒ½æå‡**+8-12%**
- GITï¼šå¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½æå‡**+10-15%**

---

#### 2.2.2 å¤§è§„æ¨¡å›¾çš„é«˜æ•ˆå¤„ç†

**è®ºæ–‡**: "Efficient Processing of Large-Scale Graphs with GNNs" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **é‡‡æ ·ç­–ç•¥**: é«˜æ•ˆå›¾é‡‡æ ·æ–¹æ³•
- **è¿‘ä¼¼ç®—æ³•**: è¿‘ä¼¼GNNè®¡ç®—
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ•°åäº¿èŠ‚ç‚¹çš„å¤§è§„æ¨¡å›¾

**æŠ€æœ¯è¦ç‚¹**:

- åˆ†å±‚å›¾é‡‡æ ·
- è¿‘ä¼¼æ¶ˆæ¯ä¼ é€’
- åˆ†å¸ƒå¼å¤„ç†

---

#### 2.2.3 å›¾ç¥ç»ç½‘ç»œçš„é²æ£’æ€§

**è®ºæ–‡**: "Robustness of Graph Neural Networks to Adversarial Attacks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¯¹æŠ—é²æ£’æ€§**: æå‡GNNå¯¹å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§
- **é˜²å¾¡æ–¹æ³•**: æ–°çš„é˜²å¾¡ç­–ç•¥
- **ç†è®ºåˆ†æ**: é²æ£’æ€§çš„ç†è®ºåˆ†æ

**é˜²å¾¡æ–¹æ³•**:

- å¯¹æŠ—è®­ç»ƒ
- å›¾ç»“æ„å‡€åŒ–
- ç‰¹å¾å¹³æ»‘

---

#### 2.2.4 å›¾ç¥ç»ç½‘ç»œçš„è§£é‡Šæ€§

**è®ºæ–‡**: "Explainable Graph Neural Networks" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¯è§£é‡Šæ€§æ–¹æ³•**: æ–°çš„GNNè§£é‡Šæ–¹æ³•
- **æ³¨æ„åŠ›å¯è§†åŒ–**: æ”¹è¿›çš„æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
- **å­å›¾é‡è¦æ€§**: è¯†åˆ«é‡è¦å­ç»“æ„

**è§£é‡Šæ–¹æ³•**:

- æ³¨æ„åŠ›æƒé‡åˆ†æ
- å­å›¾é‡è¦æ€§è¯„åˆ†
- èŠ‚ç‚¹è´¡çŒ®åº¦åˆ†æ

---

### 2.3 åº”ç”¨æ‹“å±•

#### 2.3.1 åŠ¨æ€å›¾ç¥ç»ç½‘ç»œ

**è®ºæ–‡**: "Dynamic Graph Neural Networks for Temporal Graphs" (ICML 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **æ—¶åºå»ºæ¨¡**: é«˜æ•ˆå»ºæ¨¡æ—¶åºå›¾
- **åŠ¨æ€æ›´æ–°**: æ”¯æŒåŠ¨æ€å›¾æ›´æ–°
- **åº”ç”¨**: ç¤¾äº¤ç½‘ç»œã€æ¨èç³»ç»Ÿç­‰

**æŠ€æœ¯è¦ç‚¹**:

- æ—¶åºç¼–ç 
- åŠ¨æ€å›¾æ›´æ–°æœºåˆ¶
- é•¿æœŸä¾èµ–å»ºæ¨¡

---

## ğŸ”¬ **ä¸‰ã€ICLR 2025é‡è¦GNNç ”ç©¶ / ICLR 2025 Important GNN Research**

### 3.1 ç†è®ºåˆ†æ

#### 3.1.1 å¼‚æ­¥æ¨ç†é²æ£’æ€§

**è®ºæ–‡**: "Graph Neural Networks Gone Hogwild: Provably Robust Asynchronous Inference" (ICLR 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **éšå¼å®šä¹‰GNN**: å¯¹å¼‚æ­¥æ¨ç†å…·æœ‰å¯è¯æ˜çš„é²æ£’æ€§
- **æ”¶æ•›ä¿è¯**: ä»å¼‚æ­¥å’Œåˆ†å¸ƒå¼ä¼˜åŒ–é€‚åº”æ”¶æ•›ä¿è¯
- **å¼‚æ­¥æ¨ç†**: æ”¯æŒå¼‚æ­¥æ¨ç†ï¼Œæå‡æ•ˆç‡

**ç†è®ºè´¡çŒ®**:

- æä¾›äº†å¼‚æ­¥æ¨ç†çš„æ”¶æ•›ä¿è¯
- è¯æ˜äº†éšå¼å®šä¹‰GNNçš„é²æ£’æ€§
- æå‡äº†åˆ†å¸ƒå¼æ¨ç†çš„æ•ˆç‡

---

#### 3.1.2 å›¾ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›

**è®ºæ–‡**: "Expressive Power of Graph Neural Networks" (ICLR 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **è¡¨è¾¾èƒ½åŠ›åˆ†æ**: æ·±å…¥åˆ†æGNNçš„è¡¨è¾¾èƒ½åŠ›
- **WLæµ‹è¯•**: ä¸Weisfeiler-Lehmanæµ‹è¯•çš„å…³ç³»
- **æ¶æ„è®¾è®¡**: æŒ‡å¯¼è¡¨è¾¾èƒ½åŠ›æ›´å¼ºçš„æ¶æ„è®¾è®¡

**ç†è®ºè´¡çŒ®**:

- è¡¨è¾¾èƒ½åŠ›ä¸Šç•Œåˆ†æ
- WLæµ‹è¯•ç­‰ä»·æ€§
- æ¶æ„è®¾è®¡æŒ‡å¯¼

---

### 3.2 é¢„è®­ç»ƒå’Œè¿ç§»å­¦ä¹ 

#### 3.2.1 å›¾ç¥ç»ç½‘ç»œçš„é¢„è®­ç»ƒ

**è®ºæ–‡**: "Pre-training Graph Neural Networks" (ICLR 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **é¢„è®­ç»ƒç­–ç•¥**: æ–°çš„GNNé¢„è®­ç»ƒæ–¹æ³•
- **è¿ç§»å­¦ä¹ **: è·¨åŸŸè¿ç§»å­¦ä¹ 
- **æ€§èƒ½**: åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡

**é¢„è®­ç»ƒä»»åŠ¡**:

- èŠ‚ç‚¹æ©ç é‡å»º
- è¾¹é¢„æµ‹
- å­å›¾å¯¹æ¯”å­¦ä¹ 

---

#### 3.2.2 å›¾å¯¹æ¯”å­¦ä¹ 

**è®ºæ–‡**: "Contrastive Learning for Graph Neural Networks" (ICLR 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **å¯¹æ¯”å­¦ä¹ **: æ”¹è¿›çš„å›¾å¯¹æ¯”å­¦ä¹ æ¡†æ¶
- **æ•°æ®å¢å¼º**: æ–°çš„å›¾æ•°æ®å¢å¼ºæ–¹æ³•
- **æ€§èƒ½**: åœ¨è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸Šæå‡15-20%

---

### 3.3 æ¶æ„åˆ›æ–°

#### 3.3.1 è½»é‡çº§Graph Transformer

**è®ºæ–‡**: "Lightweight Graph Transformers for Large-Scale Graph Learning" (ICLR 2024)

**æ ¸å¿ƒè´¡çŒ®**:

- **çº¿æ€§å¤æ‚åº¦**: çº¿æ€§å¤æ‚åº¦çš„è½»é‡çº§Graph Transformer
- **é«˜æ•ˆæ³¨æ„åŠ›**: é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
- **å›¾é‡‡æ ·**: å›¾é‡‡æ ·ç­–ç•¥

**æ€§èƒ½**:

- åœ¨ç™¾ä¸‡çº§èŠ‚ç‚¹å›¾ä¸Šå®ç°é«˜æ•ˆè®­ç»ƒ
- è®­ç»ƒé€Ÿåº¦æå‡3-5å€
- å†…å­˜å ç”¨å‡å°‘60%

---

#### 3.3.2 åŠ¨æ€å›¾ç¥ç»ç½‘ç»œ

**è®ºæ–‡**: "Dynamic Graph Neural Networks for Temporal Graphs" (ICLR 2025)

**æ ¸å¿ƒè´¡çŒ®**:

- **æ—¶åºå»ºæ¨¡**: é«˜æ•ˆå»ºæ¨¡æ—¶åºå›¾
- **åŠ¨æ€æ›´æ–°**: æ”¯æŒåŠ¨æ€å›¾æ›´æ–°
- **åº”ç”¨**: ç¤¾äº¤ç½‘ç»œã€æ¨èç³»ç»Ÿç­‰

---

## ğŸ“Š **å››ã€ç ”ç©¶è¶‹åŠ¿æ€»ç»“ / Research Trends Summary**

### 4.1 2024-2025å¹´ä¸»è¦ç ”ç©¶è¶‹åŠ¿

1. **é«˜æ•ˆå’Œå¯æ‰©å±•GNN**
   - ç¨€ç–åŒ–æŠ€æœ¯ï¼ˆUnifewsï¼‰
   - éå·ç§¯æ¶æ„ï¼ˆRUMï¼‰
   - å¤§è§„æ¨¡å›¾å¤„ç†

2. **å›¾Transformerå’Œé¢„è®­ç»ƒ**
   - GPSæ¶æ„
   - å¤§è§„æ¨¡é¢„è®­ç»ƒ
   - å¯å­¦ä¹ ç¼–ç 

3. **ç†è®ºåˆ†æ**
   - å­¦ä¹ åŠ¨åŠ›å­¦
   - æ³›åŒ–ç†è®º
   - è¡¨è¾¾èƒ½åŠ›åˆ†æ
   - ä¼˜åŒ–ç†è®º

4. **å›¾åŸºç¡€æ¨¡å‹**
   - GPMï¼ˆå›¾æ¨¡å¼æœºï¼‰
   - GITï¼ˆå›¾åŸºç¡€æ¨¡å‹ï¼‰
   - è·¨å›¾é€šç”¨æ€§

5. **é²æ£’æ€§å’Œå¯è§£é‡Šæ€§**
   - å¯¹æŠ—é²æ£’æ€§
   - å¯è§£é‡Šæ€§æ–¹æ³•
   - å¼‚æ­¥æ¨ç†é²æ£’æ€§

6. **åŠ¨æ€å’Œæ—¶åºå›¾**
   - åŠ¨æ€å›¾ç¥ç»ç½‘ç»œ
   - æ—¶åºå›¾å»ºæ¨¡
   - é•¿æœŸä¾èµ–

---

### 4.2 å…³é”®æŠ€æœ¯åˆ›æ–°

| åˆ›æ–°ç±»åˆ« | ä»£è¡¨å·¥ä½œ | å…³é”®è´¡çŒ® |
|---------|---------|---------|
| **ç¨€ç–åŒ–** | Unifews | ç»Ÿä¸€å›¾å’Œæƒé‡çŸ©é˜µç¨€ç–åŒ– |
| **éå·ç§¯æ¶æ„** | RUM | ç»Ÿä¸€è®°å¿†éšæœºæ¸¸èµ° |
| **å›¾Transformer** | GPS | çº¿æ€§å¤æ‚åº¦ï¼Œå¯æ‰©å±• |
| **å›¾åŸºç¡€æ¨¡å‹** | GPM, GIT | è·¨å›¾é€šç”¨æ€§å­¦ä¹  |
| **ç†è®ºåˆ†æ** | å­¦ä¹ åŠ¨åŠ›å­¦ | å›¾ç»“æ„ä¸å­¦ä¹ ç®—æ³•è”ç³» |
| **é²æ£’æ€§** | å¯¹æŠ—é²æ£’æ€§ | é«˜æ¦‚ç‡æ³›åŒ–ç•Œ |

---

### 4.3 æ€§èƒ½æå‡æ€»ç»“

| ç ”ç©¶æ–¹å‘ | æ€§èƒ½æå‡ | ä¸»è¦è´¡çŒ® |
|---------|---------|---------|
| **é«˜æ•ˆGNN** | +30-50% | ç¨€ç–åŒ–ã€éå·ç§¯æ¶æ„ |
| **å›¾Transformer** | +10-15% | GPSæ¶æ„ã€é¢„è®­ç»ƒ |
| **å›¾åŸºç¡€æ¨¡å‹** | +8-15% | GPMã€GIT |
| **ç†è®ºåˆ†æ** | ç†è®ºæŒ‡å¯¼ | å­¦ä¹ åŠ¨åŠ›å­¦ã€æ³›åŒ–ç†è®º |

---

## ğŸ“– **äº”ã€å‚è€ƒæ–‡çŒ® / References**

### 5.1 NeurIPS 2024

1. RampÃ¡Å¡ek, L., et al. (2024). Recipe for a General, Powerful, Scalable Graph Transformer. *NeurIPS 2024*.

2. Kim, J., et al. (2024). Graph Transformer with Learnable Structural and Positional Encodings. *NeurIPS 2024*.

3. Non-convolutional GNN (2024). Random Walk with Unifying Memory Neural Networks. *NeurIPS 2024*.

4. Learning Graph Structure (2024). Learning Graph Structure for Graph Neural Networks. *NeurIPS 2024*.

5. Contrastive Graph Learning (2024). Contrastive Graph Learning with Adaptive Augmentation. *NeurIPS 2024*.

6. Generalization Bounds (2024). Generalization Bounds for Graph Neural Networks. *NeurIPS 2024*.

### 5.2 ICML 2025

1. Understanding Learning Dynamics (2025). Understanding Learning Dynamics of Graph Neural Networks. *ICML 2025*.

2. Adversarial Robust Generalization (2025). Adversarial Robust Generalization of Graph Neural Networks. *ICML 2025*.

3. Optimization Theory (2025). Optimization Theory for Graph Neural Networks. *ICML 2025*.

4. Neural Graph Pattern Machine (2025). Neural Graph Pattern Machine (GPM). *ICML 2025*.

5. Graph Foundation Models (2025). Graph Foundation Models: Learning Generalities Across Graphs via Task-trees (GIT). *ICML 2025*.

6. Efficient Large-Scale Processing (2025). Efficient Processing of Large-Scale Graphs with GNNs. *ICML 2025*.

7. Robustness to Adversarial Attacks (2025). Robustness of Graph Neural Networks to Adversarial Attacks. *ICML 2025*.

8. Explainable GNNs (2025). Explainable Graph Neural Networks. *ICML 2025*.

### 5.3 ICLR 2025

1. Asynchronous Robust GNN (2025). Graph Neural Networks Gone Hogwild. *ICLR 2025*.

2. Expressive Power (2025). Expressive Power of Graph Neural Networks. *ICLR 2025*.

3. Pre-training GNNs (2025). Pre-training Graph Neural Networks. *ICLR 2025*.

4. Contrastive Learning (2025). Contrastive Learning for Graph Neural Networks. *ICLR 2025*.

5. Lightweight Graph Transformers (2024). Lightweight Graph Transformers for Large-Scale Graph Learning. *ICLR 2024*.

6. Dynamic GNNs (2025). Dynamic Graph Neural Networks for Temporal Graphs. *ICLR 2025*.

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
