# Graph Transformerä¸“é¢˜ - 2024-2025æœ€æ–°ç ”ç©¶ / Graph Transformer Special Topic - Latest Research 2024-2025

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£ç³»ç»Ÿæ¢³ç†Graph Transformeråœ¨2024-2025å¹´çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼ŒåŒ…æ‹¬æ¶æ„åˆ›æ–°ã€æ€§èƒ½ä¼˜åŒ–ã€åº”ç”¨æ‹“å±•ç­‰å‰æ²¿å†…å®¹ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§
**æœ€æ–°ç ”ç©¶è¦†ç›–**: 2024-2025å¹´é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠ

---

## ğŸ¯ **ä¸€ã€Graph TransformeråŸºç¡€å›é¡¾ / Graph Transformer Fundamentals Review**

### 1.1 ä¼ ç»Ÿå›¾ç¥ç»ç½‘ç»œ vs Graph Transformer

#### ä¼ ç»ŸGNNçš„å±€é™æ€§

**é—®é¢˜1: æ„Ÿå—é‡å—é™**

- ä¼ ç»ŸGNNï¼ˆGCN, GraphSAGE, GATï¼‰é€šè¿‡æ¶ˆæ¯ä¼ é€’æœºåˆ¶èšåˆé‚»å±…ä¿¡æ¯
- éœ€è¦å¤šå±‚å †å æ‰èƒ½è·å¾—æ›´å¤§æ„Ÿå—é‡
- æ·±åº¦å¢åŠ å¯¼è‡´è¿‡å¹³æ»‘ï¼ˆover-smoothingï¼‰é—®é¢˜

**é—®é¢˜2: è¡¨è¾¾èƒ½åŠ›æœ‰é™**

- 1-WLæµ‹è¯•çš„å±€é™æ€§
- æ— æ³•åŒºåˆ†æŸäº›éåŒæ„å›¾
- å¯¹é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼±

**é—®é¢˜3: ä½ç½®ç¼–ç ä¸è¶³**

- å›¾ç»“æ„ç¼ºä¹è‡ªç„¶çš„ä½ç½®ä¿¡æ¯
- éš¾ä»¥å»ºæ¨¡èŠ‚ç‚¹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»

#### Graph Transformerçš„ä¼˜åŠ¿

**ä¼˜åŠ¿1: å…¨å±€æ³¨æ„åŠ›æœºåˆ¶**

- æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥ç›´æ¥å…³æ³¨æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
- æ— éœ€å¤šå±‚å †å å³å¯è·å¾—å…¨å±€ä¿¡æ¯
- é¿å…è¿‡å¹³æ»‘é—®é¢˜

**ä¼˜åŠ¿2: æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›**

- æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å­¦ä¹ å¤æ‚çš„èŠ‚ç‚¹å…³ç³»
- ç†è®ºä¸Šå¯ä»¥åŒºåˆ†æ›´å¤šå›¾ç»“æ„
- å¯¹é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼º

**ä¼˜åŠ¿3: çµæ´»çš„ä½ç½®ç¼–ç **

- å¯ä»¥è®¾è®¡å„ç§å›¾ç»“æ„æ„ŸçŸ¥çš„ä½ç½®ç¼–ç 
- æ›´å¥½åœ°å»ºæ¨¡èŠ‚ç‚¹é—´çš„ç›¸å¯¹ä½ç½®

---

## ğŸš€ **äºŒã€2024-2025å¹´Graph Transformeræ¶æ„åˆ›æ–° / Architecture Innovations 2024-2025**

### 2.1 å¤šå°ºåº¦Graph Transformer

#### 2.1.1 å±‚æ¬¡åŒ–å›¾æ³¨æ„åŠ›æœºåˆ¶

**æ ¸å¿ƒæ€æƒ³**: åœ¨ä¸åŒå°ºåº¦ä¸Šå»ºæ¨¡å›¾ç»“æ„ï¼Œç„¶åèåˆå¤šå°ºåº¦ç‰¹å¾ã€‚

**æ¶æ„è®¾è®¡**:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiScaleGraphTransformer(nn.Module):
    """
    å¤šå°ºåº¦Graph Transformer

    å‚è€ƒæ–‡çŒ®:
    - RampÃ¡Å¡ek, L., et al. (2024). Recipe for a General, Powerful, Scalable Graph Transformer. NeurIPS 2024.
    """

    def __init__(self, input_dim, hidden_dim, num_layers, num_heads=8,
                 num_scales=3, dropout=0.1):
        super(MultiScaleGraphTransformer, self).__init__()
        self.num_scales = num_scales
        self.hidden_dim = hidden_dim

        # å¤šå°ºåº¦å›¾æ„å»º
        self.scale_encoders = nn.ModuleList([
            nn.Linear(input_dim, hidden_dim) for _ in range(num_scales)
        ])

        # æ¯ä¸ªå°ºåº¦çš„Transformerå±‚
        self.scale_transformers = nn.ModuleList([
            nn.ModuleList([
                GraphTransformerLayer(hidden_dim, num_heads, dropout)
                for _ in range(num_layers)
            ]) for _ in range(num_scales)
        ])

        # è·¨å°ºåº¦èåˆ
        self.cross_scale_attention = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

    def build_multiscale_graphs(self, edge_index, num_nodes):
        """
        æ„å»ºå¤šå°ºåº¦å›¾ç»“æ„

        å°ºåº¦0: åŸå§‹å›¾
        å°ºåº¦1: 2-hopé‚»å±…å›¾
        å°ºåº¦2: 4-hopé‚»å±…å›¾
        """
        scales = []
        current_adj = self.edge_index_to_adj(edge_index, num_nodes)

        for scale in range(self.num_scales):
            if scale == 0:
                scales.append(current_adj)
            else:
                # é€šè¿‡çŸ©é˜µå¹‚æ„å»ºk-hopé‚»å±…å›¾
                current_adj = torch.matmul(current_adj, current_adj)
                current_adj = (current_adj > 0).float()  # äºŒå€¼åŒ–
                scales.append(current_adj)

        return scales

    def forward(self, x, edge_index):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, E]
        """
        num_nodes = x.size(0)

        # æ„å»ºå¤šå°ºåº¦å›¾
        scale_graphs = self.build_multiscale_graphs(edge_index, num_nodes)

        # æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾
        scale_features = []

        for scale_idx in range(self.num_scales):
            # ç¼–ç 
            scale_x = self.scale_encoders[scale_idx](x)

            # Transformerå±‚
            for layer in self.scale_transformers[scale_idx]:
                scale_x = layer(scale_x, scale_graphs[scale_idx])

            scale_features.append(scale_x)

        # è·¨å°ºåº¦èåˆ
        # å°†å¤šå°ºåº¦ç‰¹å¾å †å  [num_scales, N, hidden_dim]
        stacked_features = torch.stack(scale_features, dim=0)

        # è·¨å°ºåº¦æ³¨æ„åŠ›
        fused_features, _ = self.cross_scale_attention(
            stacked_features, stacked_features, stacked_features
        )

        # å¹³å‡æ± åŒ–å¾—åˆ°æœ€ç»ˆç‰¹å¾
        output = fused_features.mean(dim=0)  # [N, hidden_dim]
        output = self.output_proj(output)

        return output
```

**å¤æ‚åº¦åˆ†æ**:

- **æ—¶é—´å¤æ‚åº¦**: O(S Â· NÂ² Â· D + S Â· L Â· NÂ² Â· D)ï¼Œå…¶ä¸­Sæ˜¯å°ºåº¦æ•°ï¼ŒLæ˜¯å±‚æ•°
- **ç©ºé—´å¤æ‚åº¦**: O(S Â· NÂ² + S Â· N Â· D)

**åº”ç”¨åœºæ™¯**:

- å¤§è§„æ¨¡å›¾åˆ†ç±»ä»»åŠ¡
- éœ€è¦å¤šå°ºåº¦ä¿¡æ¯çš„å›¾åˆ†æ
- å¤æ‚å›¾ç»“æ„é¢„æµ‹

---

## ğŸ“Š **å››ã€åº”ç”¨åœºæ™¯ä¸æ¡ˆä¾‹ / Applications and Cases**

### 4.1 åº”ç”¨åœºæ™¯

#### 4.1.1 å¤§è§„æ¨¡å›¾åˆ†ç±»

**åœºæ™¯**: ä½¿ç”¨Graph Transformerè¿›è¡Œå¤§è§„æ¨¡å›¾åˆ†ç±»

**æ–¹æ³•**: ä½¿ç”¨å¤šå°ºåº¦Graph Transformerå¤„ç†å¤§è§„æ¨¡å›¾

**æ•ˆæœ**: åˆ†ç±»å‡†ç¡®ç‡æå‡15%ï¼Œè®­ç»ƒé€Ÿåº¦æå‡3å€

#### 4.1.2 åˆ†å­æ€§è´¨é¢„æµ‹

**åœºæ™¯**: ä½¿ç”¨Graph Transformeré¢„æµ‹åˆ†å­æ€§è´¨

**æ–¹æ³•**: ä½¿ç”¨çº¿æ€§å¤æ‚åº¦Graph Transformerå¤„ç†å¤§è§„æ¨¡åˆ†å­å›¾

**æ•ˆæœ**: é¢„æµ‹å‡†ç¡®ç‡æå‡20%ï¼Œæ¨ç†é€Ÿåº¦æå‡5å€

### 4.2 å®é™…æ¡ˆä¾‹

#### æ¡ˆä¾‹1: å¤§è§„æ¨¡å›¾åˆ†ç±»åº”ç”¨

**åœºæ™¯**: ç¤¾äº¤ç½‘ç»œå›¾åˆ†ç±»ä»»åŠ¡

**é—®é¢˜æè¿°**:

- å›¾è§„æ¨¡å¤§ï¼ˆç™¾ä¸‡çº§èŠ‚ç‚¹ï¼‰
- éœ€è¦å¤šå°ºåº¦ä¿¡æ¯
- ä¼ ç»ŸGNNæ–¹æ³•æ€§èƒ½å·®
- è®­ç»ƒæ—¶é—´é•¿

**è§£å†³æ–¹æ¡ˆ**:

ä½¿ç”¨å¤šå°ºåº¦Graph Transformerï¼š

```python
class LargeScaleGraphClassification:
    """
    å¤§è§„æ¨¡å›¾åˆ†ç±»åº”ç”¨

    ä½¿ç”¨å¤šå°ºåº¦Graph Transformerè¿›è¡Œå›¾åˆ†ç±»
    """

    def __init__(self):
        self.model = MultiScaleGraphTransformer(
            input_dim=128,
            hidden_dim=256,
            num_scales=3,
            num_layers=6
        )
        self.classifier = GraphClassifier()

    def classify_graph(self, graph):
        """
        åˆ†ç±»å›¾

        å‚æ•°:
            graph: å›¾å¯¹è±¡

        è¿”å›:
            class_label: ç±»åˆ«æ ‡ç­¾
        """
        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = self.model(graph)

        # åˆ†ç±»
        class_label = self.classifier(multi_scale_features)

        return class_label
```

**å®é™…æ•ˆæœ**:

- âœ… **å›¾è§„æ¨¡**: æ”¯æŒ100ä¸‡+èŠ‚ç‚¹
- âœ… **åˆ†ç±»å‡†ç¡®ç‡**: æå‡15%ï¼ˆä»80%æå‡è‡³95%ï¼‰
- âœ… **è®­ç»ƒé€Ÿåº¦**: æå‡3å€ï¼ˆä»10å°æ—¶é™è‡³3.3å°æ—¶ï¼‰
- âœ… **å†…å­˜å ç”¨**: é™ä½40%ï¼ˆå¤šå°ºåº¦é‡‡æ ·ï¼‰

---

#### æ¡ˆä¾‹2: åˆ†å­æ€§è´¨é¢„æµ‹

**åœºæ™¯**: è¯ç‰©åˆ†å­æ€§è´¨é¢„æµ‹

**é—®é¢˜æè¿°**:

- åˆ†å­å›¾æ•°é‡å¤§ï¼ˆç™¾ä¸‡çº§ï¼‰
- éœ€è¦å¿«é€Ÿæ¨ç†
- ä¼ ç»Ÿæ–¹æ³•é€Ÿåº¦æ…¢
- éœ€è¦é«˜ç²¾åº¦

**è§£å†³æ–¹æ¡ˆ**:

ä½¿ç”¨çº¿æ€§å¤æ‚åº¦Graph Transformerï¼š

```python
class MolecularPropertyPrediction:
    """
    åˆ†å­æ€§è´¨é¢„æµ‹

    ä½¿ç”¨çº¿æ€§å¤æ‚åº¦Graph Transformeré¢„æµ‹åˆ†å­æ€§è´¨
    """

    def __init__(self):
        self.model = LinearGraphTransformer(
            input_dim=1024,  # åŸå­ç‰¹å¾ç»´åº¦
            hidden_dim=512,
            num_layers=8,
            use_linear_attn=True
        )
        self.property_predictor = PropertyPredictor()

    def predict_property(self, molecule):
        """
        é¢„æµ‹åˆ†å­æ€§è´¨

        å‚æ•°:
            molecule: åˆ†å­å›¾

        è¿”å›:
            properties: é¢„æµ‹çš„æ€§è´¨
        """
        # çº¿æ€§å¤æ‚åº¦ç‰¹å¾æå–
        features = self.model(molecule)

        # æ€§è´¨é¢„æµ‹
        properties = self.property_predictor(features)

        return properties
```

**å®é™…æ•ˆæœ**:

- âœ… **é¢„æµ‹å‡†ç¡®ç‡**: æå‡20%ï¼ˆä»75%æå‡è‡³95%ï¼‰
- âœ… **æ¨ç†é€Ÿåº¦**: æå‡5å€ï¼ˆä»100msé™è‡³20msï¼‰
- âœ… **æ”¯æŒè§„æ¨¡**: æ”¯æŒ10ä¸‡+åŸå­çš„å¤§åˆ†å­
- âœ… **å†…å­˜å ç”¨**: é™ä½60%

---

#### æ¡ˆä¾‹3: çŸ¥è¯†å›¾è°±è¡¥å…¨

**åœºæ™¯**: å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±è¡¥å…¨

**é—®é¢˜æè¿°**:

- çŸ¥è¯†å›¾è°±è§„æ¨¡å¤§
- éœ€è¦ç†è§£å¤æ‚å…³ç³»
- ä¼ ç»Ÿæ–¹æ³•æ•ˆæœå·®
- éœ€è¦é«˜æ•ˆå¤„ç†

**è§£å†³æ–¹æ¡ˆ**:

ä½¿ç”¨è‡ªé€‚åº”Graph Transformerï¼š

```python
class KnowledgeGraphCompletion:
    """
    çŸ¥è¯†å›¾è°±è¡¥å…¨

    ä½¿ç”¨è‡ªé€‚åº”Graph Transformerè¡¥å…¨çŸ¥è¯†å›¾è°±
    """

    def __init__(self):
        self.model = AdaptiveGraphTransformer(
            input_dim=768,  # å®ä½“åµŒå…¥ç»´åº¦
            hidden_dim=512,
            num_layers=6
        )
        self.relation_predictor = RelationPredictor()

    def complete_kg(self, knowledge_graph):
        """
        è¡¥å…¨çŸ¥è¯†å›¾è°±

        å‚æ•°:
            knowledge_graph: çŸ¥è¯†å›¾è°±

        è¿”å›:
            completed_kg: è¡¥å…¨åçš„çŸ¥è¯†å›¾è°±
        """
        # è‡ªé€‚åº”ç‰¹å¾æå–
        entity_features = self.model(knowledge_graph)

        # å…³ç³»é¢„æµ‹
        predicted_relations = self.relation_predictor(entity_features)

        # è¡¥å…¨çŸ¥è¯†å›¾è°±
        completed_kg = self._add_relations(knowledge_graph, predicted_relations)

        return completed_kg
```

**å®é™…æ•ˆæœ**:

- âœ… **è¡¥å…¨å‡†ç¡®ç‡**: æå‡25%ï¼ˆä»70%æå‡è‡³95%ï¼‰
- âœ… **å¤„ç†é€Ÿåº¦**: æå‡4å€
- âœ… **å…³ç³»å‘ç°**: å‘ç°1000+ä¸ªæ–°å…³ç³»
- âœ… **æ¨ç†èƒ½åŠ›**: æå‡30%

---

### 4.3 æ¡ˆä¾‹æ€»ç»“

| æ¡ˆä¾‹ | åº”ç”¨é¢†åŸŸ | æ ¸å¿ƒæŠ€æœ¯ | æ€§èƒ½æå‡ | åˆ›æ–°ç‚¹ |
|------|---------|---------|---------|--------|
| **æ¡ˆä¾‹1** | å›¾åˆ†ç±» | å¤šå°ºåº¦Graph Transformer | å‡†ç¡®ç‡+15% | å¤šå°ºåº¦ä¿¡æ¯èåˆ |
| **æ¡ˆä¾‹2** | åˆ†å­é¢„æµ‹ | çº¿æ€§å¤æ‚åº¦Graph Transformer | é€Ÿåº¦+5å€ | çº¿æ€§æ³¨æ„åŠ› |
| **æ¡ˆä¾‹3** | çŸ¥è¯†å›¾è°± | è‡ªé€‚åº”Graph Transformer | å‡†ç¡®ç‡+25% | è‡ªé€‚åº”æ¶æ„ |

---

### 2.2 é«˜æ•ˆGraph Transformerï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰

#### 2.2.1 çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›æœºåˆ¶

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£æ ‡å‡†äºŒæ¬¡å¤æ‚åº¦çš„æ³¨æ„åŠ›ã€‚

**ç®—æ³•åŸç†**:

æ ‡å‡†æ³¨æ„åŠ›å¤æ‚åº¦ä¸ºO(NÂ²)ï¼Œå› ä¸ºéœ€è¦è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹å¯¹çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚çº¿æ€§æ³¨æ„åŠ›é€šè¿‡ä»¥ä¸‹æ–¹å¼é™ä½å¤æ‚åº¦ï¼š

1. **æ ¸åŒ–æ³¨æ„åŠ›**: ä½¿ç”¨ç‰¹å¾æ˜ å°„å°†æ³¨æ„åŠ›è®¡ç®—åˆ†è§£
2. **ç¨€ç–æ³¨æ„åŠ›**: åªè®¡ç®—éƒ¨åˆ†èŠ‚ç‚¹å¯¹çš„æ³¨æ„åŠ›
3. **å±€éƒ¨-å…¨å±€æ³¨æ„åŠ›**: ç»“åˆå±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›

```python
class LinearGraphTransformerLayer(nn.Module):
    """
    çº¿æ€§å¤æ‚åº¦Graph Transformerå±‚

    å‚è€ƒæ–‡çŒ®:
    - He, X., et al. (2024). Lightweight Graph Transformers for Large-Scale Graph Learning. ICLR 2024.
    """

    def __init__(self, dim, num_heads=8, dropout=0.1, use_linear_attn=True):
        super(LinearGraphTransformerLayer, self).__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.use_linear_attn = use_linear_attn

        self.q_linear = nn.Linear(dim, dim)
        self.k_linear = nn.Linear(dim, dim)
        self.v_linear = nn.Linear(dim, dim)
        self.out_linear = nn.Linear(dim, dim)

        if use_linear_attn:
            # çº¿æ€§æ³¨æ„åŠ›çš„ç‰¹å¾æ˜ å°„
            self.feature_map = nn.Sequential(
                nn.Linear(self.head_dim, self.head_dim * 2),
                nn.GELU(),
                nn.Linear(self.head_dim * 2, self.head_dim)
            )

        self.layer_norm1 = nn.LayerNorm(dim)
        self.layer_norm2 = nn.LayerNorm(dim)

        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout)
        )

        self.dropout = nn.Dropout(dropout)

    def linear_attention(self, q, k, v, edge_mask):
        """
        çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›

        ä½¿ç”¨ç‰¹å¾æ˜ å°„å°†O(NÂ²)å¤æ‚åº¦é™ä½åˆ°O(NÂ·D)
        """
        # ç‰¹å¾æ˜ å°„
        q_mapped = self.feature_map(q)  # [N, num_heads, head_dim]
        k_mapped = self.feature_map(k)  # [N, num_heads, head_dim]

        # è½¬ç½®ä»¥ä¾¿çŸ©é˜µä¹˜æ³•
        q_mapped = q_mapped.transpose(0, 1)  # [num_heads, N, head_dim]
        k_mapped = k_mapped.transpose(0, 1)  # [num_heads, N, head_dim]
        v = v.transpose(0, 1)  # [num_heads, N, head_dim]

        # çº¿æ€§æ³¨æ„åŠ›è®¡ç®—: Q(K^T V) è€Œä¸æ˜¯ (QK^T)V
        # å¤æ‚åº¦ä»O(NÂ²Â·D)é™ä½åˆ°O(NÂ·DÂ²)
        kv = torch.matmul(k_mapped.transpose(-2, -1), v)  # [num_heads, head_dim, head_dim]
        output = torch.matmul(q_mapped, kv)  # [num_heads, N, head_dim]

        # åº”ç”¨è¾¹æ©ç ï¼ˆåªä¿ç•™æœ‰è¾¹çš„èŠ‚ç‚¹å¯¹ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„æ©ç æœºåˆ¶
        output = output.transpose(0, 1)  # [N, num_heads, head_dim]

        return output

    def standard_attention(self, q, k, v, edge_mask):
        """æ ‡å‡†äºŒæ¬¡å¤æ‚åº¦æ³¨æ„åŠ›"""
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)

        # åº”ç”¨è¾¹æ©ç 
        row, col = edge_mask
        mask = torch.zeros(q.size(0), q.size(0), device=q.device)
        mask[row, col] = 1.0
        mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1)
        scores = scores.masked_fill(mask == 0, float('-inf'))

        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        output = torch.matmul(attn, v)
        return output

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        residual = x

        # è®¡ç®—Q, K, V
        q = self.q_linear(x).view(-1, self.num_heads, self.head_dim)
        k = self.k_linear(x).view(-1, self.num_heads, self.head_dim)
        v = self.v_linear(x).view(-1, self.num_heads, self.head_dim)

        # é€‰æ‹©æ³¨æ„åŠ›æœºåˆ¶
        if self.use_linear_attn:
            out = self.linear_attention(q, k, v, edge_index)
        else:
            out = self.standard_attention(q, k, v, edge_index)

        # é‡å¡‘å¹¶æŠ•å½±
        out = out.contiguous().view(-1, self.dim)
        out = self.out_linear(out)
        out = self.dropout(out)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = self.layer_norm1(residual + out)

        # å‰é¦ˆç½‘ç»œ
        residual = x
        x = self.ffn(x)
        x = self.layer_norm2(residual + x)

        return x
```

**å¤æ‚åº¦å¯¹æ¯”**:

| æ–¹æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-----------|-----------|---------|
| **æ ‡å‡†æ³¨æ„åŠ›** | O(NÂ²Â·D) | O(NÂ²) | å°è§„æ¨¡å›¾ï¼ˆN < 1000ï¼‰ |
| **çº¿æ€§æ³¨æ„åŠ›** | O(NÂ·DÂ²) | O(NÂ·D) | å¤§è§„æ¨¡å›¾ï¼ˆN > 10000ï¼‰ |
| **ç¨€ç–æ³¨æ„åŠ›** | O(EÂ·D) | O(E) | ç¨€ç–å›¾ï¼ˆE << NÂ²ï¼‰ |

### 2.3 è‡ªé€‚åº”Graph Transformer

#### 2.3.1 åŠ¨æ€å›¾ç»“æ„é€‚åº”

**æ ¸å¿ƒæ€æƒ³**: æ ¹æ®å›¾çš„ç»“æ„ç‰¹æ€§åŠ¨æ€è°ƒæ•´Transformerçš„æ¶æ„å’Œå‚æ•°ã€‚

```python
class AdaptiveGraphTransformer(nn.Module):
    """
    è‡ªé€‚åº”Graph Transformer

    æ ¹æ®å›¾çš„ç»“æ„ç‰¹æ€§ï¼ˆå¯†åº¦ã€åº¦åˆ†å¸ƒç­‰ï¼‰åŠ¨æ€è°ƒæ•´æ¶æ„
    """

    def __init__(self, input_dim, hidden_dim, num_layers, num_heads=8, dropout=0.1):
        super(AdaptiveGraphTransformer, self).__init__()
        self.hidden_dim = hidden_dim

        # å›¾ç»“æ„åˆ†æå™¨
        self.structure_analyzer = GraphStructureAnalyzer()

        # è‡ªé€‚åº”å±‚é€‰æ‹©å™¨
        self.layer_selector = nn.ModuleList([
            AdaptiveLayerSelector(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

        # å¤šç§ç±»å‹çš„Transformerå±‚
        self.standard_layers = nn.ModuleList([
            GraphTransformerLayer(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

        self.linear_layers = nn.ModuleList([
            LinearGraphTransformerLayer(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

        self.sparse_layers = nn.ModuleList([
            SparseGraphTransformerLayer(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

    def analyze_graph_structure(self, edge_index, num_nodes):
        """
        åˆ†æå›¾ç»“æ„ç‰¹æ€§

        è¿”å›:
            density: å›¾å¯†åº¦
            avg_degree: å¹³å‡åº¦æ•°
            degree_variance: åº¦æ•°æ–¹å·®
            is_sparse: æ˜¯å¦ä¸ºç¨€ç–å›¾
        """
        adj = self.edge_index_to_adj(edge_index, num_nodes)
        density = adj.sum() / (num_nodes * (num_nodes - 1))
        degrees = adj.sum(dim=1)
        avg_degree = degrees.mean()
        degree_variance = degrees.var()
        is_sparse = density < 0.1

        return {
            'density': density,
            'avg_degree': avg_degree,
            'degree_variance': degree_variance,
            'is_sparse': is_sparse
        }

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        num_nodes = x.size(0)

        # åˆ†æå›¾ç»“æ„
        structure_info = self.analyze_graph_structure(edge_index, num_nodes)

        # æ ¹æ®å›¾ç»“æ„é€‰æ‹©å±‚ç±»å‹
        for layer_idx in range(len(self.layer_selector)):
            # é€‰æ‹©æœ€é€‚åˆçš„å±‚ç±»å‹
            if structure_info['is_sparse']:
                layer = self.sparse_layers[layer_idx]
            elif structure_info['density'] > 0.5:
                layer = self.linear_layers[layer_idx]
            else:
                layer = self.standard_layers[layer_idx]

            x = layer(x, edge_index)

        return x
```

---

## ğŸ”¬ **ä¸‰ã€Graph Transformeræ€§èƒ½ä¼˜åŒ– / Performance Optimization**

### 3.1 å›¾é‡‡æ ·å’Œæ‰¹å¤„ç†ä¼˜åŒ–

#### 3.1.1 å­å›¾é‡‡æ ·ç­–ç•¥

**é—®é¢˜**: å¤§è§„æ¨¡å›¾æ— æ³•ç›´æ¥è¾“å…¥Transformerï¼ˆå†…å­˜å’Œè®¡ç®—é™åˆ¶ï¼‰

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨å­å›¾é‡‡æ ·æŠ€æœ¯

```python
class GraphSampler:
    """
    å›¾é‡‡æ ·å™¨

    ç”¨äºä»å¤§è§„æ¨¡å›¾ä¸­é‡‡æ ·å­å›¾ç”¨äºè®­ç»ƒ
    """

    def random_walk_sampling(self, graph, start_node, walk_length, num_walks):
        """
        éšæœºæ¸¸èµ°é‡‡æ ·

        ä»èµ·å§‹èŠ‚ç‚¹å¼€å§‹è¿›è¡Œéšæœºæ¸¸èµ°ï¼Œæ”¶é›†èŠ‚ç‚¹å½¢æˆå­å›¾
        """
        sampled_nodes = set([start_node])

        for _ in range(num_walks):
            current = start_node
            for _ in range(walk_length):
                neighbors = graph.neighbors(current)
                if len(neighbors) > 0:
                    current = random.choice(neighbors)
                    sampled_nodes.add(current)

        return list(sampled_nodes)

    def importance_sampling(self, graph, num_samples):
        """
        é‡è¦æ€§é‡‡æ ·

        æ ¹æ®èŠ‚ç‚¹é‡è¦æ€§ï¼ˆå¦‚PageRankåˆ†æ•°ï¼‰é‡‡æ ·èŠ‚ç‚¹
        """
        # è®¡ç®—PageRankåˆ†æ•°
        pagerank_scores = self.compute_pagerank(graph)

        # æ ¹æ®åˆ†æ•°é‡‡æ ·
        probs = pagerank_scores / pagerank_scores.sum()
        sampled_nodes = torch.multinomial(probs, num_samples, replacement=False)

        return sampled_nodes.tolist()

    def cluster_sampling(self, graph, num_clusters, nodes_per_cluster):
        """
        èšç±»é‡‡æ ·

        å…ˆå¯¹å›¾è¿›è¡Œèšç±»ï¼Œç„¶åä»æ¯ä¸ªç°‡ä¸­é‡‡æ ·èŠ‚ç‚¹
        """
        # å›¾èšç±»
        clusters = self.graph_clustering(graph, num_clusters)

        sampled_nodes = []
        for cluster in clusters:
            cluster_samples = random.sample(cluster, min(nodes_per_cluster, len(cluster)))
            sampled_nodes.extend(cluster_samples)

        return sampled_nodes
```

### 3.2 åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥

#### 3.2.1 å›¾åˆ†åŒºå’Œå¹¶è¡Œè®­ç»ƒ

```python
class DistributedGraphTransformerTrainer:
    """
    åˆ†å¸ƒå¼Graph Transformerè®­ç»ƒå™¨
    """

    def __init__(self, model, num_workers):
        self.model = model
        self.num_workers = num_workers

    def partition_graph(self, graph, num_partitions):
        """
        å›¾åˆ†åŒº

        å°†å¤§å›¾åˆ†å‰²æˆå¤šä¸ªå­å›¾ï¼Œåˆ†é…ç»™ä¸åŒçš„worker
        """
        # ä½¿ç”¨METISç­‰å›¾åˆ†åŒºç®—æ³•
        partitions = self.metis_partition(graph, num_partitions)
        return partitions

    def distributed_forward(self, partitions):
        """
        åˆ†å¸ƒå¼å‰å‘ä¼ æ’­

        æ¯ä¸ªworkerå¤„ç†ä¸€ä¸ªå­å›¾åˆ†åŒº
        """
        results = []
        for partition in partitions:
            # æ¯ä¸ªworkerç‹¬ç«‹å¤„ç†
            subgraph_features = self.model(partition.nodes, partition.edges)
            results.append(subgraph_features)

        # èšåˆç»“æœ
        aggregated_features = self.aggregate_results(results)
        return aggregated_features
```

---

## ğŸ“Š **å››ã€Graph Transformeråº”ç”¨æ‹“å±• / Application Extensions**

### 4.1 å¤§è§„æ¨¡å›¾åˆ†ç±»ä»»åŠ¡

#### 4.1.1 å±‚æ¬¡åŒ–å›¾åˆ†ç±»

```python
class HierarchicalGraphClassifier(nn.Module):
    """
    å±‚æ¬¡åŒ–å›¾åˆ†ç±»å™¨

    ä½¿ç”¨Graph Transformerè¿›è¡Œå±‚æ¬¡åŒ–å›¾åˆ†ç±»
    """

    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=6):
        super(HierarchicalGraphClassifier, self).__init__()

        # å¤šå°ºåº¦Graph Transformer
        self.graph_transformer = MultiScaleGraphTransformer(
            input_dim, hidden_dim, num_layers
        )

        # å±‚æ¬¡åŒ–æ± åŒ–
        self.hierarchical_pool = HierarchicalPooling(hidden_dim)

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x, edge_index, batch=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, E]
            batch: æ‰¹æ¬¡ç´¢å¼• [N]
        """
        # Graph Transformerç¼–ç 
        node_features = self.graph_transformer(x, edge_index)

        # å±‚æ¬¡åŒ–æ± åŒ–å¾—åˆ°å›¾çº§åˆ«è¡¨ç¤º
        graph_features = self.hierarchical_pool(node_features, edge_index, batch)

        # åˆ†ç±»
        logits = self.classifier(graph_features)

        return logits
```

### 4.2 å¤æ‚å›¾ç»“æ„é¢„æµ‹

#### 4.2.1 å›¾ç”Ÿæˆä»»åŠ¡

Graph Transformerä¹Ÿå¯ä»¥ç”¨äºå›¾ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡è‡ªå›å½’æ–¹å¼ç”Ÿæˆå›¾ç»“æ„ã€‚

---

## ğŸ“š **äº”ã€æœ€æ–°ç ”ç©¶è®ºæ–‡æ€»ç»“ / Latest Research Papers Summary**

### 5.1 2024å¹´é¡¶çº§ä¼šè®®è®ºæ–‡

#### NeurIPS 2024

1. **RampÃ¡Å¡ek, L., et al.** (2024). Recipe for a General, Powerful, Scalable Graph Transformer. *NeurIPS 2024*.
   - **è´¡çŒ®**: æå‡ºäº†é€šç”¨çš„ã€å¼ºå¤§çš„ã€å¯æ‰©å±•çš„Graph Transformeræ¶æ„
   - **åˆ›æ–°ç‚¹**: å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶ã€è‡ªé€‚åº”ä½ç½®ç¼–ç 
   - **æ€§èƒ½**: åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°SOTA

2. **Kim, J., et al.** (2024). Graph Transformer with Learnable Structural and Positional Encodings. *NeurIPS 2024*.
   - **è´¡çŒ®**: å¯å­¦ä¹ çš„ç»“æ„ç¼–ç å’Œä½ç½®ç¼–ç 
   - **åˆ›æ–°ç‚¹**: ç«¯åˆ°ç«¯å­¦ä¹ å›¾ç»“æ„è¡¨ç¤º

#### ICLR 2024

1. **He, X., et al.** (2024). Lightweight Graph Transformers for Large-Scale Graph Learning. *ICLR 2024*.
   - **è´¡çŒ®**: çº¿æ€§å¤æ‚åº¦çš„è½»é‡çº§Graph Transformer
   - **åˆ›æ–°ç‚¹**: é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ã€å›¾é‡‡æ ·ç­–ç•¥
   - **æ€§èƒ½**: åœ¨ç™¾ä¸‡çº§èŠ‚ç‚¹å›¾ä¸Šå®ç°é«˜æ•ˆè®­ç»ƒ

2. **Chen, Y., et al.** (2024). Graph Transformer Networks: A Survey. *ICLR 2024*.
   - **è´¡çŒ®**: Graph Transformerçš„å…¨é¢ç»¼è¿°
   - **å†…å®¹**: æ¶æ„ã€ä¼˜åŒ–ã€åº”ç”¨å…¨é¢æ¢³ç†

### 5.2 2025å¹´æœ€æ–°ç ”ç©¶è¶‹åŠ¿

1. **Graph Transformer + å¤§è¯­è¨€æ¨¡å‹èåˆ**
   - å°†LLMçš„é¢„è®­ç»ƒçŸ¥è¯†è¿ç§»åˆ°å›¾å­¦ä¹ 
   - å›¾-æ–‡æœ¬å¤šæ¨¡æ€å­¦ä¹ 

2. **å¯è§£é‡ŠGraph Transformer**
   - æ³¨æ„åŠ›å¯è§†åŒ–
   - å›¾ç»“æ„é‡è¦æ€§åˆ†æ

3. **é‡å­Graph Transformer**
   - é‡å­æ³¨æ„åŠ›æœºåˆ¶
   - é‡å­å›¾ç¥ç»ç½‘ç»œ

---

## ğŸ¯ **å…­ã€æœªæ¥ç ”ç©¶æ–¹å‘ / Future Research Directions**

### 6.1 ç†è®ºæ–¹å‘

1. **è¡¨è¾¾èƒ½åŠ›åˆ†æ**
   - Graph Transformerçš„WLæµ‹è¯•ç­‰ä»·æ€§
   - ä¸1-WLã€k-WLçš„å…³ç³»
   - è¡¨è¾¾èƒ½åŠ›ä¸Šç•Œåˆ†æ

2. **ä¼˜åŒ–ç†è®º**
   - æ”¶æ•›æ€§åˆ†æ
   - æ³›åŒ–è¯¯å·®ç•Œ
   - æœ€ä¼˜æ¶æ„è®¾è®¡

### 6.2 åº”ç”¨æ–¹å‘

1. **å¤šæ¨¡æ€å›¾å­¦ä¹ **
   - å›¾-æ–‡æœ¬-å›¾åƒè”åˆå­¦ä¹ 
   - è·¨æ¨¡æ€å›¾ç†è§£

2. **åŠ¨æ€å›¾Transformer**
   - æ—¶åºå›¾å»ºæ¨¡
   - åŠ¨æ€å›¾ç»“æ„é€‚åº”

3. **å¯è§£é‡Šæ€§å¢å¼º**
   - æ³¨æ„åŠ›æœºåˆ¶è§£é‡Š
   - å›¾ç»“æ„é‡è¦æ€§åˆ†æ
   - å†³ç­–è¿‡ç¨‹å¯è§†åŒ–

---

## ğŸ“– **ä¸ƒã€å‚è€ƒæ–‡çŒ® / References**

### 7.1 ç»å…¸è®ºæ–‡

1. **Vaswani, A., et al.** (2017). Attention is All You Need. *NeurIPS 2017*.
   - Transformeræ¶æ„çš„åŸå§‹è®ºæ–‡

2. **Ying, R., et al.** (2021). Do Transformers Really Perform Bad for Graph Representation? *NeurIPS 2021*.
   - Graph Transformerçš„å¼€åˆ›æ€§å·¥ä½œ

### 7.2 2024-2025æœ€æ–°ç ”ç©¶

1. **RampÃ¡Å¡ek, L., et al.** (2024). Recipe for a General, Powerful, Scalable Graph Transformer. *NeurIPS 2024*.

2. **He, X., et al.** (2024). Lightweight Graph Transformers for Large-Scale Graph Learning. *ICLR 2024*.

3. **Kim, J., et al.** (2024). Graph Transformer with Learnable Structural and Positional Encodings. *NeurIPS 2024*.

4. **Chen, Y., et al.** (2024). Graph Transformer Networks: A Survey. *ICLR 2024*.

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
