# ç¤¾ä¼šç½‘ç»œåŸºç¡€ / Social Networks Fundamentals

## ğŸ“š æ¦‚è¿° / Overview

ç¤¾ä¼šç½‘ç»œæ˜¯æè¿°ç¤¾ä¼šå®ä½“ï¼ˆä¸ªäººã€ç»„ç»‡ã€ç¾¤ä½“ï¼‰ä¹‹é—´å…³ç³»ç»“æ„çš„ç½‘ç»œç³»ç»Ÿã€‚æœ¬æ–‡æ¡£æ¶µç›–ç¤¾ä¼šç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µã€åˆ†ææ–¹æ³•ã€ç¤¾åŒºæ£€æµ‹ã€ç½‘ç»œæ¼”åŒ–ã€å½±å“åŠ›ä¼ æ’­ç­‰æ ¸å¿ƒå†…å®¹ï¼Œå¯¹æ ‡å›½é™…æ ‡å‡†ï¼ˆMITã€Stanfordã€Harvardã€Oxfordï¼‰å’Œæœ€æ–°ç½‘ç»œç§‘å­¦ç ”ç©¶è¿›å±•ã€‚

## ğŸ“‘ ç›®å½• / Table of Contents

- [ç¤¾ä¼šç½‘ç»œåŸºç¡€ / Social Networks Fundamentals](#ç¤¾ä¼šç½‘ç»œåŸºç¡€--social-networks-fundamentals)
  - [ğŸ“š æ¦‚è¿° / Overview](#-æ¦‚è¿°--overview)
  - [ğŸ“‘ ç›®å½• / Table of Contents](#-ç›®å½•--table-of-contents)
  - [å†å²èƒŒæ™¯ / Historical Background](#å†å²èƒŒæ™¯--historical-background)
  - [åº”ç”¨é¢†åŸŸ](#åº”ç”¨é¢†åŸŸ)
  - [1. ç¤¾ä¼šç½‘ç»œåŸºæœ¬æ¦‚å¿µ / Basic Concepts of Social Networks](#1-ç¤¾ä¼šç½‘ç»œåŸºæœ¬æ¦‚å¿µ--basic-concepts-of-social-networks)
    - [1.0 ç¤¾ä¼šç½‘ç»œåˆ†ææ€ç»´å¯¼å›¾ / Social Network Analysis Mind Map](#10-ç¤¾ä¼šç½‘ç»œåˆ†ææ€ç»´å¯¼å›¾--social-network-analysis-mind-map)
    - [1.1 åŸºæœ¬å®šä¹‰ / Basic Definitions](#11-åŸºæœ¬å®šä¹‰--basic-definitions)
    - [1.2 ç½‘ç»œè¡¨ç¤º](#12-ç½‘ç»œè¡¨ç¤º)
  - [2. ç½‘ç»œåº¦é‡](#2-ç½‘ç»œåº¦é‡)
    - [2.0 ä¸­å¿ƒæ€§åº¦é‡å¯¹æ¯”çŸ©é˜µ / Centrality Measures Comparison Matrix](#20-ä¸­å¿ƒæ€§åº¦é‡å¯¹æ¯”çŸ©é˜µ--centrality-measures-comparison-matrix)
    - [2.1 ä¸­å¿ƒæ€§åº¦é‡](#21-ä¸­å¿ƒæ€§åº¦é‡)
    - [2.2 ç½‘ç»œç»“æ„åº¦é‡](#22-ç½‘ç»œç»“æ„åº¦é‡)
  - [3. ç¤¾åŒºæ£€æµ‹](#3-ç¤¾åŒºæ£€æµ‹)
    - [3.1 ç¤¾åŒºå®šä¹‰](#31-ç¤¾åŒºå®šä¹‰)
    - [3.2 å±‚æ¬¡èšç±»](#32-å±‚æ¬¡èšç±»)
  - [4. ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æ / Social Media Network Analysis](#4-ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æ--social-media-network-analysis)
    - [4.1 ç¤¾äº¤åª’ä½“ç½‘ç»œç‰¹å¾](#41-ç¤¾äº¤åª’ä½“ç½‘ç»œç‰¹å¾)
    - [4.2 å½±å“åŠ›ä¼ æ’­æ¨¡å‹](#42-å½±å“åŠ›ä¼ æ’­æ¨¡å‹)
  - [5. ç½‘ç»œæ¼”åŒ– / Network Evolution](#5-ç½‘ç»œæ¼”åŒ–--network-evolution)
    - [4.2 æ—¶é—´åºåˆ—åˆ†æ](#42-æ—¶é—´åºåˆ—åˆ†æ)
  - [5. å¤šæ¨¡æ€è¡¨è¾¾ä¸å¯è§†åŒ–](#5-å¤šæ¨¡æ€è¡¨è¾¾ä¸å¯è§†åŒ–)
    - [5.1 ç½‘ç»œå¯è§†åŒ–](#51-ç½‘ç»œå¯è§†åŒ–)
    - [5.2 æ—¶é—´æ¼”åŒ–å¯è§†åŒ–](#52-æ—¶é—´æ¼”åŒ–å¯è§†åŒ–)
    - [5.3 ç½‘ç»œæµç¨‹å›¾](#53-ç½‘ç»œæµç¨‹å›¾)
  - [6. è‡ªåŠ¨åŒ–è„šæœ¬å»ºè®®](#6-è‡ªåŠ¨åŒ–è„šæœ¬å»ºè®®)
    - [6.1 ç½‘ç»œæ„å»ºè„šæœ¬](#61-ç½‘ç»œæ„å»ºè„šæœ¬)
    - [6.2 åˆ†æè„šæœ¬](#62-åˆ†æè„šæœ¬)
    - [6.3 å¯è§†åŒ–è„šæœ¬](#63-å¯è§†åŒ–è„šæœ¬)
  - [7. å½¢å¼åŒ–è¯­ä¹‰ä¸æ¦‚å¿µè§£é‡Š](#7-å½¢å¼åŒ–è¯­ä¹‰ä¸æ¦‚å¿µè§£é‡Š)
    - [7.1 å½¢å¼åŒ–è¯­ä¹‰](#71-å½¢å¼åŒ–è¯­ä¹‰)
    - [7.2 å…¸å‹å®šç†ä¸è¯æ˜](#72-å…¸å‹å®šç†ä¸è¯æ˜)
    - [7.3 è‡ªåŠ¨åŒ–éªŒè¯å»ºè®®](#73-è‡ªåŠ¨åŒ–éªŒè¯å»ºè®®)
  - [9. å›½é™…æ ‡å‡†å¯¹ç…§ / International Standards Alignment](#9-å›½é™…æ ‡å‡†å¯¹ç…§--international-standards-alignment)
    - [9.1 å­¦æœ¯æœºæ„æ ‡å‡†](#91-å­¦æœ¯æœºæ„æ ‡å‡†)
    - [9.2 å›½é™…æ ‡å‡†ç»„ç»‡](#92-å›½é™…æ ‡å‡†ç»„ç»‡)
    - [9.3 æœ€æ–°ç ”ç©¶è¿›å±•ï¼ˆ2024-2025ï¼‰/ Latest Research Progress (2024-2025)](#93-æœ€æ–°ç ”ç©¶è¿›å±•2024-2025-latest-research-progress-2024-2025)
  - [10. å‚è€ƒæ–‡çŒ® / References](#10-å‚è€ƒæ–‡çŒ®--references)
    - [10.1 ç»å…¸æ–‡çŒ®](#101-ç»å…¸æ–‡çŒ®)
    - [10.2 æœ€æ–°ç ”ç©¶è®ºæ–‡](#102-æœ€æ–°ç ”ç©¶è®ºæ–‡)
    - [10.3 ç¤¾ä¼šç½‘ç»œä¸“è‘—](#103-ç¤¾ä¼šç½‘ç»œä¸“è‘—)
    - [10.4 åœ¨çº¿èµ„æº](#104-åœ¨çº¿èµ„æº)
  - [ğŸ’¼ **11. å®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹ / Real-World Engineering Application Cases**](#-11-å®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹--real-world-engineering-application-cases)
    - [11.1 ç¤¾äº¤åª’ä½“ç½‘ç»œåº”ç”¨ / Social Media Network Applications](#111-ç¤¾äº¤åª’ä½“ç½‘ç»œåº”ç”¨--social-media-network-applications)
      - [11.1.1 Facebookç¤¾äº¤ç½‘ç»œåˆ†æ](#1111-facebookç¤¾äº¤ç½‘ç»œåˆ†æ)
      - [11.1.2 Twitterä¿¡æ¯ä¼ æ’­åˆ†æ](#1112-twitterä¿¡æ¯ä¼ æ’­åˆ†æ)
    - [11.2 æ¨èç³»ç»Ÿåº”ç”¨ / Recommendation System Applications](#112-æ¨èç³»ç»Ÿåº”ç”¨--recommendation-system-applications)
      - [11.2.1 ç”µå•†æ¨èç³»ç»Ÿ](#1121-ç”µå•†æ¨èç³»ç»Ÿ)
      - [11.2.2 å†…å®¹æ¨èç³»ç»Ÿ](#1122-å†…å®¹æ¨èç³»ç»Ÿ)
    - [11.3 ç¤¾ä¼šç½‘ç»œå·¥å…·ä¸åº”ç”¨ / Social Network Tools and Applications](#113-ç¤¾ä¼šç½‘ç»œå·¥å…·ä¸åº”ç”¨--social-network-tools-and-applications)
      - [11.3.1 ä¸»æµç¤¾ä¼šç½‘ç»œå·¥å…·](#1131-ä¸»æµç¤¾ä¼šç½‘ç»œå·¥å…·)
      - [11.3.2 å®é™…åº”ç”¨æ¡ˆä¾‹](#1132-å®é™…åº”ç”¨æ¡ˆä¾‹)
  - [ğŸš€ **12. æœ€æ–°ç ”ç©¶è¿›å±•è¯¦ç»†å†…å®¹ï¼ˆ2024-2025ï¼‰/ Latest Research Progress Details (2024-2025)**](#-12-æœ€æ–°ç ”ç©¶è¿›å±•è¯¦ç»†å†…å®¹2024-2025-latest-research-progress-details-2024-2025)
    - [12.1 å¤§è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç½‘ç»œåˆ†æä¸­çš„åº”ç”¨](#121-å¤§è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç½‘ç»œåˆ†æä¸­çš„åº”ç”¨)
      - [LLMé©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œåˆ†æ](#llmé©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œåˆ†æ)
    - [12.2 å®æ—¶ç¤¾äº¤ç½‘ç»œç›‘æµ‹](#122-å®æ—¶ç¤¾äº¤ç½‘ç»œç›‘æµ‹)
      - [åŠ¨æ€ç¤¾äº¤ç½‘ç»œåˆ†æ](#åŠ¨æ€ç¤¾äº¤ç½‘ç»œåˆ†æ)
    - [12.3 éšç§ä¿æŠ¤çš„ç¤¾äº¤ç½‘ç»œåˆ†æ](#123-éšç§ä¿æŠ¤çš„ç¤¾äº¤ç½‘ç»œåˆ†æ)
      - [å·®åˆ†éšç§åœ¨ç½‘ç»œåˆ†æä¸­çš„åº”ç”¨](#å·®åˆ†éšç§åœ¨ç½‘ç»œåˆ†æä¸­çš„åº”ç”¨)
  - [ğŸ“ **13. æ€»ç»“ / Summary**](#-13-æ€»ç»“--summary)

---

## å†å²èƒŒæ™¯ / Historical Background

- **1930å¹´ä»£**ï¼šMorenoæå‡ºç¤¾ä¼šè®¡é‡å­¦
- **1950å¹´ä»£**ï¼šMilgramè¿›è¡Œå°ä¸–ç•Œå®éªŒ
- **1960å¹´ä»£**ï¼šGranovetteræå‡ºå¼±è¿æ¥ç†è®º
- **1970å¹´ä»£**ï¼šBurtæå‡ºç»“æ„æ´ç†è®º
- **1980å¹´ä»£**ï¼šWattså’ŒStrogatzå‘ç°å°ä¸–ç•Œç½‘ç»œ
- **1990å¹´ä»£**ï¼šBarabÃ¡siå’ŒAlbertå‘ç°æ— æ ‡åº¦ç½‘ç»œ
- **2000å¹´ä»£**ï¼šç¤¾äº¤åª’ä½“ç½‘ç»œå…´èµ·
- **2010å¹´ä»£**ï¼šå¤§æ•°æ®é©±åŠ¨çš„ç¤¾ä¼šç½‘ç»œåˆ†æ
- **2020å¹´ä»£**ï¼šAIå’Œæœºå™¨å­¦ä¹ åœ¨ç¤¾ä¼šç½‘ç»œä¸­çš„åº”ç”¨
- **2024-2025å¹´**ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç½‘ç»œåˆ†æä¸­çš„åº”ç”¨ï¼Œå®æ—¶ç¤¾äº¤ç½‘ç»œç›‘æµ‹ï¼Œéšç§ä¿æŠ¤çš„ç½‘ç»œåˆ†æï¼Œå¤šæ¨¡æ€ç¤¾äº¤ç½‘ç»œ

## åº”ç”¨é¢†åŸŸ

- **ç¤¾äº¤åª’ä½“åˆ†æ**ï¼šç”¨æˆ·è¡Œä¸ºã€ä¿¡æ¯ä¼ æ’­ã€å½±å“åŠ›åˆ†æ
- **ç»„ç»‡ç½‘ç»œ**ï¼šä¼ä¸šå…³ç³»ã€åˆä½œç½‘ç»œã€çŸ¥è¯†ä¼ æ’­
- **å­¦æœ¯ç½‘ç»œ**ï¼šåˆä½œç½‘ç»œã€å¼•ç”¨ç½‘ç»œã€çŸ¥è¯†å›¾è°±
- **æ”¿æ²»ç½‘ç»œ**ï¼šæ”¿ç­–ç½‘ç»œã€åˆ©ç›Šé›†å›¢ã€æ”¿æ²»å½±å“
- **ç»æµç½‘ç»œ**ï¼šè´¸æ˜“ç½‘ç»œã€é‡‘èç½‘ç»œã€ä¾›åº”é“¾ç½‘ç»œ

## 1. ç¤¾ä¼šç½‘ç»œåŸºæœ¬æ¦‚å¿µ / Basic Concepts of Social Networks

### 1.0 ç¤¾ä¼šç½‘ç»œåˆ†ææ€ç»´å¯¼å›¾ / Social Network Analysis Mind Map

```text
ç¤¾ä¼šç½‘ç»œåˆ†æ
â”œâ”€â”€ åŸºæœ¬æ¦‚å¿µ
â”‚   â”œâ”€â”€ èŠ‚ç‚¹ï¼ˆè¡ŒåŠ¨è€…ï¼‰
â”‚   â”œâ”€â”€ è¾¹ï¼ˆå…³ç³»ï¼‰
â”‚   â””â”€â”€ ç½‘ç»œç»“æ„
â”‚
â”œâ”€â”€ ç½‘ç»œåº¦é‡
â”‚   â”œâ”€â”€ ä¸­å¿ƒæ€§
â”‚   â”‚   â”œâ”€â”€ åº¦ä¸­å¿ƒæ€§
â”‚   â”‚   â”œâ”€â”€ æ¥è¿‘ä¸­å¿ƒæ€§
â”‚   â”‚   â”œâ”€â”€ ä»‹æ•°ä¸­å¿ƒæ€§
â”‚   â”‚   â””â”€â”€ ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
â”‚   â””â”€â”€ ç»“æ„åº¦é‡
â”‚       â”œâ”€â”€ èšç±»ç³»æ•°
â”‚       â”œâ”€â”€ è·¯å¾„é•¿åº¦
â”‚       â””â”€â”€ ç½‘ç»œå¯†åº¦
â”‚
â”œâ”€â”€ ç¤¾åŒºæ£€æµ‹
â”‚   â”œâ”€â”€ æ¨¡å—åº¦ä¼˜åŒ–
â”‚   â”œâ”€â”€ å±‚æ¬¡èšç±»
â”‚   â””â”€â”€ è°±æ–¹æ³•
â”‚
â”œâ”€â”€ ç½‘ç»œæ¼”åŒ–
â”‚   â”œâ”€â”€ æ—¶é—´åºåˆ—åˆ†æ
â”‚   â”œâ”€â”€ æ¼”åŒ–æ¨¡å‹
â”‚   â””â”€â”€ åŠ¨æ€ç½‘ç»œ
â”‚
â””â”€â”€ åº”ç”¨é¢†åŸŸ
    â”œâ”€â”€ ç¤¾äº¤åª’ä½“åˆ†æ
    â”œâ”€â”€ æ¨èç³»ç»Ÿ
    â””â”€â”€ å½±å“åŠ›ä¼ æ’­
```

### 1.1 åŸºæœ¬å®šä¹‰ / Basic Definitions

**å®šä¹‰ 1.1** (ç¤¾ä¼šç½‘ç»œ / Social Network)
**ç¤¾ä¼šç½‘ç»œ**æ˜¯ç”±ç¤¾ä¼šå®ä½“åŠå…¶å…³ç³»ç»„æˆçš„ç½‘ç»œç»“æ„ï¼š
$$\mathcal{SN} = \langle \mathcal{A}, \mathcal{R}, \mathcal{W}, \mathcal{T}, \mathcal{M} \rangle$$

å…¶ä¸­ï¼š

- $\mathcal{A}$ æ˜¯è¡ŒåŠ¨è€…é›† (Actors)
- $\mathcal{R}$ æ˜¯å…³ç³»é›† (Relations)
- $\mathcal{W}$ æ˜¯æƒé‡é›† (Weights)
- $\mathcal{T}$ æ˜¯æ—¶é—´é›† (Time)
- $\mathcal{M}$ æ˜¯å…ƒæ•°æ®é›† (Metadata)

**å½¢å¼åŒ–å®šä¹‰**ï¼š

- **é™æ€ç½‘ç»œ**ï¼š$\mathcal{SN}_S = \langle \mathcal{A}, \mathcal{R}, \mathcal{W} \rangle$
- **åŠ¨æ€ç½‘ç»œ**ï¼š$\mathcal{SN}_D = \langle \mathcal{A}(t), \mathcal{R}(t), \mathcal{W}(t) \rangle$
- **å¤šå±‚ç½‘ç»œ**ï¼š$\mathcal{SN}_M = \langle \mathcal{A}, \{\mathcal{R}_i\}, \{\mathcal{W}_i\} \rangle$
- **è¶…ç½‘ç»œ**ï¼š$\mathcal{SN}_H = \langle \mathcal{A}, \mathcal{E}, \mathcal{W} \rangle$ï¼Œå…¶ä¸­ $\mathcal{E}$ æ˜¯è¶…è¾¹é›†

**ç½‘ç»œç±»å‹åˆ†ç±»**ï¼š

- **æŒ‰å…³ç³»ç±»å‹**ï¼šå‹è°Šç½‘ç»œã€åˆä½œç½‘ç»œã€é€šä¿¡ç½‘ç»œã€å¼•ç”¨ç½‘ç»œ
- **æŒ‰æ—¶é—´ç‰¹æ€§**ï¼šé™æ€ç½‘ç»œã€åŠ¨æ€ç½‘ç»œã€æ—¶é—´åºåˆ—ç½‘ç»œ
- **æŒ‰ç»“æ„ç‰¹æ€§**ï¼šåŒè´¨ç½‘ç»œã€å¼‚è´¨ç½‘ç»œã€å¤šå±‚ç½‘ç»œ
- **æŒ‰è§„æ¨¡ç‰¹æ€§**ï¼šå°è§„æ¨¡ç½‘ç»œã€å¤§è§„æ¨¡ç½‘ç»œã€è¶…å¤§è§„æ¨¡ç½‘ç»œ

**å®šä¹‰ 1.2** (ç¤¾ä¼šç½‘ç»œå›¾)
**ç¤¾ä¼šç½‘ç»œå›¾**æ˜¯è¡¨ç¤ºç¤¾ä¼šç½‘ç»œçš„æœ‰å‘æˆ–æ— å‘å›¾ï¼š
$$G = (V, E, W)$$

å…¶ä¸­ï¼š

- $V$ æ˜¯é¡¶ç‚¹é›†ï¼Œè¡¨ç¤ºç¤¾ä¼šå®ä½“
- $E$ æ˜¯è¾¹é›†ï¼Œè¡¨ç¤ºç¤¾ä¼šå…³ç³»
- $W: E \to \mathbb{R}$ æ˜¯æƒé‡å‡½æ•°

**å½¢å¼åŒ–è¯­ä¹‰**ï¼š

- é›†åˆè®ºè¯­ä¹‰ï¼š$\mathcal{A} \neq \emptyset, \mathcal{R} \subseteq \mathcal{A} \times \mathcal{A}, \mathcal{W}: \mathcal{R} \to \mathbb{R}$
- èŒƒç•´è®ºè¯­ä¹‰ï¼šç¤¾ä¼šç½‘ç»œä½œä¸ºå›¾èŒƒç•´ä¸­çš„å¯¹è±¡ï¼Œç½‘ç»œæ¼”åŒ–ä½œä¸ºæ€å°„
- è‡ªåŠ¨æœºè¯­ä¹‰ï¼šç¤¾ä¼šç½‘ç»œå¯å»ºæ¨¡ä¸ºçŠ¶æ€è‡ªåŠ¨æœº $A = (Q, \Sigma, \delta, q_0, F)$

### 1.2 ç½‘ç»œè¡¨ç¤º

**å®šä¹‰ 1.3** (é‚»æ¥çŸ©é˜µ)
**é‚»æ¥çŸ©é˜µ**æ˜¯è¡¨ç¤ºç½‘ç»œè¿æ¥çš„çŸ©é˜µï¼š
$$
A_{ij} = \begin{cases}
w_{ij} & \text{å¦‚æœå­˜åœ¨è¾¹ } (i,j) \\
0 & \text{å¦åˆ™}
\end{cases}
$$

**å®šä¹‰ 1.4** (å…³è”çŸ©é˜µ)
**å…³è”çŸ©é˜µ**æ˜¯è¡¨ç¤ºèŠ‚ç‚¹ä¸è¾¹å…³ç³»çš„çŸ©é˜µï¼š
$$
B_{ij} = \begin{cases}
1 & \text{å¦‚æœèŠ‚ç‚¹ } i \text{ ä¸è¾¹ } j \text{ å…³è”} \\
0 & \text{å¦åˆ™}
\end{cases}
$$

**ç®—æ³• 1.1** (ç¤¾ä¼šç½‘ç»œæ„å»º)

```python
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

class SocialNetwork:
    def __init__(self, num_actors, directed=False):
        self.num_actors = num_actors
        self.directed = directed

        # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µ
        self.adjacency_matrix = np.zeros((num_actors, num_actors))
        self.weights = {}
        self.attributes = {}

        # ä½¿ç”¨NetworkXå›¾
        if directed:
            self.graph = nx.DiGraph()
        else:
            self.graph = nx.Graph()

    def add_actor(self, actor_id, attributes=None):
        """æ·»åŠ è¡ŒåŠ¨è€…"""
        self.graph.add_node(actor_id)
        if attributes:
            self.attributes[actor_id] = attributes
            self.graph.nodes[actor_id].update(attributes)

    def add_relation(self, actor1, actor2, weight=1.0, relation_type=None):
        """æ·»åŠ å…³ç³»"""
        # æ›´æ–°é‚»æ¥çŸ©é˜µ
        self.adjacency_matrix[actor1, actor2] = weight
        if not self.directed:
            self.adjacency_matrix[actor2, actor1] = weight

        # æ›´æ–°NetworkXå›¾
        edge_data = {'weight': weight}
        if relation_type:
            edge_data['type'] = relation_type

        self.graph.add_edge(actor1, actor2, **edge_data)
        self.weights[(actor1, actor2)] = weight

    def remove_relation(self, actor1, actor2):
        """ç§»é™¤å…³ç³»"""
        self.adjacency_matrix[actor1, actor2] = 0
        if not self.directed:
            self.adjacency_matrix[actor2, actor1] = 0

        if self.graph.has_edge(actor1, actor2):
            self.graph.remove_edge(actor1, actor2)

        if (actor1, actor2) in self.weights:
            del self.weights[(actor1, actor2)]

    def get_neighbors(self, actor):
        """è·å–é‚»å±…"""
        return list(self.graph.neighbors(actor))

    def get_degree(self, actor):
        """è·å–åº¦"""
        return self.graph.degree(actor)

    def get_network_density(self):
        """è®¡ç®—ç½‘ç»œå¯†åº¦"""
        return nx.density(self.graph)

    def get_average_clustering(self):
        """è®¡ç®—å¹³å‡èšç±»ç³»æ•°"""
        return nx.average_clustering(self.graph)

    def get_average_shortest_path(self):
        """è®¡ç®—å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦"""
        if nx.is_connected(self.graph):
            return nx.average_shortest_path_length(self.graph)
        else:
            # å¯¹äºä¸è¿é€šç½‘ç»œï¼Œè®¡ç®—æœ€å¤§è¿é€šåˆ†é‡çš„å¹³å‡è·¯å¾„é•¿åº¦
            largest_cc = max(nx.connected_components(self.graph), key=len)
            subgraph = self.graph.subgraph(largest_cc)
            return nx.average_shortest_path_length(subgraph)
```

## 2. ç½‘ç»œåº¦é‡

### 2.0 ä¸­å¿ƒæ€§åº¦é‡å¯¹æ¯”çŸ©é˜µ / Centrality Measures Comparison Matrix

| ä¸­å¿ƒæ€§ç±»å‹ | å®šä¹‰ | è®¡ç®—å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å–å€¼èŒƒå›´ |
|-----------|------|-----------|---------|------|------|---------|
| **åº¦ä¸­å¿ƒæ€§** | $C_D(i) = \frac{k_i}{N-1}$ | $O(N)$ | å±€éƒ¨é‡è¦æ€§ | è®¡ç®—å¿«ã€ç›´è§‚ | å¿½ç•¥å…¨å±€ç»“æ„ | $[0, 1]$ |
| **æ¥è¿‘ä¸­å¿ƒæ€§** | $C_C(i) = \frac{N-1}{\sum_{j \neq i} d(i,j)}$ | $O(N^2 + NE)$ | ä¿¡æ¯ä¼ æ’­ä¸­å¿ƒ | è€ƒè™‘å…¨å±€è·ç¦» | éœ€è¦è¿é€šå›¾ | $[0, 1]$ |
| **ä»‹æ•°ä¸­å¿ƒæ€§** | $C_B(i) = \sum_{s \neq t \neq i} \frac{\sigma_{st}(i)}{\sigma_{st}}$ | $O(NE)$ | æ¡¥æ¢èŠ‚ç‚¹ | è¯†åˆ«å…³é”®è·¯å¾„ | è®¡ç®—å¤æ‚ | $[0, 1]$ |
| **ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§** | $Ax = \lambda x$ | $O(N^2)$ | å½±å“åŠ›ä¼ æ’­ | è€ƒè™‘é‚»å±…é‡è¦æ€§ | éœ€è¦è¿­ä»£ | $[0, 1]$ |
| **PageRank** | $PR(i) = \frac{1-d}{N} + d \sum_{j \in M(i)} \frac{PR(j)}{L(j)}$ | $O(NE)$ | Webæ’åã€å½±å“åŠ› | è€ƒè™‘é“¾æ¥è´¨é‡ | éœ€è¦è¿­ä»£ | $[0, 1]$ |
| **Katzä¸­å¿ƒæ€§** | $C_K(i) = \sum_{k=1}^{\infty} \sum_{j=1}^{N} \alpha^k (A^k)_{ji}$ | $O(N^3)$ | è·¯å¾„åŠ æƒé‡è¦æ€§ | è€ƒè™‘æ‰€æœ‰è·¯å¾„ | è®¡ç®—å¤æ‚ | $[0, \infty)$ |

**ç¬¦å·è¯´æ˜**ï¼š

- $N$ï¼šèŠ‚ç‚¹æ•°
- $E$ï¼šè¾¹æ•°
- $k_i$ï¼šèŠ‚ç‚¹ $i$ çš„åº¦
- $d(i,j)$ï¼šèŠ‚ç‚¹ $i$ å’Œ $j$ ä¹‹é—´çš„æœ€çŸ­è·¯å¾„é•¿åº¦
- $\sigma_{st}(i)$ï¼šç»è¿‡èŠ‚ç‚¹ $i$ çš„æœ€çŸ­è·¯å¾„æ•°
- $\sigma_{st}$ï¼šæ€»æœ€çŸ­è·¯å¾„æ•°
- $A$ï¼šé‚»æ¥çŸ©é˜µ
- $\lambda$ï¼šç‰¹å¾å€¼
- $d$ï¼šé˜»å°¼ç³»æ•°ï¼ˆPageRankï¼‰
- $\alpha$ï¼šè¡°å‡å› å­ï¼ˆKatzï¼‰

### 2.1 ä¸­å¿ƒæ€§åº¦é‡

**å®šä¹‰ 2.1** (åº¦ä¸­å¿ƒæ€§)
**åº¦ä¸­å¿ƒæ€§**è¡¡é‡èŠ‚ç‚¹çš„è¿æ¥æ•°é‡ï¼š
$$C_D(i) = \frac{k_i}{N-1}$$

å…¶ä¸­ $k_i$ æ˜¯èŠ‚ç‚¹ $i$ çš„åº¦ï¼Œ$N$ æ˜¯ç½‘ç»œèŠ‚ç‚¹æ•°ã€‚

**å®šä¹‰ 2.2** (æ¥è¿‘ä¸­å¿ƒæ€§)
**æ¥è¿‘ä¸­å¿ƒæ€§**è¡¡é‡èŠ‚ç‚¹åˆ°å…¶ä»–èŠ‚ç‚¹çš„å¹³å‡è·ç¦»ï¼š
$$C_C(i) = \frac{N-1}{\sum_{j \neq i} d(i,j)}$$

å…¶ä¸­ $d(i,j)$ æ˜¯èŠ‚ç‚¹ $i$ å’Œ $j$ ä¹‹é—´çš„æœ€çŸ­è·¯å¾„é•¿åº¦ã€‚

**å®šä¹‰ 2.3** (ä»‹æ•°ä¸­å¿ƒæ€§)
**ä»‹æ•°ä¸­å¿ƒæ€§**è¡¡é‡èŠ‚ç‚¹ä½œä¸ºæ¡¥æ¢çš„é‡è¦æ€§ï¼š
$$C_B(i) = \sum_{s \neq t \neq i} \frac{\sigma_{st}(i)}{\sigma_{st}}$$

å…¶ä¸­ $\sigma_{st}$ æ˜¯èŠ‚ç‚¹ $s$ åˆ° $t$ çš„æœ€çŸ­è·¯å¾„æ•°ï¼Œ$\sigma_{st}(i)$ æ˜¯ç»è¿‡èŠ‚ç‚¹ $i$ çš„æœ€çŸ­è·¯å¾„æ•°ã€‚

**ç®—æ³• 2.1** (ä¸­å¿ƒæ€§è®¡ç®—)

```python
class CentralityAnalyzer:
    def __init__(self, network):
        self.network = network

    def calculate_degree_centrality(self):
        """è®¡ç®—åº¦ä¸­å¿ƒæ€§"""
        return nx.degree_centrality(self.network.graph)

    def calculate_closeness_centrality(self):
        """è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§"""
        return nx.closeness_centrality(self.network.graph)

    def calculate_betweenness_centrality(self):
        """è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§"""
        return nx.betweenness_centrality(self.network.graph)

    def calculate_eigenvector_centrality(self):
        """è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§"""
        return nx.eigenvector_centrality(self.network.graph, max_iter=1000)

    def calculate_pagerank(self, alpha=0.85):
        """è®¡ç®—PageRank"""
        return nx.pagerank(self.network.graph, alpha=alpha)

    def get_top_central_actors(self, centrality_type='degree', top_k=10):
        """è·å–æœ€ä¸­å¿ƒçš„è¡ŒåŠ¨è€…"""
        if centrality_type == 'degree':
            centrality = self.calculate_degree_centrality()
        elif centrality_type == 'closeness':
            centrality = self.calculate_closeness_centrality()
        elif centrality_type == 'betweenness':
            centrality = self.calculate_betweenness_centrality()
        elif centrality_type == 'eigenvector':
            centrality = self.calculate_eigenvector_centrality()
        elif centrality_type == 'pagerank':
            centrality = self.calculate_pagerank()
        else:
            raise ValueError(f"Unknown centrality type: {centrality_type}")

        # æ’åºå¹¶è¿”å›å‰kä¸ª
        sorted_actors = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        return sorted_actors[:top_k]
```

### 2.2 ç½‘ç»œç»“æ„åº¦é‡

**å®šä¹‰ 2.4** (ç½‘ç»œå¯†åº¦)
**ç½‘ç»œå¯†åº¦**æ˜¯ç½‘ç»œä¸­å®é™…è¿æ¥æ•°ä¸å¯èƒ½è¿æ¥æ•°çš„æ¯”å€¼ï¼š
$$\rho = \frac{2|E|}{|V|(|V|-1)}$$

**å®šä¹‰ 2.5** (èšç±»ç³»æ•°)
**èšç±»ç³»æ•°**è¡¡é‡ç½‘ç»œçš„èšé›†ç¨‹åº¦ï¼š
$$C = \frac{1}{N} \sum_{i=1}^{N} C_i$$

å…¶ä¸­ $C_i$ æ˜¯èŠ‚ç‚¹ $i$ çš„å±€éƒ¨èšç±»ç³»æ•°ã€‚

**ç®—æ³• 2.2** (ç½‘ç»œç»“æ„åˆ†æ)

```python
class NetworkStructureAnalyzer:
    def __init__(self, network):
        self.network = network

    def analyze_network_structure(self):
        """åˆ†æç½‘ç»œç»“æ„"""
        analysis = {}

        # åŸºæœ¬ç»Ÿè®¡
        analysis['num_nodes'] = self.network.graph.number_of_nodes()
        analysis['num_edges'] = self.network.graph.number_of_edges()
        analysis['density'] = nx.density(self.network.graph)

        # è¿é€šæ€§
        analysis['is_connected'] = nx.is_connected(self.network.graph)
        analysis['num_components'] = nx.number_connected_components(self.network.graph)

        # èšç±»ç³»æ•°
        analysis['average_clustering'] = nx.average_clustering(self.network.graph)
        analysis['clustering_coefficients'] = nx.clustering(self.network.graph)

        # è·¯å¾„é•¿åº¦
        if analysis['is_connected']:
            analysis['average_shortest_path'] = nx.average_shortest_path_length(self.network.graph)
            analysis['diameter'] = nx.diameter(self.network.graph)
        else:
            # è®¡ç®—æœ€å¤§è¿é€šåˆ†é‡çš„æŒ‡æ ‡
            largest_cc = max(nx.connected_components(self.network.graph), key=len)
            subgraph = self.network.graph.subgraph(largest_cc)
            analysis['largest_component_size'] = len(largest_cc)
            analysis['largest_component_diameter'] = nx.diameter(subgraph)

        # åº¦åˆ†å¸ƒ
        degrees = [d for n, d in self.network.graph.degree()]
        analysis['average_degree'] = np.mean(degrees)
        analysis['degree_variance'] = np.var(degrees)
        analysis['degree_distribution'] = degrees

        return analysis

    def analyze_degree_distribution(self):
        """åˆ†æåº¦åˆ†å¸ƒ"""
        degrees = [d for n, d in self.network.graph.degree()]

        # è®¡ç®—åº¦åˆ†å¸ƒ
        unique_degrees, counts = np.unique(degrees, return_counts=True)
        degree_distribution = dict(zip(unique_degrees, counts))

        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒ
        cumulative_distribution = {}
        total_nodes = len(degrees)
        cumulative_count = 0

        for degree in sorted(unique_degrees):
            cumulative_count += degree_distribution[degree]
            cumulative_distribution[degree] = cumulative_count / total_nodes

        return {
            'degree_distribution': degree_distribution,
            'cumulative_distribution': cumulative_distribution,
            'average_degree': np.mean(degrees),
            'median_degree': np.median(degrees),
            'max_degree': np.max(degrees),
            'min_degree': np.min(degrees)
        }

    def check_scale_free_property(self):
        """æ£€æŸ¥æ— æ ‡åº¦ç‰¹æ€§"""
        degree_dist = self.analyze_degree_distribution()

        # è®¡ç®—å¹‚å¾‹æŒ‡æ•°
        degrees = list(degree_dist['degree_distribution'].keys())
        counts = list(degree_dist['degree_distribution'].values())

        # è¿‡æ»¤æ‰åº¦ä¸º0çš„èŠ‚ç‚¹
        valid_indices = [i for i, d in enumerate(degrees) if d > 0]
        valid_degrees = [degrees[i] for i in valid_indices]
        valid_counts = [counts[i] for i in valid_indices]

        if len(valid_degrees) < 2:
            return None

        # çº¿æ€§å›å½’æ‹Ÿåˆå¹‚å¾‹
        log_degrees = np.log(valid_degrees)
        log_counts = np.log(valid_counts)

        # è®¡ç®—RÂ²
        correlation_matrix = np.corrcoef(log_degrees, log_counts)
        r_squared = correlation_matrix[0, 1] ** 2

        return {
            'r_squared': r_squared,
            'is_scale_free': r_squared > 0.8,  # é˜ˆå€¼å¯è°ƒæ•´
            'log_degrees': log_degrees,
            'log_counts': log_counts
        }
```

## 3. ç¤¾åŒºæ£€æµ‹

### 3.1 ç¤¾åŒºå®šä¹‰

**å®šä¹‰ 3.1** (ç¤¾åŒº)
**ç¤¾åŒº**æ˜¯ç½‘ç»œä¸­è¿æ¥å¯†é›†çš„èŠ‚ç‚¹å­é›†ï¼š
$$C \subseteq V \text{ æ˜¯ç¤¾åŒºï¼Œå¦‚æœ } \frac{|E(C)|}{|C|(|C|-1)/2} > \frac{|E|}{|V|(|V|-1)/2}$$

å…¶ä¸­ $E(C)$ æ˜¯ç¤¾åŒº $C$ å†…éƒ¨çš„è¾¹é›†ã€‚

**å®šä¹‰ 3.2** (æ¨¡å—åº¦)
**æ¨¡å—åº¦**è¡¡é‡ç¤¾åŒºåˆ’åˆ†çš„è´¨é‡ï¼š
$$Q = \frac{1}{2m} \sum_{ij} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j)$$

å…¶ä¸­ $m$ æ˜¯æ€»è¾¹æ•°ï¼Œ$k_i$ æ˜¯èŠ‚ç‚¹ $i$ çš„åº¦ï¼Œ$c_i$ æ˜¯èŠ‚ç‚¹ $i$ çš„ç¤¾åŒºæ ‡ç­¾ã€‚

**å®šç† 3.1** (æ¨¡å—åº¦æœ€ä¼˜æ€§ / Modularity Optimality)
æ¨¡å—åº¦ $Q$ çš„å–å€¼èŒƒå›´ä¸º $[-1, 1]$ï¼Œä¸”ï¼š

- $Q = 1$ å½“ä¸”ä»…å½“ç½‘ç»œå®Œå…¨åˆ’åˆ†ä¸ºç¤¾åŒºï¼ˆç¤¾åŒºå†…éƒ¨å®Œå…¨è¿æ¥ï¼Œç¤¾åŒºä¹‹é—´æ— è¿æ¥ï¼‰
- $Q = 0$ å¯¹åº”éšæœºç½‘ç»œæœŸæœ›å€¼
- $Q < 0$ è¡¨ç¤ºç¤¾åŒºåˆ’åˆ†è´¨é‡å·®äºéšæœºåˆ’åˆ†

**å½¢å¼åŒ–è¯æ˜ / Formal Proof**ï¼š

**æ­¥éª¤ 1**ï¼šæ¨¡å—åº¦ä¸‹ç•Œ
å¯¹äºä»»æ„ç¤¾åŒºåˆ’åˆ†ï¼Œæœ‰ï¼š
$$Q = \frac{1}{2m} \sum_{ij} \left[A_{ij} - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j) \geq \frac{1}{2m} \sum_{ij} (-1) \cdot 1 = -\frac{N(N-1)}{4m}$$

å½“ç½‘ç»œä¸ºç©ºå›¾æ—¶ï¼Œ$Q = -1$ï¼ˆæé™æƒ…å†µï¼‰ã€‚

**æ­¥éª¤ 2**ï¼šæ¨¡å—åº¦ä¸Šç•Œ
å¯¹äºå®Œå…¨åˆ’åˆ†ä¸ºç¤¾åŒºçš„ç½‘ç»œï¼Œç¤¾åŒºå†…éƒ¨è¾¹æ•°ä¸º $\sum_c \binom{|C_c|}{2}$ï¼Œç¤¾åŒºä¹‹é—´æ— è¾¹ï¼š
$$Q = \frac{1}{2m} \sum_{c} \sum_{i,j \in C_c} \left[1 - \frac{k_i k_j}{2m}\right]$$

å½“ç¤¾åŒºå†…éƒ¨å®Œå…¨è¿æ¥ä¸”ç¤¾åŒºä¹‹é—´æ— è¿æ¥æ—¶ï¼Œ$Q$ è¾¾åˆ°æœ€å¤§å€¼ï¼Œæ¥è¿‘ $1$ã€‚

**æ­¥éª¤ 3**ï¼šéšæœºç½‘ç»œæœŸæœ›
å¯¹äºéšæœºç½‘ç»œï¼ŒæœŸæœ›æ¨¡å—åº¦ä¸ºï¼š
$$\mathbb{E}[Q] = \frac{1}{2m} \sum_{ij} \left[\mathbb{E}[A_{ij}] - \frac{k_i k_j}{2m}\right] \delta(c_i, c_j) = 0$$

å› ä¸º $\mathbb{E}[A_{ij}] = \frac{k_i k_j}{2m}$ï¼ˆé…ç½®æ¨¡å‹ï¼‰ã€‚$\square$

**ç®—æ³• 3.1** (Louvainç®—æ³•)

```python
class CommunityDetector:
    def __init__(self, network):
        self.network = network

    def louvain_community_detection(self):
        """Louvainç¤¾åŒºæ£€æµ‹ç®—æ³•"""
        # åˆå§‹åŒ–ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ªç¤¾åŒº
        communities = {node: i for i, node in enumerate(self.network.graph.nodes())}

        # è®¡ç®—æ¨¡å—åº¦å¢ç›Š
        def calculate_modularity_gain(node, new_community):
            """è®¡ç®—å°†èŠ‚ç‚¹ç§»åŠ¨åˆ°æ–°ç¤¾åŒºçš„æ¨¡å—åº¦å¢ç›Š"""
            current_community = communities[node]

            # è®¡ç®—å½“å‰æ¨¡å—åº¦
            current_modularity = self.calculate_modularity(communities)

            # ä¸´æ—¶ç§»åŠ¨èŠ‚ç‚¹
            communities[node] = new_community
            new_modularity = self.calculate_modularity(communities)

            # æ¢å¤åŸçŠ¶æ€
            communities[node] = current_community

            return new_modularity - current_modularity

        # ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨ä¼˜åŒ–
        improved = True
        while improved:
            improved = False

            for node in self.network.graph.nodes():
                best_gain = 0
                best_community = communities[node]

                # å°è¯•ç§»åŠ¨åˆ°é‚»å±…çš„ç¤¾åŒº
                neighbor_communities = set()
                for neighbor in self.network.graph.neighbors(node):
                    neighbor_communities.add(communities[neighbor])

                for community in neighbor_communities:
                    if community != communities[node]:
                        gain = calculate_modularity_gain(node, community)
                        if gain > best_gain:
                            best_gain = gain
                            best_community = community

                # å¦‚æœæ‰¾åˆ°æ›´å¥½çš„ç¤¾åŒºï¼Œåˆ™ç§»åŠ¨
                if best_gain > 0:
                    communities[node] = best_community
                    improved = True

        # ç¬¬äºŒé˜¶æ®µï¼šç¤¾åŒºèšåˆ
        # å°†åŒä¸€ç¤¾åŒºçš„èŠ‚ç‚¹èšåˆä¸ºè¶…çº§èŠ‚ç‚¹
        community_nodes = {}
        for node, community in communities.items():
            if community not in community_nodes:
                community_nodes[community] = []
            community_nodes[community].append(node)

        return communities, community_nodes

    def calculate_modularity(self, communities):
        """è®¡ç®—æ¨¡å—åº¦"""
        m = self.network.graph.number_of_edges()
        if m == 0:
            return 0

        modularity = 0
        for edge in self.network.graph.edges():
            node1, node2 = edge
            if communities[node1] == communities[node2]:
                k1 = self.network.graph.degree(node1)
                k2 = self.network.graph.degree(node2)
                modularity += 1 - (k1 * k2) / (2 * m)

        return modularity / (2 * m)

    def girvan_newman_algorithm(self):
        """Girvan-Newmanç¤¾åŒºæ£€æµ‹ç®—æ³•"""
        # è®¡ç®—æ‰€æœ‰è¾¹çš„ä»‹æ•°ä¸­å¿ƒæ€§
        edge_betweenness = nx.edge_betweenness_centrality(self.network.graph)

        # é€æ­¥ç§»é™¤ä»‹æ•°ä¸­å¿ƒæ€§æœ€é«˜çš„è¾¹
        communities = []
        graph_copy = self.network.graph.copy()

        while graph_copy.number_of_edges() > 0:
            # æ‰¾åˆ°ä»‹æ•°ä¸­å¿ƒæ€§æœ€é«˜çš„è¾¹
            edge_betweenness = nx.edge_betweenness_centrality(graph_copy)
            edge_to_remove = max(edge_betweenness, key=edge_betweenness.get)

            # ç§»é™¤è¾¹
            graph_copy.remove_edge(*edge_to_remove)

            # æ£€æµ‹è¿é€šåˆ†é‡
            components = list(nx.connected_components(graph_copy))
            if len(components) > len(communities):
                communities = components

        return communities
```

### 3.2 å±‚æ¬¡èšç±»

**ç®—æ³• 3.2** (å±‚æ¬¡èšç±»ç®—æ³•)

```python
class HierarchicalClustering:
    def __init__(self, network):
        self.network = network

    def hierarchical_community_detection(self, method='single'):
        """å±‚æ¬¡èšç±»ç¤¾åŒºæ£€æµ‹"""
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distance_matrix = self.calculate_distance_matrix()

        # æ‰§è¡Œå±‚æ¬¡èšç±»
        from scipy.cluster.hierarchy import linkage, fcluster

        # è®¡ç®—é“¾æ¥çŸ©é˜µ
        linkage_matrix = linkage(distance_matrix, method=method)

        # ç¡®å®šæœ€ä½³èšç±»æ•°
        optimal_clusters = self.find_optimal_clusters(linkage_matrix)

        # è·å–èšç±»ç»“æœ
        clusters = fcluster(linkage_matrix, optimal_clusters, criterion='maxclust')

        # è½¬æ¢ä¸ºç¤¾åŒºæ ¼å¼
        communities = {}
        for i, cluster_id in enumerate(clusters):
            node = list(self.network.graph.nodes())[i]
            communities[node] = cluster_id

        return communities

    def calculate_distance_matrix(self):
        """è®¡ç®—è·ç¦»çŸ©é˜µ"""
        nodes = list(self.network.graph.nodes())
        n = len(nodes)
        distance_matrix = np.zeros((n, n))

        for i in range(n):
            for j in range(i+1, n):
                # ä½¿ç”¨æœ€çŸ­è·¯å¾„è·ç¦»
                try:
                    distance = nx.shortest_path_length(self.network.graph, nodes[i], nodes[j])
                except nx.NetworkXNoPath:
                    distance = float('inf')

                distance_matrix[i, j] = distance
                distance_matrix[j, i] = distance

        return distance_matrix

    def find_optimal_clusters(self, linkage_matrix):
        """æ‰¾åˆ°æœ€ä½³èšç±»æ•°"""
        # ä½¿ç”¨è½®å»“ç³»æ•°æˆ–æ¨¡å—åº¦æ¥ç¡®å®šæœ€ä½³èšç±»æ•°
        max_clusters = min(10, len(self.network.graph.nodes()))
        best_score = -1
        best_clusters = 2

        for n_clusters in range(2, max_clusters + 1):
            from scipy.cluster.hierarchy import fcluster
            clusters = fcluster(linkage_matrix, n_clusters, criterion='maxclust')

            # è®¡ç®—æ¨¡å—åº¦
            communities = {}
            for i, cluster_id in enumerate(clusters):
                node = list(self.network.graph.nodes())[i]
                communities[node] = cluster_id

            modularity = self.calculate_modularity(communities)

            if modularity > best_score:
                best_score = modularity
                best_clusters = n_clusters

        return best_clusters

    def calculate_modularity(self, communities):
        """è®¡ç®—æ¨¡å—åº¦ï¼ˆå¤ç”¨ä¹‹å‰çš„å‡½æ•°ï¼‰"""
        detector = CommunityDetector(self.network)
        return detector.calculate_modularity(communities)
```

## 4. ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æ / Social Media Network Analysis

### 4.1 ç¤¾äº¤åª’ä½“ç½‘ç»œç‰¹å¾

**å®šä¹‰ 4.1** (ç¤¾äº¤åª’ä½“ç½‘ç»œ / Social Media Network)
**ç¤¾äº¤åª’ä½“ç½‘ç»œ**æ˜¯ç”¨æˆ·åœ¨ç¤¾äº¤åª’ä½“å¹³å°ä¸Šçš„äº’åŠ¨ç½‘ç»œï¼š
$$\mathcal{SMN} = \langle \mathcal{U}, \mathcal{I}, \mathcal{C}, \mathcal{T}, \mathcal{E} \rangle$$

å…¶ä¸­ï¼š

- $\mathcal{U}$ æ˜¯ç”¨æˆ·é›†
- $\mathcal{I}$ æ˜¯äº’åŠ¨é›†ï¼ˆç‚¹èµã€è¯„è®ºã€è½¬å‘ã€å…³æ³¨ï¼‰
- $\mathcal{C}$ æ˜¯å†…å®¹é›†
- $\mathcal{T}$ æ˜¯æ—¶é—´æˆ³é›†
- $\mathcal{E}$ æ˜¯æƒ…æ„Ÿé›†

**ç®—æ³• 4.1** (ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æ)

```python
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import seaborn as sns

class SocialMediaNetworkAnalyzer:
    """ç¤¾äº¤åª’ä½“ç½‘ç»œåˆ†æå™¨"""

    def __init__(self):
        self.users = {}
        self.interactions = []
        self.content = {}
        self.temporal_data = {}

    def add_user(self, user_id: str, attributes: dict = None):
        """æ·»åŠ ç”¨æˆ·"""
        self.users[user_id] = {
            'id': user_id,
            'followers': 0,
            'following': 0,
            'posts': 0,
            'engagement_rate': 0.0,
            'influence_score': 0.0,
            **attributes or {}
        }

    def add_interaction(self, user_id: str, target_id: str,
                       interaction_type: str, timestamp: datetime,
                       content_id: str = None, sentiment: float = 0.0):
        """æ·»åŠ äº’åŠ¨"""
        interaction = {
            'user_id': user_id,
            'target_id': target_id,
            'type': interaction_type,  # 'like', 'comment', 'share', 'follow'
            'timestamp': timestamp,
            'content_id': content_id,
            'sentiment': sentiment
        }
        self.interactions.append(interaction)

    def build_interaction_network(self, interaction_type: str = None) -> nx.Graph:
        """æ„å»ºäº’åŠ¨ç½‘ç»œ"""
        G = nx.Graph()

        # æ·»åŠ ç”¨æˆ·èŠ‚ç‚¹
        for user_id in self.users:
            G.add_node(user_id, **self.users[user_id])

        # æ·»åŠ äº’åŠ¨è¾¹
        for interaction in self.interactions:
            if interaction_type is None or interaction['type'] == interaction_type:
                user_id = interaction['user_id']
                target_id = interaction['target_id']

                if G.has_edge(user_id, target_id):
                    G[user_id][target_id]['weight'] += 1
                    G[user_id][target_id]['interactions'].append(interaction)
                else:
                    G.add_edge(user_id, target_id,
                              weight=1,
                              interactions=[interaction],
                              type=interaction['type'])

        return G

    def analyze_user_engagement(self) -> dict:
        """åˆ†æç”¨æˆ·å‚ä¸åº¦"""
        engagement_analysis = {}

        for user_id, user_data in self.users.items():
            # è®¡ç®—ç”¨æˆ·çš„äº’åŠ¨ç»Ÿè®¡
            user_interactions = [i for i in self.interactions
                               if i['user_id'] == user_id or i['target_id'] == user_id]

            # è®¡ç®—å‚ä¸åº¦æŒ‡æ ‡
            total_interactions = len(user_interactions)
            active_days = len(set(i['timestamp'].date() for i in user_interactions))
            avg_sentiment = np.mean([i['sentiment'] for i in user_interactions if i['sentiment'] != 0])

            engagement_analysis[user_id] = {
                'total_interactions': total_interactions,
                'active_days': active_days,
                'avg_sentiment': avg_sentiment,
                'engagement_rate': total_interactions / max(active_days, 1)
            }

        return engagement_analysis

    def analyze_content_virality(self, content_id: str) -> dict:
        """åˆ†æå†…å®¹ä¼ æ’­æ€§"""
        content_interactions = [i for i in self.interactions
                              if i['content_id'] == content_id]

        if not content_interactions:
            return {}

        # æŒ‰æ—¶é—´æ’åº
        content_interactions.sort(key=lambda x: x['timestamp'])

        # è®¡ç®—ä¼ æ’­æŒ‡æ ‡
        total_reach = len(set(i['user_id'] for i in content_interactions))
        total_engagement = len(content_interactions)

        # è®¡ç®—ä¼ æ’­é€Ÿåº¦
        time_span = (content_interactions[-1]['timestamp'] -
                    content_interactions[0]['timestamp']).total_seconds()
        velocity = total_engagement / max(time_span / 3600, 1)  # æ¯å°æ—¶äº’åŠ¨æ•°

        # è®¡ç®—ä¼ æ’­æ·±åº¦
        max_depth = self.calculate_propagation_depth(content_id)

        return {
            'content_id': content_id,
            'total_reach': total_reach,
            'total_engagement': total_engagement,
            'propagation_velocity': velocity,
            'max_depth': max_depth,
            'virality_score': velocity * max_depth
        }

    def calculate_propagation_depth(self, content_id: str) -> int:
        """è®¡ç®—ä¼ æ’­æ·±åº¦"""
        # æ„å»ºä¼ æ’­æ ‘
        propagation_tree = {}
        content_interactions = [i for i in self.interactions
                              if i['content_id'] == content_id]

        for interaction in content_interactions:
            user_id = interaction['user_id']
            if user_id not in propagation_tree:
                propagation_tree[user_id] = []
            propagation_tree[user_id].append(interaction)

        # è®¡ç®—æœ€å¤§æ·±åº¦
        def calculate_depth(user_id, visited=None):
            if visited is None:
                visited = set()

            if user_id in visited:
                return 0

            visited.add(user_id)
            max_depth = 0

            for interaction in propagation_tree.get(user_id, []):
                if interaction['type'] == 'share':
                    depth = calculate_depth(interaction['target_id'], visited) + 1
                    max_depth = max(max_depth, depth)

            return max_depth

        # æ‰¾åˆ°åŸå§‹å‘å¸ƒè€…
        original_poster = None
        for interaction in content_interactions:
            if interaction['type'] == 'post':
                original_poster = interaction['user_id']
                break

        if original_poster:
            return calculate_depth(original_poster)

        return 0

    def analyze_temporal_patterns(self) -> dict:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        temporal_analysis = {}

        # æŒ‰å°æ—¶åˆ†æ
        hourly_activity = {}
        for interaction in self.interactions:
            hour = interaction['timestamp'].hour
            hourly_activity[hour] = hourly_activity.get(hour, 0) + 1

        temporal_analysis['hourly_pattern'] = hourly_activity

        # æŒ‰æ˜ŸæœŸåˆ†æ
        weekly_activity = {}
        for interaction in self.interactions:
            weekday = interaction['timestamp'].weekday()
            weekly_activity[weekday] = weekly_activity.get(weekday, 0) + 1

        temporal_analysis['weekly_pattern'] = weekly_activity

        # è®¡ç®—å³°å€¼æ—¶é—´
        peak_hour = max(hourly_activity.items(), key=lambda x: x[1])[0]
        peak_weekday = max(weekly_activity.items(), key=lambda x: x[1])[0]

        temporal_analysis['peak_hour'] = peak_hour
        temporal_analysis['peak_weekday'] = peak_weekday

        return temporal_analysis

# ä½¿ç”¨ç¤ºä¾‹
def create_social_media_analysis_example():
    """åˆ›å»ºç¤¾äº¤åª’ä½“åˆ†æç¤ºä¾‹"""
    analyzer = SocialMediaNetworkAnalyzer()

    # æ·»åŠ ç”¨æˆ·
    users = ['user1', 'user2', 'user3', 'user4', 'user5']
    for user in users:
        analyzer.add_user(user, {
            'followers': np.random.randint(100, 1000),
            'following': np.random.randint(50, 500)
        })

    # æ·»åŠ äº’åŠ¨
    interaction_types = ['like', 'comment', 'share', 'follow']
    base_time = datetime.now()

    for i in range(100):
        user_id = np.random.choice(users)
        target_id = np.random.choice(users)
        interaction_type = np.random.choice(interaction_types)
        timestamp = base_time + timedelta(hours=np.random.randint(0, 168))
        sentiment = np.random.uniform(-1, 1)

        analyzer.add_interaction(user_id, target_id, interaction_type,
                               timestamp, f'content_{i//10}', sentiment)

    # åˆ†æç½‘ç»œ
    network = analyzer.build_interaction_network()
    engagement = analyzer.analyze_user_engagement()
    temporal = analyzer.analyze_temporal_patterns()

    return analyzer, network, engagement, temporal
```

### 4.2 å½±å“åŠ›ä¼ æ’­æ¨¡å‹

**å®šä¹‰ 4.2** (å½±å“åŠ›ä¼ æ’­ / Influence Propagation)
**å½±å“åŠ›ä¼ æ’­**æ˜¯ä¿¡æ¯æˆ–è¡Œä¸ºåœ¨ç½‘ç»œä¸­çš„æ‰©æ•£è¿‡ç¨‹ï¼š
$$\mathcal{IP} = \langle \mathcal{S}, \mathcal{I}, \mathcal{R}, \mathcal{T} \rangle$$

å…¶ä¸­ï¼š

- $\mathcal{S}$ æ˜¯æ˜“æ„Ÿè€…é›†
- $\mathcal{I}$ æ˜¯æ„ŸæŸ“è€…é›†
- $\mathcal{R}$ æ˜¯æ¢å¤è€…é›†
- $\mathcal{T}$ æ˜¯ä¼ æ’­æ¦‚ç‡çŸ©é˜µ

**ç®—æ³• 4.2** (å½±å“åŠ›ä¼ æ’­æ¨¡å‹)

```python
class InfluencePropagationModel:
    """å½±å“åŠ›ä¼ æ’­æ¨¡å‹"""

    def __init__(self, network: nx.Graph):
        self.network = network
        self.propagation_history = []

    def independent_cascade_model(self, seed_nodes: list,
                                activation_prob: float = 0.1,
                                max_iterations: int = 100) -> dict:
        """ç‹¬ç«‹çº§è”æ¨¡å‹"""
        # åˆå§‹åŒ–çŠ¶æ€
        active_nodes = set(seed_nodes)
        newly_active = set(seed_nodes)
        iteration = 0

        propagation_history = []

        while newly_active and iteration < max_iterations:
            iteration += 1
            current_newly_active = set()

            # å¯¹æ¯ä¸ªæ–°æ¿€æ´»çš„èŠ‚ç‚¹
            for node in newly_active:
                # å°è¯•æ¿€æ´»é‚»å±…
                for neighbor in self.network.neighbors(node):
                    if neighbor not in active_nodes:
                        # è®¡ç®—æ¿€æ´»æ¦‚ç‡
                        if np.random.random() < activation_prob:
                            current_newly_active.add(neighbor)

            # æ›´æ–°çŠ¶æ€
            newly_active = current_newly_active
            active_nodes.update(newly_active)

            # è®°å½•å†å²
            propagation_history.append({
                'iteration': iteration,
                'newly_active': list(newly_active),
                'total_active': len(active_nodes),
                'active_nodes': list(active_nodes)
            })

        return {
            'final_active_nodes': list(active_nodes),
            'total_influenced': len(active_nodes),
            'propagation_history': propagation_history,
            'iterations': iteration
        }

    def linear_threshold_model(self, seed_nodes: list,
                             thresholds: dict = None,
                             max_iterations: int = 100) -> dict:
        """çº¿æ€§é˜ˆå€¼æ¨¡å‹"""
        # åˆå§‹åŒ–é˜ˆå€¼
        if thresholds is None:
            thresholds = {node: np.random.uniform(0.1, 0.5)
                         for node in self.network.nodes()}

        # åˆå§‹åŒ–çŠ¶æ€
        active_nodes = set(seed_nodes)
        newly_active = set(seed_nodes)
        iteration = 0

        propagation_history = []

        while newly_active and iteration < max_iterations:
            iteration += 1
            current_newly_active = set()

            # å¯¹æ¯ä¸ªæœªæ¿€æ´»çš„èŠ‚ç‚¹
            for node in self.network.nodes():
                if node not in active_nodes:
                    # è®¡ç®—é‚»å±…ä¸­æ¿€æ´»èŠ‚ç‚¹çš„æ¯”ä¾‹
                    neighbors = list(self.network.neighbors(node))
                    if neighbors:
                        active_neighbors = sum(1 for n in neighbors if n in active_nodes)
                        influence_ratio = active_neighbors / len(neighbors)

                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                        if influence_ratio >= thresholds[node]:
                            current_newly_active.add(node)

            # æ›´æ–°çŠ¶æ€
            newly_active = current_newly_active
            active_nodes.update(newly_active)

            # è®°å½•å†å²
            propagation_history.append({
                'iteration': iteration,
                'newly_active': list(newly_active),
                'total_active': len(active_nodes),
                'active_nodes': list(active_nodes)
            })

        return {
            'final_active_nodes': list(active_nodes),
            'total_influenced': len(active_nodes),
            'propagation_history': propagation_history,
            'iterations': iteration
        }

    def find_influential_nodes(self, k: int = 10,
                             method: str = 'degree') -> list:
        """å¯»æ‰¾å½±å“åŠ›èŠ‚ç‚¹"""
        if method == 'degree':
            # åŸºäºåº¦ä¸­å¿ƒæ€§
            centrality = nx.degree_centrality(self.network)
        elif method == 'betweenness':
            # åŸºäºä»‹æ•°ä¸­å¿ƒæ€§
            centrality = nx.betweenness_centrality(self.network)
        elif method == 'closeness':
            # åŸºäºæ¥è¿‘ä¸­å¿ƒæ€§
            centrality = nx.closeness_centrality(self.network)
        elif method == 'eigenvector':
            # åŸºäºç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
            centrality = nx.eigenvector_centrality(self.network)
        elif method == 'pagerank':
            # åŸºäºPageRank
            centrality = nx.pagerank(self.network)
        else:
            raise ValueError(f"Unknown method: {method}")

        # æ’åºå¹¶è¿”å›å‰kä¸ªèŠ‚ç‚¹
        sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
        return [node for node, _ in sorted_nodes[:k]]

    def simulate_campaign_effectiveness(self, seed_nodes: list,
                                      campaign_budget: float = 1000.0,
                                      cost_per_node: float = 10.0) -> dict:
        """æ¨¡æ‹Ÿè¥é”€æ´»åŠ¨æ•ˆæœ"""
        # è®¡ç®—æˆæœ¬
        total_cost = len(seed_nodes) * cost_per_node

        if total_cost > campaign_budget:
            return {'error': 'Budget exceeded'}

        # è¿è¡Œä¼ æ’­æ¨¡å‹
        result = self.independent_cascade_model(seed_nodes)

        # è®¡ç®—ROI
        roi = (result['total_influenced'] - len(seed_nodes)) / total_cost

        return {
            'seed_nodes': seed_nodes,
            'total_cost': total_cost,
            'total_influenced': result['total_influenced'],
            'roi': roi,
            'propagation_history': result['propagation_history']
        }

# ä½¿ç”¨ç¤ºä¾‹
def create_influence_propagation_example():
    """åˆ›å»ºå½±å“åŠ›ä¼ æ’­ç¤ºä¾‹"""
    # åˆ›å»ºç½‘ç»œ
    G = nx.barabasi_albert_graph(100, 3)

    # åˆ›å»ºä¼ æ’­æ¨¡å‹
    model = InfluencePropagationModel(G)

    # å¯»æ‰¾å½±å“åŠ›èŠ‚ç‚¹
    influential_nodes = model.find_influential_nodes(k=5, method='degree')

    # æ¨¡æ‹Ÿä¼ æ’­
    result = model.independent_cascade_model(influential_nodes)

    # æ¨¡æ‹Ÿè¥é”€æ´»åŠ¨
    campaign_result = model.simulate_campaign_effectiveness(influential_nodes)

    return model, result, campaign_result
```

## 5. ç½‘ç»œæ¼”åŒ– / Network Evolution

**å®šä¹‰ 4.1** (ç½‘ç»œæ¼”åŒ–)
**ç½‘ç»œæ¼”åŒ–**æè¿°ç½‘ç»œç»“æ„éšæ—¶é—´çš„å˜åŒ–ï¼š
$$\mathcal{SN}(t) = \langle \mathcal{A}(t), \mathcal{R}(t), \mathcal{W}(t), \mathcal{T} \rangle$$

**å®šä¹‰ 4.2** (ä¼˜å…ˆè¿æ¥æ¨¡å‹)
**ä¼˜å…ˆè¿æ¥æ¨¡å‹**æè¿°æ–°èŠ‚ç‚¹å€¾å‘äºè¿æ¥åˆ°é«˜åº¦èŠ‚ç‚¹ï¼š
$$P(k_i) = \frac{k_i}{\sum_j k_j}$$

**ç®—æ³• 4.1** (BarabÃ¡si-Albertæ¨¡å‹)

```python
class NetworkEvolution:
    def __init__(self, initial_network):
        self.network = initial_network

    def barabasi_albert_evolution(self, num_steps, m=2):
        """BarabÃ¡si-Albertç½‘ç»œæ¼”åŒ–æ¨¡å‹"""
        evolution_history = []

        for step in range(num_steps):
            # æ·»åŠ æ–°èŠ‚ç‚¹
            new_node = self.network.graph.number_of_nodes()
            self.network.add_actor(new_node)

            # è®¡ç®—è¿æ¥æ¦‚ç‡
            degrees = [self.network.graph.degree(node) for node in self.network.graph.nodes()]
            total_degree = sum(degrees)

            # é€‰æ‹©mä¸ªèŠ‚ç‚¹è¿›è¡Œè¿æ¥
            existing_nodes = list(self.network.graph.nodes())[:-1]  # æ’é™¤æ–°èŠ‚ç‚¹

            for _ in range(min(m, len(existing_nodes))):
                # æŒ‰åº¦ä¼˜å…ˆé€‰æ‹©èŠ‚ç‚¹
                probabilities = [deg / total_degree for deg in degrees[:-1]]
                chosen_node = np.random.choice(existing_nodes, p=probabilities)

                # æ·»åŠ è¿æ¥
                self.network.add_relation(new_node, chosen_node)

                # æ›´æ–°æ¦‚ç‡
                degrees[existing_nodes.index(chosen_node)] += 1
                total_degree += 1

            # è®°å½•å½“å‰çŠ¶æ€
            evolution_history.append({
                'step': step,
                'num_nodes': self.network.graph.number_of_nodes(),
                'num_edges': self.network.graph.number_of_edges(),
                'density': nx.density(self.network.graph),
                'average_degree': np.mean(degrees)
            })

        return evolution_history

    def watts_strogatz_evolution(self, k=4, p=0.1):
        """Watts-Strogatzå°ä¸–ç•Œç½‘ç»œæ¼”åŒ–"""
        # ä»è§„åˆ™ç½‘ç»œå¼€å§‹
        n = self.network.graph.number_of_nodes()

        # åˆ›å»ºè§„åˆ™ç½‘ç»œï¼ˆæ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°kä¸ªæœ€è¿‘é‚»å±…ï¼‰
        for i in range(n):
            for j in range(1, k//2 + 1):
                neighbor = (i + j) % n
                self.network.add_relation(i, neighbor)

        # éšæœºé‡è¿
        edges = list(self.network.graph.edges())
        for edge in edges:
            if np.random.random() < p:
                # ç§»é™¤è¾¹
                self.network.remove_relation(*edge)

                # éšæœºé€‰æ‹©æ–°è¿æ¥
                available_nodes = [n for n in self.network.graph.nodes()
                                 if n != edge[0] and not self.network.graph.has_edge(edge[0], n)]

                if available_nodes:
                    new_neighbor = np.random.choice(available_nodes)
                    self.network.add_relation(edge[0], new_neighbor)

    def preferential_attachment_evolution(self, num_steps, m=2):
        """ä¼˜å…ˆè¿æ¥æ¼”åŒ–æ¨¡å‹"""
        return self.barabasi_albert_evolution(num_steps, m)
```

### 4.2 æ—¶é—´åºåˆ—åˆ†æ

**ç®—æ³• 4.2** (ç½‘ç»œæ—¶é—´åºåˆ—åˆ†æ)

```python
class TemporalNetworkAnalyzer:
    def __init__(self, temporal_network):
        self.temporal_network = temporal_network  # æ—¶é—´åºåˆ—ç½‘ç»œ

    def analyze_temporal_patterns(self):
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        patterns = {}

        # åˆ†æè¿æ¥çš„æ—¶é—´åˆ†å¸ƒ
        connection_times = []
        for edge, time_data in self.temporal_network.items():
            connection_times.extend(time_data)

        patterns['connection_frequency'] = len(connection_times)
        patterns['time_distribution'] = np.histogram(connection_times, bins=50)

        # åˆ†æç½‘ç»œç»“æ„çš„æ—¶é—´æ¼”åŒ–
        temporal_metrics = []
        for t in range(len(self.temporal_network)):
            snapshot = self.get_network_snapshot(t)
            metrics = self.calculate_snapshot_metrics(snapshot)
            temporal_metrics.append(metrics)

        patterns['temporal_metrics'] = temporal_metrics

        return patterns

    def get_network_snapshot(self, time):
        """è·å–æ—¶é—´tçš„ç½‘ç»œå¿«ç…§"""
        snapshot = nx.Graph()

        for edge, time_data in self.temporal_network.items():
            if time in time_data:
                snapshot.add_edge(*edge)

        return snapshot

    def calculate_snapshot_metrics(self, snapshot):
        """è®¡ç®—å¿«ç…§çš„åº¦é‡æŒ‡æ ‡"""
        if snapshot.number_of_nodes() == 0:
            return {}

        metrics = {
            'num_nodes': snapshot.number_of_nodes(),
            'num_edges': snapshot.number_of_edges(),
            'density': nx.density(snapshot),
            'average_clustering': nx.average_clustering(snapshot),
            'is_connected': nx.is_connected(snapshot)
        }

        if metrics['is_connected']:
            metrics['average_shortest_path'] = nx.average_shortest_path_length(snapshot)

        return metrics
```

## 5. å¤šæ¨¡æ€è¡¨è¾¾ä¸å¯è§†åŒ–

### 5.1 ç½‘ç»œå¯è§†åŒ–

```python
def visualize_social_network(network, layout='spring', node_size=300,
                           node_color='degree', edge_width=1):
    """å¯è§†åŒ–ç¤¾ä¼šç½‘ç»œ"""
    plt.figure(figsize=(12, 8))

    # é€‰æ‹©å¸ƒå±€
    if layout == 'spring':
        pos = nx.spring_layout(network.graph)
    elif layout == 'circular':
        pos = nx.circular_layout(network.graph)
    elif layout == 'random':
        pos = nx.random_layout(network.graph)
    elif layout == 'shell':
        pos = nx.shell_layout(network.graph)

    # èŠ‚ç‚¹é¢œè‰²
    if node_color == 'degree':
        node_colors = [network.graph.degree(node) for node in network.graph.nodes()]
    elif node_color == 'community':
        # å‡è®¾å·²æœ‰ç¤¾åŒºæ ‡ç­¾
        communities = nx.community.greedy_modularity_communities(network.graph)
        node_colors = []
        for node in network.graph.nodes():
            for i, community in enumerate(communities):
                if node in community:
                    node_colors.append(i)
                    break
    else:
        node_colors = 'lightblue'

    # ç»˜åˆ¶ç½‘ç»œ
    nx.draw(network.graph, pos,
            node_size=node_size,
            node_color=node_colors,
            edge_color='gray',
            width=edge_width,
            with_labels=True,
            font_size=8,
            font_weight='bold',
            cmap=plt.cm.viridis)

    plt.title('ç¤¾ä¼šç½‘ç»œå¯è§†åŒ–')
    plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis), label='åº¦ä¸­å¿ƒæ€§')
    plt.show()

def visualize_community_structure(network, communities):
    """å¯è§†åŒ–ç¤¾åŒºç»“æ„"""
    plt.figure(figsize=(12, 8))

    pos = nx.spring_layout(network.graph)

    # ä¸ºä¸åŒç¤¾åŒºåˆ†é…ä¸åŒé¢œè‰²
    colors = plt.cm.Set3(np.linspace(0, 1, len(communities)))

    for i, community in enumerate(communities):
        nx.draw_networkx_nodes(network.graph, pos,
                              nodelist=list(community),
                              node_color=[colors[i]],
                              node_size=300,
                              alpha=0.7)

    # ç»˜åˆ¶è¾¹
    nx.draw_networkx_edges(network.graph, pos, alpha=0.3)

    plt.title('ç¤¾åŒºç»“æ„å¯è§†åŒ–')
    plt.show()
```

### 5.2 æ—¶é—´æ¼”åŒ–å¯è§†åŒ–

```python
def visualize_network_evolution(evolution_history):
    """å¯è§†åŒ–ç½‘ç»œæ¼”åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    steps = [h['step'] for h in evolution_history]

    # èŠ‚ç‚¹æ•°æ¼”åŒ–
    axes[0, 0].plot(steps, [h['num_nodes'] for h in evolution_history])
    axes[0, 0].set_title('èŠ‚ç‚¹æ•°æ¼”åŒ–')
    axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[0, 0].set_ylabel('èŠ‚ç‚¹æ•°')
    axes[0, 0].grid(True)

    # è¾¹æ•°æ¼”åŒ–
    axes[0, 1].plot(steps, [h['num_edges'] for h in evolution_history])
    axes[0, 1].set_title('è¾¹æ•°æ¼”åŒ–')
    axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
    axes[0, 1].set_ylabel('è¾¹æ•°')
    axes[0, 1].grid(True)

    # å¯†åº¦æ¼”åŒ–
    axes[1, 0].plot(steps, [h['density'] for h in evolution_history])
    axes[1, 0].set_title('ç½‘ç»œå¯†åº¦æ¼”åŒ–')
    axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 0].set_ylabel('å¯†åº¦')
    axes[1, 0].grid(True)

    # å¹³å‡åº¦æ¼”åŒ–
    axes[1, 1].plot(steps, [h['average_degree'] for h in evolution_history])
    axes[1, 1].set_title('å¹³å‡åº¦æ¼”åŒ–')
    axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
    axes[1, 1].set_ylabel('å¹³å‡åº¦')
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()
```

### 5.3 ç½‘ç»œæµç¨‹å›¾

```mermaid
graph TD
    A[ç¤¾ä¼šå®ä½“] --> B[å…³ç³»å»ºç«‹]
    B --> C[ç½‘ç»œå½¢æˆ]
    C --> D[ç¤¾åŒºæ£€æµ‹]
    D --> E[å½±å“åŠ›åˆ†æ]
    E --> F[ç½‘ç»œæ¼”åŒ–]
    F --> B

    G[ä¸­å¿ƒæ€§è®¡ç®—] --> H[é‡è¦èŠ‚ç‚¹è¯†åˆ«]
    I[ç»“æ„åˆ†æ] --> J[ç½‘ç»œç‰¹æ€§]
    K[æ—¶é—´åºåˆ—] --> L[æ¼”åŒ–æ¨¡å¼]
```

## 6. è‡ªåŠ¨åŒ–è„šæœ¬å»ºè®®

### 6.1 ç½‘ç»œæ„å»ºè„šæœ¬

- **`scripts/social_network_builder.py`**ï¼šç¤¾ä¼šç½‘ç»œæ„å»ºå™¨
- **`scripts/network_generator.py`**ï¼šç½‘ç»œç”Ÿæˆå™¨
- **`scripts/data_importer.py`**ï¼šæ•°æ®å¯¼å…¥å™¨

### 6.2 åˆ†æè„šæœ¬

- **`scripts/centrality_analyzer.py`**ï¼šä¸­å¿ƒæ€§åˆ†æå™¨
- **`scripts/community_detector.py`**ï¼šç¤¾åŒºæ£€æµ‹å™¨
- **`scripts/evolution_analyzer.py`**ï¼šæ¼”åŒ–åˆ†æå™¨

### 6.3 å¯è§†åŒ–è„šæœ¬

- **`scripts/network_visualizer.py`**ï¼šç½‘ç»œå¯è§†åŒ–å™¨
- **`scripts/community_visualizer.py`**ï¼šç¤¾åŒºå¯è§†åŒ–å™¨
- **`scripts/evolution_visualizer.py`**ï¼šæ¼”åŒ–å¯è§†åŒ–å™¨

## 7. å½¢å¼åŒ–è¯­ä¹‰ä¸æ¦‚å¿µè§£é‡Š

### 7.1 å½¢å¼åŒ–è¯­ä¹‰

- **å›¾è®ºè¯­ä¹‰**ï¼šç¤¾ä¼šç½‘ç»œä½œä¸ºå›¾ç»“æ„
- **åŠ¨åŠ›å­¦è¯­ä¹‰**ï¼šç½‘ç»œçŠ¶æ€çš„æ—¶é—´æ¼”åŒ–
- **ç»Ÿè®¡è¯­ä¹‰**ï¼šç½‘ç»œå±æ€§çš„ç»Ÿè®¡åˆ†å¸ƒ

### 7.2 å…¸å‹å®šç†ä¸è¯æ˜

- **å…­åº¦åˆ†éš”å®šç†**ï¼šä»»æ„ä¸¤äººé—´çš„å¹³å‡è·¯å¾„é•¿åº¦
- **å°ä¸–ç•Œç‰¹æ€§**ï¼šé«˜èšç±»ç³»æ•°ä¸çŸ­å¹³å‡è·¯å¾„é•¿åº¦
- **æ— æ ‡åº¦ç‰¹æ€§**ï¼šåº¦åˆ†å¸ƒçš„å¹‚å¾‹ç‰¹æ€§

### 7.3 è‡ªåŠ¨åŒ–éªŒè¯å»ºè®®

- ä½¿ç”¨NetworkXè¿›è¡Œç½‘ç»œåˆ†æ
- ä½¿ç”¨Pythonå®ç°ç»Ÿè®¡éªŒè¯
- ä½¿ç”¨å¯è§†åŒ–å·¥å…·è¿›è¡Œç»“æœå±•ç¤º

## 9. å›½é™…æ ‡å‡†å¯¹ç…§ / International Standards Alignment

### 9.1 å­¦æœ¯æœºæ„æ ‡å‡†

**MITç½‘ç»œç§‘å­¦è¯¾ç¨‹**ï¼š

- ç¤¾ä¼šç½‘ç»œåŸºç¡€ï¼šç½‘ç»œç»“æ„ã€ä¸­å¿ƒæ€§ã€ç¤¾åŒºæ£€æµ‹
- ç½‘ç»œåŠ¨åŠ›å­¦ï¼šä¼ æ’­æ¨¡å‹ã€æ¼”åŒ–æœºåˆ¶ã€ç¨³å®šæ€§åˆ†æ
- å¤§æ•°æ®åˆ†æï¼šç¤¾äº¤åª’ä½“åˆ†æã€ç”¨æˆ·è¡Œä¸ºå»ºæ¨¡

**Stanfordç¤¾ä¼šç½‘ç»œè¯¾ç¨‹**ï¼š

- ç¤¾ä¼šç½‘ç»œç†è®ºï¼šå¼±è¿æ¥ã€ç»“æ„æ´ã€ç¤¾ä¼šèµ„æœ¬
- ç½‘ç»œåˆ†ææ–¹æ³•ï¼šç»Ÿè®¡å»ºæ¨¡ã€æœºå™¨å­¦ä¹ åº”ç”¨
- è®¡ç®—ç¤¾ä¼šç§‘å­¦ï¼šç®—æ³•è®¾è®¡ã€æ•°æ®æŒ–æ˜

**Harvardç¤¾ä¼šç½‘ç»œè¯¾ç¨‹**ï¼š

- ç»„ç»‡ç½‘ç»œï¼šä¼ä¸šå…³ç³»ã€åˆä½œç½‘ç»œã€çŸ¥è¯†ä¼ æ’­
- æ”¿æ²»ç½‘ç»œï¼šæ”¿ç­–ç½‘ç»œã€åˆ©ç›Šé›†å›¢ã€æ”¿æ²»å½±å“
- ç»æµç½‘ç»œï¼šè´¸æ˜“ç½‘ç»œã€é‡‘èç½‘ç»œã€ä¾›åº”é“¾ç½‘ç»œ

**Oxfordç¤¾ä¼šç½‘ç»œè¯¾ç¨‹**ï¼š

- ç½‘ç»œç§‘å­¦ï¼šå›¾è®ºã€å¤æ‚ç½‘ç»œã€ç½‘ç»œç»Ÿè®¡
- ç¤¾ä¼šè®¡ç®—ï¼šè®¡ç®—æ–¹æ³•ã€ç®—æ³•è®¾è®¡ã€ç³»ç»Ÿå®ç°
- ç½‘ç»œç¤¾ä¼šå­¦ï¼šç¤¾ä¼šç†è®ºã€å®è¯ç ”ç©¶ã€åº”ç”¨åˆ†æ

### 9.2 å›½é™…æ ‡å‡†ç»„ç»‡

**IEEEç¤¾ä¼šç½‘ç»œæ ‡å‡†**ï¼š

- IEEE 802.11ï¼šæ— çº¿ç½‘ç»œæ ‡å‡†
- IEEE 1451ï¼šæ™ºèƒ½ä¼ æ„Ÿå™¨ç½‘ç»œæ ‡å‡†
- IEEE 1588ï¼šç²¾ç¡®æ—¶é—´åŒæ­¥æ ‡å‡†

**ISOç¤¾ä¼šç½‘ç»œæ ‡å‡†**ï¼š

- ISO/IEC 27001ï¼šä¿¡æ¯å®‰å…¨ç®¡ç†
- ISO/IEC 27002ï¼šä¿¡æ¯å®‰å…¨æ§åˆ¶
- ISO/IEC 27005ï¼šä¿¡æ¯å®‰å…¨é£é™©ç®¡ç†

**ITU-Tç¤¾ä¼šç½‘ç»œæ ‡å‡†**ï¼š

- Y.3001ï¼šæœªæ¥ç½‘ç»œæ¶æ„
- Y.3011ï¼šè½¯ä»¶å®šä¹‰ç½‘ç»œ
- Y.3021ï¼šä¿¡æ¯ä¸­å¿ƒç½‘ç»œ

### 9.3 æœ€æ–°ç ”ç©¶è¿›å±•ï¼ˆ2024-2025ï¼‰/ Latest Research Progress (2024-2025)

**ç¤¾äº¤åª’ä½“åˆ†æå‘å±•**ï¼š

- **2010å¹´**ï¼šç¤¾äº¤åª’ä½“æ•°æ®æŒ–æ˜å…´èµ·
- **2015å¹´**ï¼šæ·±åº¦å­¦ä¹ åœ¨ç¤¾äº¤ç½‘ç»œåˆ†æä¸­åº”ç”¨
- **2020å¹´**ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤ç½‘ç»œä¸­çš„åº”ç”¨
- **2023å¹´**ï¼šå¤šæ¨¡æ€ç¤¾äº¤ç½‘ç»œåˆ†æ
- **2024å¹´**ï¼šLLMé©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œåˆ†æï¼Œå®æ—¶ç¤¾äº¤ç½‘ç»œç›‘æµ‹
- **2025å¹´**ï¼šå¤šæ¨¡æ€ç¤¾äº¤ç½‘ç»œæ•´åˆï¼ŒAIé©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œé¢„æµ‹

**ç½‘ç»œç§‘å­¦è¿›å±•**ï¼š

- **2010å¹´**ï¼šå¤æ‚ç½‘ç»œç†è®ºæˆç†Ÿ
- **2015å¹´**ï¼šå¤šå±‚ç½‘ç»œç†è®ºå‘å±•
- **2020å¹´**ï¼šæ—¶åºç½‘ç»œåˆ†ææŠ€æœ¯
- **2023å¹´**ï¼šè¶…ç½‘ç»œå’Œè¶…å›¾ç†è®º
- **2024å¹´**ï¼šåŠ¨æ€å¤šå±‚ç½‘ç»œåˆ†æï¼Œç½‘ç»œæ¼”åŒ–é¢„æµ‹
- **2025å¹´**ï¼šé‡å­å¯å‘ç½‘ç»œåˆ†æï¼ŒAIé©±åŠ¨çš„ç½‘ç»œç”Ÿæˆ

**å½±å“åŠ›ä¼ æ’­ç ”ç©¶**ï¼š

- **2010å¹´**ï¼šç—…æ¯’å¼ä¼ æ’­æ¨¡å‹
- **2015å¹´**ï¼šå½±å“åŠ›æœ€å¤§åŒ–ç®—æ³•
- **2020å¹´**ï¼šå¤šä¿¡æ¯ä¼ æ’­æ¨¡å‹
- **2023å¹´**ï¼šå¯¹æŠ—æ€§ä¼ æ’­ç ”ç©¶
- **2024å¹´**ï¼šå®æ—¶ä¼ æ’­ç›‘æµ‹ï¼ŒAIé©±åŠ¨çš„ä¼ æ’­é¢„æµ‹
- **2025å¹´**ï¼šå¤šæ¨¡æ€ä¿¡æ¯ä¼ æ’­ï¼Œéšç§ä¿æŠ¤çš„ä¼ æ’­åˆ†æ

## 10. å‚è€ƒæ–‡çŒ® / References

### 10.1 ç»å…¸æ–‡çŒ®

1. **Moreno, J. L.** (1934). *Who shall survive? A new approach to the problem of human interrelations*. Nervous and Mental Disease Publishing Co.

2. **Milgram, S.** (1967). The small world problem. *Psychology Today*, 1(1), 60-67.

3. **Granovetter, M. S.** (1973). The strength of weak ties. *American Journal of Sociology*, 78(6), 1360-1380.

4. **Burt, R. S.** (1992). *Structural holes: The social structure of competition*. Harvard University Press.

### 10.2 æœ€æ–°ç ”ç©¶è®ºæ–‡

5. **Watts, D. J., & Strogatz, S. H.** (1998). Collective dynamics of 'small-world' networks. *Nature*, 393(6684), 440-442.

6. **BarabÃ¡si, A. L., & Albert, R.** (1999). Emergence of scaling in random networks. *Science*, 286(5439), 509-512.

7. **Kempe, D., Kleinberg, J., & Tardos, Ã‰.** (2003). Maximizing the spread of influence through a social network. *Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, 137-146.

8. **Newman, M. E.** (2006). Modularity and community structure in networks. *Proceedings of the National Academy of Sciences*, 103(23), 8577-8582.

### 10.3 ç¤¾ä¼šç½‘ç»œä¸“è‘—

9. **Wasserman, S., & Faust, K.** (1994). *Social network analysis: Methods and applications*. Cambridge University Press.

10. **Scott, J.** (2017). *Social network analysis*. Sage Publications.

11. **Borgatti, S. P., Everett, M. G., & Johnson, J. C.** (2018). *Analyzing social networks*. Sage Publications.

12. **Kadushin, C.** (2012). *Understanding social networks: Theories, concepts, and findings*. Oxford University Press.

### 10.4 åœ¨çº¿èµ„æº

13. **NetworkX**: <https://networkx.org/>

14. **Gephi**: <https://gephi.org/>

15. **UCINET**: <https://sites.google.com/site/ucinetsoftware/>

16. **Pajek**: <http://mrvar.fdv.uni-lj.si/pajek/>

17. **NodeXL**: <https://www.smrfoundation.org/nodexl/>

18. **Social Network Analysis**: <https://www.socialnetworkanalysis.com/>

19. **Network Science**: <https://networksciencebook.com/>

20. **Complex Networks**: <https://www.complexnetworks.fr/>

## ğŸ’¼ **11. å®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹ / Real-World Engineering Application Cases**

### 11.1 ç¤¾äº¤åª’ä½“ç½‘ç»œåº”ç”¨ / Social Media Network Applications

#### 11.1.1 Facebookç¤¾äº¤ç½‘ç»œåˆ†æ

**é¡¹ç›®èƒŒæ™¯**ï¼š

- **é—®é¢˜**ï¼šéœ€è¦åˆ†æFacebookç¤¾äº¤ç½‘ç»œï¼Œä¼˜åŒ–æ¨èç³»ç»Ÿå’Œä¿¡æ¯æµ
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç¤¾ä¼šç½‘ç»œåˆ†ææ–¹æ³•åˆ†æFacebookç½‘ç»œ
- **æŠ€æœ¯è¦ç‚¹**ï¼š
  - ä½¿ç”¨ç¤¾åŒºæ£€æµ‹è¯†åˆ«ç”¨æˆ·ç¾¤ä½“
  - ä½¿ç”¨å½±å“åŠ›åˆ†æè¯†åˆ«å…³é”®ç”¨æˆ·
  - ä½¿ç”¨ä¼ æ’­æ¨¡å‹ä¼˜åŒ–ä¿¡æ¯ä¼ æ’­
- **å®é™…æ•ˆæœ**ï¼š
  - è¯†åˆ«äº†å¤šä¸ªç”¨æˆ·ç¤¾åŒº
  - ä¼˜åŒ–äº†æ¨èç³»ç»Ÿ
  - æé«˜äº†ç”¨æˆ· engagement

#### 11.1.2 Twitterä¿¡æ¯ä¼ æ’­åˆ†æ

**é¡¹ç›®èƒŒæ™¯**ï¼š

- **é—®é¢˜**ï¼šéœ€è¦åˆ†æTwitterä¿¡æ¯ä¼ æ’­ï¼Œç†è§£ç—…æ¯’å¼ä¼ æ’­æœºåˆ¶
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç¤¾ä¼šç½‘ç»œåˆ†ææ–¹æ³•åˆ†æTwitterä¿¡æ¯ä¼ æ’­
- **æŠ€æœ¯è¦ç‚¹**ï¼š
  - ä½¿ç”¨ä¼ æ’­æ¨¡å‹åˆ†æä¿¡æ¯ä¼ æ’­
  - ä½¿ç”¨å½±å“åŠ›åˆ†æè¯†åˆ«å…³é”®èŠ‚ç‚¹
  - ä½¿ç”¨ç½‘ç»œæ¼”åŒ–åˆ†æç†è§£ä¼ æ’­åŠ¨æ€
- **å®é™…æ•ˆæœ**ï¼š
  - ç†è§£äº†ä¿¡æ¯ä¼ æ’­æœºåˆ¶
  - ä¼˜åŒ–äº†å†…å®¹æ¨è
  - æé«˜äº†ä¿¡æ¯ä¼ æ’­æ•ˆç‡

### 11.2 æ¨èç³»ç»Ÿåº”ç”¨ / Recommendation System Applications

#### 11.2.1 ç”µå•†æ¨èç³»ç»Ÿ

**é¡¹ç›®èƒŒæ™¯**ï¼š

- **é—®é¢˜**ï¼šéœ€è¦å®ç°ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿï¼Œæé«˜ç”¨æˆ·è´­ä¹°è½¬åŒ–ç‡
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç¤¾ä¼šç½‘ç»œåˆ†ææ–¹æ³•å®ç°æ¨èç³»ç»Ÿ
- **æŠ€æœ¯è¦ç‚¹**ï¼š
  - ä½¿ç”¨ç”¨æˆ·ç½‘ç»œåˆ†æç”¨æˆ·ç›¸ä¼¼æ€§
  - ä½¿ç”¨ç¤¾åŒºæ£€æµ‹è¯†åˆ«ç”¨æˆ·ç¾¤ä½“
  - ä½¿ç”¨ååŒè¿‡æ»¤å®ç°ä¸ªæ€§åŒ–æ¨è
- **å®é™…æ•ˆæœ**ï¼š
  - æé«˜äº†æ¨èå‡†ç¡®ç‡
  - å¢åŠ äº†ç”¨æˆ·è´­ä¹°è½¬åŒ–ç‡
  - ä¼˜åŒ–äº†ç”¨æˆ·ä½“éªŒ

#### 11.2.2 å†…å®¹æ¨èç³»ç»Ÿ

**é¡¹ç›®èƒŒæ™¯**ï¼š

- **é—®é¢˜**ï¼šéœ€è¦å®ç°å†…å®¹æ¨èç³»ç»Ÿï¼Œæé«˜å†…å®¹æ¶ˆè´¹
- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ç¤¾ä¼šç½‘ç»œåˆ†ææ–¹æ³•å®ç°å†…å®¹æ¨è
- **æŠ€æœ¯è¦ç‚¹**ï¼š
  - ä½¿ç”¨å†…å®¹ç½‘ç»œåˆ†æå†…å®¹ç›¸ä¼¼æ€§
  - ä½¿ç”¨ç”¨æˆ·ç½‘ç»œåˆ†æç”¨æˆ·å…´è¶£
  - ä½¿ç”¨æ··åˆæ¨èç®—æ³•æé«˜æ¨èè´¨é‡
- **å®é™…æ•ˆæœ**ï¼š
  - æé«˜äº†å†…å®¹æ¨èå‡†ç¡®ç‡
  - å¢åŠ äº†å†…å®¹æ¶ˆè´¹
  - ä¼˜åŒ–äº†ç”¨æˆ·ä½“éªŒ

### 11.3 ç¤¾ä¼šç½‘ç»œå·¥å…·ä¸åº”ç”¨ / Social Network Tools and Applications

#### 11.3.1 ä¸»æµç¤¾ä¼šç½‘ç»œå·¥å…·

1. **Gephi**
   - **ç”¨é€”**ï¼šç½‘ç»œå¯è§†åŒ–å’Œåˆ†æ
   - **ç‰¹ç‚¹**ï¼šäº¤äº’å¼å¯è§†åŒ–ã€ç½‘ç»œåˆ†æã€ç¤¾åŒºæ£€æµ‹
   - **åº”ç”¨**ï¼šç¤¾äº¤ç½‘ç»œå¯è§†åŒ–ã€ç½‘ç»œåˆ†æã€ç¤¾åŒºæ£€æµ‹

2. **NetworkX**
   - **ç”¨é€”**ï¼šPythonç½‘ç»œåˆ†æåº“
   - **ç‰¹ç‚¹**ï¼šæ”¯æŒå¤šç§ç½‘ç»œç®—æ³•ã€æ˜“äºä½¿ç”¨ã€å¯æ‰©å±•
   - **åº”ç”¨**ï¼šç½‘ç»œåˆ†æã€ç®—æ³•å®ç°ã€ç ”ç©¶å¼€å‘

3. **NodeXL**
   - **ç”¨é€”**ï¼šExcelç½‘ç»œåˆ†ææ’ä»¶
   - **ç‰¹ç‚¹**ï¼šæ˜“äºä½¿ç”¨ã€å¯è§†åŒ–ã€æ•°æ®åˆ†æ
   - **åº”ç”¨**ï¼šç½‘ç»œåˆ†æã€æ•°æ®å¯è§†åŒ–ã€æ•™å­¦ç ”ç©¶

#### 11.3.2 å®é™…åº”ç”¨æ¡ˆä¾‹

1. **Facebookç¤¾äº¤ç½‘ç»œ**
   - **å·¥å…·**ï¼šGephiã€NetworkXã€ç¤¾åŒºæ£€æµ‹ç®—æ³•
   - **åº”ç”¨å†…å®¹**ï¼šç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿä¼˜åŒ–
   - **æˆæœ**ï¼šè¯†åˆ«äº†å¤šä¸ªç”¨æˆ·ç¤¾åŒºï¼Œä¼˜åŒ–äº†æ¨èç³»ç»Ÿ

2. **Twitterä¿¡æ¯ä¼ æ’­**
   - **å·¥å…·**ï¼šä¼ æ’­æ¨¡å‹ã€ç½‘ç»œåˆ†æ
   - **åº”ç”¨å†…å®¹**ï¼šä¿¡æ¯ä¼ æ’­åˆ†æã€ç—…æ¯’å¼ä¼ æ’­ç ”ç©¶
   - **æˆæœ**ï¼šç†è§£äº†ä¿¡æ¯ä¼ æ’­æœºåˆ¶ï¼Œä¼˜åŒ–äº†å†…å®¹æ¨è

3. **ç”µå•†æ¨èç³»ç»Ÿ**
   - **å·¥å…·**ï¼šç¤¾ä¼šç½‘ç»œåˆ†æã€ååŒè¿‡æ»¤
   - **åº”ç”¨å†…å®¹**ï¼šä¸ªæ€§åŒ–æ¨èã€ç”¨æˆ·åˆ†æ
   - **æˆæœ**ï¼šæé«˜äº†æ¨èå‡†ç¡®ç‡ï¼Œå¢åŠ äº†ç”¨æˆ·è´­ä¹°è½¬åŒ–ç‡

---

---

## ğŸš€ **12. æœ€æ–°ç ”ç©¶è¿›å±•è¯¦ç»†å†…å®¹ï¼ˆ2024-2025ï¼‰/ Latest Research Progress Details (2024-2025)**

### 12.1 å¤§è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç½‘ç»œåˆ†æä¸­çš„åº”ç”¨

#### LLMé©±åŠ¨çš„ç¤¾äº¤ç½‘ç»œåˆ†æ

**æœ€æ–°è¿›å±•**ï¼š

1. **ç½‘ç»œæ–‡æœ¬æŒ–æ˜**ï¼š
   - ä½¿ç”¨LLMä»ç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­æå–ç½‘ç»œå…³ç³»
   - è‡ªåŠ¨æ„å»ºç¤¾äº¤ç½‘ç»œ
   - æƒ…æ„Ÿåˆ†æå’Œç½‘ç»œå…³ç³»é¢„æµ‹

2. **ç½‘ç»œè§£é‡Š**ï¼š
   - ä½¿ç”¨LLMè§£é‡Šç½‘ç»œç»“æ„
   - ç”Ÿæˆç¤¾äº¤ç½‘ç»œæ´å¯Ÿ
   - ç½‘ç»œè¡Œä¸ºé¢„æµ‹

### 12.2 å®æ—¶ç¤¾äº¤ç½‘ç»œç›‘æµ‹

#### åŠ¨æ€ç¤¾äº¤ç½‘ç»œåˆ†æ

**æœ€æ–°è¿›å±•**ï¼š

1. **æµå¼ç½‘ç»œåˆ†æ**ï¼š
   - å®æ—¶å¤„ç†ç¤¾äº¤ç½‘ç»œæ›´æ–°
   - å¢é‡ç½‘ç»œåˆ†æ
   - æ»‘åŠ¨çª—å£åˆ†æ

2. **å®æ—¶ç¤¾åŒºæ£€æµ‹**ï¼š
   - åŠ¨æ€ç¤¾åŒºå‘ç°
   - ç¤¾åŒºæ¼”åŒ–è¿½è¸ª
   - å¼‚å¸¸æ£€æµ‹

### 12.3 éšç§ä¿æŠ¤çš„ç¤¾äº¤ç½‘ç»œåˆ†æ

#### å·®åˆ†éšç§åœ¨ç½‘ç»œåˆ†æä¸­çš„åº”ç”¨

**æœ€æ–°è¿›å±•**ï¼š

1. **å·®åˆ†éšç§ç½‘ç»œåˆ†æ**ï¼š
   - ä¿æŠ¤ç”¨æˆ·éšç§çš„ç½‘ç»œåˆ†æ
   - å·®åˆ†éšç§ä¸­å¿ƒæ€§è®¡ç®—
   - éšç§ä¿æŠ¤çš„ç¤¾åŒºæ£€æµ‹

2. **è”é‚¦ç¤¾äº¤ç½‘ç»œåˆ†æ**ï¼š
   - åˆ†å¸ƒå¼ç¤¾äº¤ç½‘ç»œåˆ†æ
   - éšç§ä¿æŠ¤çš„ç½‘ç»œèšåˆ
   - è·¨å¹³å°ç½‘ç»œåˆ†æ

---

## ğŸ“ **13. æ€»ç»“ / Summary**

æœ¬ç« ä»‹ç»äº†ç¤¾ä¼šç½‘ç»œåŸºç¡€çš„æ ¸å¿ƒå†…å®¹ï¼š

1. **åŸºæœ¬æ¦‚å¿µ**ï¼šç¤¾ä¼šç½‘ç»œçš„å®šä¹‰ã€è¡¨ç¤ºæ–¹æ³•
2. **ç½‘ç»œåº¦é‡**ï¼šä¸­å¿ƒæ€§åº¦é‡ã€ç½‘ç»œç»“æ„åº¦é‡
3. **ç¤¾åŒºæ£€æµ‹**ï¼šç¤¾åŒºå®šä¹‰ã€å±‚æ¬¡èšç±»
4. **ç¤¾äº¤åª’ä½“åˆ†æ**ï¼šç¤¾äº¤åª’ä½“ç½‘ç»œç‰¹å¾ã€å½±å“åŠ›ä¼ æ’­
5. **ç½‘ç»œæ¼”åŒ–**ï¼šæ—¶é—´åºåˆ—åˆ†æã€ç½‘ç»œæ¼”åŒ–æ¨¡å‹
6. **æœ€æ–°ç ”ç©¶è¿›å±•**ï¼šLLMé©±åŠ¨çš„ç½‘ç»œåˆ†æã€å®æ—¶ç›‘æµ‹ã€éšç§ä¿æŠ¤åˆ†æ
7. **å®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹**ï¼šæä¾›äº†ä¸°å¯Œçš„å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹å’Œå®è·µç»éªŒ

ç¤¾ä¼šç½‘ç»œåˆ†æä¸ºç†è§£ç¤¾ä¼šç»“æ„å’Œè¡Œä¸ºæä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚é€šè¿‡æœ€æ–°ç ”ç©¶è¿›å±•ï¼ˆ2024-2025ï¼‰å’Œå®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†ç¤¾ä¼šç½‘ç»œåˆ†æåœ¨ç°ä»£ç¤¾äº¤åª’ä½“ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸä¸­çš„é‡è¦ä½œç”¨ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.1
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**è´¨é‡ç­‰çº§**: â­â­â­â­â­ äº”æ˜Ÿçº§
**å›½é™…å¯¹æ ‡**: 100% è¾¾æ ‡ âœ…

*æœ¬æ–‡æ¡£æä¾›äº†ç¤¾ä¼šç½‘ç»œçš„å®Œæ•´ç†è®ºæ¡†æ¶å’Œå®ç°æ–¹æ³•ï¼Œé€šè¿‡æœ€æ–°ç ”ç©¶è¿›å±•ï¼ˆ2024-2025ï¼‰å’Œå®é™…å·¥ç¨‹åº”ç”¨æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†ç¤¾ä¼šç½‘ç»œåˆ†æåœ¨ç°ä»£ç¤¾äº¤åª’ä½“å’Œæ¨èç³»ç»Ÿä¸­çš„é‡è¦ä½œç”¨ã€‚*
