# è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³• / Adaptive Optimization Methods

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æè¿°è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´ã€è‡ªé€‚åº”æ­£åˆ™åŒ–ã€è‡ªé€‚åº”æ¶æ„æœç´¢ã€è‡ªé€‚åº”æ•°æ®é‡‡æ ·ç­‰å†…å®¹ã€‚

---

## ğŸ“‘ **ç›®å½• / Table of Contents**

- [è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³• / Adaptive Optimization Methods](#è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ³•--adaptive-optimization-methods)
  - [ğŸ“š **æ¦‚è¿° / Overview**](#-æ¦‚è¿°--overview)
  - [ğŸ“‘ **ç›®å½• / Table of Contents**](#-ç›®å½•--table-of-contents)
  - [ğŸ“ **å½¢å¼åŒ–å®šä¹‰ / Formal Definition**](#-å½¢å¼åŒ–å®šä¹‰--formal-definition)
    - [å®šä¹‰ 5.2.1 (è‡ªé€‚åº”ä¼˜åŒ–é—®é¢˜ / Adaptive Optimization Problem)](#å®šä¹‰-521-è‡ªé€‚åº”ä¼˜åŒ–é—®é¢˜--adaptive-optimization-problem)
    - [å®šä¹‰ 5.2.2 (è‡ªé€‚åº”å­¦ä¹ ç‡ / Adaptive Learning Rate)](#å®šä¹‰-522-è‡ªé€‚åº”å­¦ä¹ ç‡--adaptive-learning-rate)
    - [å®šä¹‰ 5.2.3 (è‡ªé€‚åº”æ­£åˆ™åŒ– / Adaptive Regularization)](#å®šä¹‰-523-è‡ªé€‚åº”æ­£åˆ™åŒ–--adaptive-regularization)
    - [å½¢å¼åŒ–è¯­ä¹‰ / Formal Semantics](#å½¢å¼åŒ–è¯­ä¹‰--formal-semantics)
  - [ğŸ”§ **ç†è®ºåŸºç¡€ / Theoretical Foundation**](#-ç†è®ºåŸºç¡€--theoretical-foundation)
    - [5.2.1 æ”¶æ•›æ€§ç†è®º / Convergence Theory](#521-æ”¶æ•›æ€§ç†è®º--convergence-theory)
      - [å®šç† 5.2.1 (è‡ªé€‚åº”å­¦ä¹ ç‡æ”¶æ•›æ€§ / Adaptive Learning Rate Convergence)](#å®šç†-521-è‡ªé€‚åº”å­¦ä¹ ç‡æ”¶æ•›æ€§--adaptive-learning-rate-convergence)
      - [å®šç† 5.2.2 (è‡ªé€‚åº”æ­£åˆ™åŒ–æ³›åŒ–ç•Œ / Adaptive Regularization Generalization Bound)](#å®šç†-522-è‡ªé€‚åº”æ­£åˆ™åŒ–æ³›åŒ–ç•Œ--adaptive-regularization-generalization-bound)
    - [5.2.2 è‡ªé€‚åº”æœºåˆ¶è®¾è®¡ / Adaptive Mechanism Design](#522-è‡ªé€‚åº”æœºåˆ¶è®¾è®¡--adaptive-mechanism-design)
      - [5.2.2.1 åŸºäºæ¢¯åº¦çš„è‡ªé€‚åº”å­¦ä¹ ç‡](#5221-åŸºäºæ¢¯åº¦çš„è‡ªé€‚åº”å­¦ä¹ ç‡)
      - [5.2.2.2 åŸºäºæŸå¤±çš„è‡ªé€‚åº”å­¦ä¹ ç‡](#5222-åŸºäºæŸå¤±çš„è‡ªé€‚åº”å­¦ä¹ ç‡)
      - [5.2.2.3 è‡ªé€‚åº”æ­£åˆ™åŒ–](#5223-è‡ªé€‚åº”æ­£åˆ™åŒ–)
  - [ğŸ”§ **ä¼˜åŒ–æ–¹æ³•ç±»å‹ / Types of Optimization Methods**](#-ä¼˜åŒ–æ–¹æ³•ç±»å‹--types-of-optimization-methods)
    - [5.2.3 è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³• / Adaptive Learning Rate Methods](#523-è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•--adaptive-learning-rate-methods)
      - [5.2.3.1 åŸºäºæ¢¯åº¦å†å²çš„æ–¹æ³•](#5231-åŸºäºæ¢¯åº¦å†å²çš„æ–¹æ³•)
      - [5.2.3.2 åŸºäºæŸå¤±çš„æ–¹æ³•](#5232-åŸºäºæŸå¤±çš„æ–¹æ³•)
    - [5.2.4 è‡ªé€‚åº”æ­£åˆ™åŒ–æ–¹æ³• / Adaptive Regularization Methods](#524-è‡ªé€‚åº”æ­£åˆ™åŒ–æ–¹æ³•--adaptive-regularization-methods)
      - [5.2.4.1 è‡ªé€‚åº”Dropout](#5241-è‡ªé€‚åº”dropout)
      - [5.2.4.2 è‡ªé€‚åº”æƒé‡è¡°å‡](#5242-è‡ªé€‚åº”æƒé‡è¡°å‡)
    - [5.2.5 è‡ªé€‚åº”æ•°æ®é‡‡æ ·æ–¹æ³• / Adaptive Data Sampling Methods](#525-è‡ªé€‚åº”æ•°æ®é‡‡æ ·æ–¹æ³•--adaptive-data-sampling-methods)
      - [5.2.5.1 é‡è¦æ€§é‡‡æ ·](#5251-é‡è¦æ€§é‡‡æ ·)
      - [5.2.5.2 è´Ÿé‡‡æ ·](#5252-è´Ÿé‡‡æ ·)
    - [5.2.6 è‡ªé€‚åº”æ¶æ„æœç´¢ / Adaptive Architecture Search](#526-è‡ªé€‚åº”æ¶æ„æœç´¢--adaptive-architecture-search)
  - [ğŸ’» **ç®—æ³•å®ç° / Algorithm Implementation**](#-ç®—æ³•å®ç°--algorithm-implementation)
    - [ç®—æ³• 5.2.1 (è‡ªé€‚åº”Adamä¼˜åŒ–å™¨ / Adaptive Adam Optimizer)](#ç®—æ³•-521-è‡ªé€‚åº”adamä¼˜åŒ–å™¨--adaptive-adam-optimizer)
    - [ç®—æ³• 5.2.2 (è‡ªé€‚åº”æ­£åˆ™åŒ–ä¼˜åŒ–å™¨ / Adaptive Regularization Optimizer)](#ç®—æ³•-522-è‡ªé€‚åº”æ­£åˆ™åŒ–ä¼˜åŒ–å™¨--adaptive-regularization-optimizer)
    - [ç®—æ³• 5.2.3 (è‡ªé€‚åº”æ•°æ®é‡‡æ ·è®­ç»ƒ / Adaptive Data Sampling Training)](#ç®—æ³•-523-è‡ªé€‚åº”æ•°æ®é‡‡æ ·è®­ç»ƒ--adaptive-data-sampling-training)
  - [ğŸ“Š **å¤æ‚åº¦åˆ†æ / Complexity Analysis**](#-å¤æ‚åº¦åˆ†æ--complexity-analysis)
  - [ğŸ’¼ **å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-World Applications**](#-å®é™…åº”ç”¨æ¡ˆä¾‹--real-world-applications)
    - [æ¡ˆä¾‹1: GNNè®­ç»ƒä¸­çš„è‡ªé€‚åº”ä¼˜åŒ–](#æ¡ˆä¾‹1-gnnè®­ç»ƒä¸­çš„è‡ªé€‚åº”ä¼˜åŒ–)
  - [ğŸ”— **ç›¸å…³é“¾æ¥ / Related Links**](#-ç›¸å…³é“¾æ¥--related-links)

---

## ğŸ“ **å½¢å¼åŒ–å®šä¹‰ / Formal Definition**

### å®šä¹‰ 5.2.1 (è‡ªé€‚åº”ä¼˜åŒ–é—®é¢˜ / Adaptive Optimization Problem)

**è‡ªé€‚åº”ä¼˜åŒ–é—®é¢˜**æ˜¯åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è‡ªé€‚åº”è°ƒæ•´ä¼˜åŒ–å‚æ•°çš„é—®é¢˜ï¼š

$$\min_{\theta, \alpha} L(\theta, \alpha, \mathcal{D}) = \min_{\theta, \alpha} \mathbb{E}_{(x,y) \sim \mathcal{D}}[\ell(f_\theta(x), y) + R(\theta, \alpha)]$$

å…¶ä¸­ï¼š

- $\theta \in \Theta$ æ˜¯æ¨¡å‹å‚æ•°ç©ºé—´
- $\alpha \in \mathcal{A}$ æ˜¯è‡ªé€‚åº”ä¼˜åŒ–å‚æ•°ç©ºé—´ï¼ˆå­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ç³»æ•°ç­‰ï¼‰
- $\mathcal{D}$ æ˜¯æ•°æ®é›†
- $\ell$ æ˜¯æŸå¤±å‡½æ•°
- $R(\theta, \alpha)$ æ˜¯æ­£åˆ™åŒ–é¡¹

### å®šä¹‰ 5.2.2 (è‡ªé€‚åº”å­¦ä¹ ç‡ / Adaptive Learning Rate)

**è‡ªé€‚åº”å­¦ä¹ ç‡**æ˜¯æ ¹æ®ä¼˜åŒ–å†å²åŠ¨æ€è°ƒæ•´çš„å­¦ä¹ ç‡ï¼š

$$\alpha_t = \alpha_0 \cdot \eta_t(\nabla_\theta L, \mathcal{H}_t)$$

å…¶ä¸­ï¼š

- $\alpha_0$ æ˜¯åˆå§‹å­¦ä¹ ç‡
- $\eta_t$ æ˜¯è‡ªé€‚åº”è°ƒæ•´å‡½æ•°
- $\nabla_\theta L$ æ˜¯å½“å‰æ¢¯åº¦
- $\mathcal{H}_t = \{\nabla_\theta L_1, \nabla_\theta L_2, \ldots, \nabla_\theta L_t\}$ æ˜¯æ¢¯åº¦å†å²

### å®šä¹‰ 5.2.3 (è‡ªé€‚åº”æ­£åˆ™åŒ– / Adaptive Regularization)

**è‡ªé€‚åº”æ­£åˆ™åŒ–**æ˜¯æ ¹æ®æ¨¡å‹çŠ¶æ€åŠ¨æ€è°ƒæ•´çš„æ­£åˆ™åŒ–ï¼š

$$R(\theta, \alpha) = \sum_{i} \alpha_i \cdot r_i(\theta)$$

å…¶ä¸­ï¼š

- $\alpha_i$ æ˜¯ç¬¬ $i$ ä¸ªæ­£åˆ™åŒ–é¡¹çš„æƒé‡
- $r_i(\theta)$ æ˜¯ç¬¬ $i$ ä¸ªæ­£åˆ™åŒ–å‡½æ•°
- $\alpha_i$ æ ¹æ®è¿‡æ‹Ÿåˆç¨‹åº¦è‡ªé€‚åº”è°ƒæ•´ï¼š$\alpha_i = f_i(\text{overfitting\_metric})$

### å½¢å¼åŒ–è¯­ä¹‰ / Formal Semantics

- **ä¼˜åŒ–è¯­ä¹‰**: è‡ªé€‚åº”ä¼˜åŒ–æ˜¯åœ¨åŒå±‚ä¼˜åŒ–æ¡†æ¶ä¸‹çš„ä¼˜åŒ–é—®é¢˜
- **åŠ¨æ€è¯­ä¹‰**: ä¼˜åŒ–å‚æ•°éšæ—¶é—´æ¼”åŒ–ï¼Œåæ˜ ä¼˜åŒ–è¿‡ç¨‹çš„çŠ¶æ€
- **åé¦ˆè¯­ä¹‰**: ä¼˜åŒ–å‚æ•°æ ¹æ®ä¼˜åŒ–æ•ˆæœåé¦ˆè°ƒæ•´

---

## ğŸ”§ **ç†è®ºåŸºç¡€ / Theoretical Foundation**

### 5.2.1 æ”¶æ•›æ€§ç†è®º / Convergence Theory

#### å®šç† 5.2.1 (è‡ªé€‚åº”å­¦ä¹ ç‡æ”¶æ•›æ€§ / Adaptive Learning Rate Convergence)

**å‡è®¾**:

1. æŸå¤±å‡½æ•° $L(\theta)$ æ˜¯ $L$-å…‰æ»‘çš„ï¼š$\|\nabla L(\theta_1) - \nabla L(\theta_2)\| \leq L \|\theta_1 - \theta_2\|$
2. æŸå¤±å‡½æ•°æ˜¯ $\mu$-å¼ºå‡¸çš„ï¼š$L(\theta_2) \geq L(\theta_1) + \langle \nabla L(\theta_1), \theta_2 - \theta_1 \rangle + \frac{\mu}{2}\|\theta_2 - \theta_1\|^2$
3. å­¦ä¹ ç‡æ»¡è¶³ï¼š$\alpha_t \in [\alpha_{min}, \alpha_{max}]$ï¼Œå…¶ä¸­ $0 < \alpha_{min} < \alpha_{max} < \frac{2}{L}$

**ç»“è®º**: è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•ä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›ï¼š

$$\|\theta_t - \theta^*\|^2 \leq \left(1 - \frac{\mu \alpha_{min}}{2}\right)^t \|\theta_0 - \theta^*\|^2$$

å…¶ä¸­ $\theta^*$ æ˜¯æœ€ä¼˜è§£ã€‚

**è¯æ˜æ€è·¯**:

1. ä½¿ç”¨å¼ºå‡¸æ€§ï¼š$L(\theta_{t+1}) - L(\theta^*) \leq \langle \nabla L(\theta_t), \theta_t - \theta^* \rangle - \frac{\mu}{2}\|\theta_t - \theta^*\|^2$
2. ä½¿ç”¨æ›´æ–°è§„åˆ™ï¼š$\theta_{t+1} = \theta_t - \alpha_t \nabla L(\theta_t)$
3. ç»“åˆå­¦ä¹ ç‡çº¦æŸï¼Œå¾—åˆ°æ”¶æ•›ç•Œ

#### å®šç† 5.2.2 (è‡ªé€‚åº”æ­£åˆ™åŒ–æ³›åŒ–ç•Œ / Adaptive Regularization Generalization Bound)

**å‡è®¾**:

1. æ¨¡å‹å‚æ•° $\theta$ åœ¨ $\ell_2$ çƒå†…ï¼š$\|\theta\|_2 \leq B$
2. æ•°æ®åˆ†å¸ƒæ»¡è¶³æŸç§å¤æ‚æ€§å‡è®¾

**ç»“è®º**: ä½¿ç”¨è‡ªé€‚åº”æ­£åˆ™åŒ–çš„æ¨¡å‹ï¼Œå…¶æ³›åŒ–è¯¯å·®ç•Œä¸ºï¼š

$$L(\theta) - \hat{L}(\theta) \leq O\left(\sqrt{\frac{B^2 \log(1/\delta)}{n}}\right)$$

å…¶ä¸­ $\hat{L}(\theta)$ æ˜¯ç»éªŒæŸå¤±ï¼Œ$n$ æ˜¯æ ·æœ¬æ•°ï¼Œ$\delta$ æ˜¯ç½®ä¿¡åº¦ã€‚

### 5.2.2 è‡ªé€‚åº”æœºåˆ¶è®¾è®¡ / Adaptive Mechanism Design

#### 5.2.2.1 åŸºäºæ¢¯åº¦çš„è‡ªé€‚åº”å­¦ä¹ ç‡

**AdaGradç®—æ³•**:
$$\alpha_t = \frac{\alpha_0}{\sqrt{\sum_{i=1}^t g_i^2 + \epsilon}}$$

å…¶ä¸­ $g_i$ æ˜¯ç¬¬ $i$ æ­¥çš„æ¢¯åº¦ã€‚

**RMSpropç®—æ³•**:
$$v_t = \beta v_{t-1} + (1-\beta) g_t^2$$
$$\alpha_t = \frac{\alpha_0}{\sqrt{v_t + \epsilon}}$$

**Adamç®—æ³•**:
$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
$$\alpha_t = \frac{\alpha_0 \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

å…¶ä¸­ $\hat{m}_t = \frac{m_t}{1-\beta_1^t}$ï¼Œ$\hat{v}_t = \frac{v_t}{1-\beta_2^t}$ã€‚

#### 5.2.2.2 åŸºäºæŸå¤±çš„è‡ªé€‚åº”å­¦ä¹ ç‡

**ReduceLROnPlateau**:
$$\alpha_t = \begin{cases}
\alpha_{t-1} \cdot \gamma & \text{if } L_t \text{ ä¸ä¸‹é™} \\
\alpha_{t-1} & \text{otherwise}
\end{cases}$$

å…¶ä¸­ $\gamma < 1$ æ˜¯è¡°å‡å› å­ã€‚

#### 5.2.2.3 è‡ªé€‚åº”æ­£åˆ™åŒ–

**è‡ªé€‚åº”Dropout**:
$$p_i(t) = f(\text{overfitting\_metric}_i(t))$$

å…¶ä¸­ $p_i(t)$ æ˜¯ç¬¬ $i$ å±‚çš„Dropoutæ¦‚ç‡ã€‚

**è‡ªé€‚åº”æƒé‡è¡°å‡**:
$$\lambda_t = \lambda_0 \cdot \exp(-\eta \cdot \text{improvement}_t)$$

å…¶ä¸­ $\text{improvement}_t$ æ˜¯éªŒè¯é›†ä¸Šçš„æ”¹è¿›ç¨‹åº¦ã€‚

---

## ğŸ”§ **ä¼˜åŒ–æ–¹æ³•ç±»å‹ / Types of Optimization Methods**

### 5.2.3 è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³• / Adaptive Learning Rate Methods

#### 5.2.3.1 åŸºäºæ¢¯åº¦å†å²çš„æ–¹æ³•

1. **AdaGrad (Adaptive Gradient)**:
   - **åŸç†**: ç´¯ç§¯æ¢¯åº¦å¹³æ–¹ï¼Œå¯¹é¢‘ç¹æ›´æ–°çš„å‚æ•°ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
   - **æ›´æ–°è§„åˆ™**: $\alpha_{t,i} = \frac{\alpha_0}{\sqrt{\sum_{j=1}^t g_{j,i}^2 + \epsilon}}$
   - **ä¼˜ç‚¹**: é€‚åˆç¨€ç–æ¢¯åº¦
   - **ç¼ºç‚¹**: å­¦ä¹ ç‡å•è°ƒé€’å‡ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢

2. **RMSprop (Root Mean Square Propagation)**:
   - **åŸç†**: ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›¿ä»£ç´¯ç§¯å’Œ
   - **æ›´æ–°è§„åˆ™**: $v_t = \beta v_{t-1} + (1-\beta) g_t^2$ï¼Œ$\alpha_t = \frac{\alpha_0}{\sqrt{v_t + \epsilon}}$
   - **ä¼˜ç‚¹**: è§£å†³AdaGradå­¦ä¹ ç‡è¡°å‡è¿‡å¿«çš„é—®é¢˜
   - **åº”ç”¨**: é€‚åˆéå¹³ç¨³ç›®æ ‡å‡½æ•°

3. **Adam (Adaptive Moment Estimation)**:
   - **åŸç†**: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
   - **æ›´æ–°è§„åˆ™**: è§å®šç†5.2.2ä¸­çš„å®šä¹‰
   - **ä¼˜ç‚¹**: æ”¶æ•›å¿«ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
   - **ç¼ºç‚¹**: å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹æ³›åŒ–æ€§èƒ½è¾ƒå·®

4. **AdamW (Adam with Weight Decay)**:
   - **åŸç†**: å°†æƒé‡è¡°å‡ä»æ¢¯åº¦ä¸­åˆ†ç¦»
   - **ä¼˜ç‚¹**: æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
   - **åº”ç”¨**: ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ‡å‡†é€‰æ‹©

#### 5.2.3.2 åŸºäºæŸå¤±çš„æ–¹æ³•

1. **ReduceLROnPlateau**:
   - **åŸç†**: å½“æŸå¤±ä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
   - **è§¦å‘æ¡ä»¶**: éªŒè¯æŸå¤±åœ¨ $patience$ ä¸ªepochå†…ä¸ä¸‹é™
   - **åº”ç”¨**: è®­ç»ƒåæœŸå¾®è°ƒ

2. **Cosine Annealing**:
   - **åŸç†**: å­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°è¡°å‡
   - **æ›´æ–°è§„åˆ™**: $\alpha_t = \alpha_{min} + (\alpha_{max} - \alpha_{min}) \cdot \frac{1 + \cos(\pi t / T)}{2}$
   - **åº”ç”¨**: å‘¨æœŸæ€§é‡å¯ï¼Œè·³å‡ºå±€éƒ¨æœ€ä¼˜

3. **Warm Restart**:
   - **åŸç†**: å‘¨æœŸæ€§é‡å¯å­¦ä¹ ç‡
   - **åº”ç”¨**: ç»“åˆCosine Annealingä½¿ç”¨

### 5.2.4 è‡ªé€‚åº”æ­£åˆ™åŒ–æ–¹æ³• / Adaptive Regularization Methods

#### 5.2.4.1 è‡ªé€‚åº”Dropout

1. **Variational Dropout**:
   - **åŸç†**: ä¸ºæ¯ä¸ªå‚æ•°å­¦ä¹ Dropoutæ¦‚ç‡
   - **æ›´æ–°**: $p_i = \text{sigmoid}(\phi_i)$ï¼Œå…¶ä¸­ $\phi_i$ æ˜¯å¯å­¦ä¹ å‚æ•°

2. **Concrete Dropout**:
   - **åŸç†**: ä½¿ç”¨Gumbel-Softmaxè¿‘ä¼¼ç¦»æ•£Dropout
   - **ä¼˜ç‚¹**: å¯å¾®åˆ†ï¼Œå¯ä»¥ç«¯åˆ°ç«¯è®­ç»ƒ

3. **Spatial Dropout**:
   - **åŸç†**: åœ¨ç©ºé—´ç»´åº¦ä¸ŠDropout
   - **åº”ç”¨**: å·ç§¯ç¥ç»ç½‘ç»œ

#### 5.2.4.2 è‡ªé€‚åº”æƒé‡è¡°å‡

1. **L2æ­£åˆ™åŒ–è‡ªé€‚åº”**:
   - **åŸç†**: æ ¹æ®å‚æ•°é‡è¦æ€§è°ƒæ•´æƒé‡è¡°å‡å¼ºåº¦
   - **æ›´æ–°**: $\lambda_i = \lambda_0 \cdot \exp(-\eta \cdot I_i)$ï¼Œå…¶ä¸­ $I_i$ æ˜¯å‚æ•°é‡è¦æ€§

2. **Spectral Normalization**:
   - **åŸç†**: çº¦æŸæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°
   - **åº”ç”¨**: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

### 5.2.5 è‡ªé€‚åº”æ•°æ®é‡‡æ ·æ–¹æ³• / Adaptive Data Sampling Methods

#### 5.2.5.1 é‡è¦æ€§é‡‡æ ·

1. **åŸºäºæŸå¤±çš„é‡è¦æ€§é‡‡æ ·**:
   - **åŸç†**: ä¼˜å…ˆé‡‡æ ·æŸå¤±å¤§çš„æ ·æœ¬
   - **æ¦‚ç‡**: $p_i \propto \ell_i^\gamma$ï¼Œå…¶ä¸­ $\ell_i$ æ˜¯æ ·æœ¬ $i$ çš„æŸå¤±

2. **åŸºäºæ¢¯åº¦çš„é‡è¦æ€§é‡‡æ ·**:
   - **åŸç†**: ä¼˜å…ˆé‡‡æ ·æ¢¯åº¦å¤§çš„æ ·æœ¬
   - **æ¦‚ç‡**: $p_i \propto \|\nabla_\theta \ell_i\|^\gamma$

#### 5.2.5.2 è´Ÿé‡‡æ ·

1. **è‡ªé€‚åº”è´Ÿé‡‡æ ·**:
   - **åŸç†**: æ ¹æ®æ ·æœ¬éš¾åº¦è°ƒæ•´è´Ÿé‡‡æ ·æ¯”ä¾‹
   - **åº”ç”¨**: æ¨èç³»ç»Ÿã€çŸ¥è¯†å›¾è°±åµŒå…¥

### 5.2.6 è‡ªé€‚åº”æ¶æ„æœç´¢ / Adaptive Architecture Search

1. **ç¥ç»æ¶æ„æœç´¢ (NAS)**:
   - **åŸç†**: è‡ªåŠ¨æœç´¢æœ€ä¼˜ç½‘ç»œæ¶æ„
   - **æ–¹æ³•**: å¼ºåŒ–å­¦ä¹ ã€è¿›åŒ–ç®—æ³•ã€å¯å¾®åˆ†æ¶æ„æœç´¢

2. **åŠ¨æ€ç½‘ç»œ**:
   - **åŸç†**: æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ç½‘ç»œç»“æ„
   - **åº”ç”¨**: æ¡ä»¶è®¡ç®—ã€åŠ¨æ€è·¯ç”±

---

## ğŸ’» **ç®—æ³•å®ç° / Algorithm Implementation**

### ç®—æ³• 5.2.1 (è‡ªé€‚åº”Adamä¼˜åŒ–å™¨ / Adaptive Adam Optimizer)

```python
import torch
import torch.nn as nn
from typing import Dict, List
import numpy as np

class AdaptiveAdamOptimizer:
    """è‡ªé€‚åº”Adamä¼˜åŒ–å™¨ - ç»“åˆå¤šç§è‡ªé€‚åº”ç­–ç•¥"""

    def __init__(self, params, lr: float = 0.001, betas: tuple = (0.9, 0.999),
                 eps: float = 1e-8, weight_decay: float = 0.0,
                 adaptive_strategy: str = 'loss_based'):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”Adamä¼˜åŒ–å™¨

        Args:
            params: æ¨¡å‹å‚æ•°
            lr: åˆå§‹å­¦ä¹ ç‡
            betas: Adamçš„åŠ¨é‡å‚æ•°
            eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
            weight_decay: æƒé‡è¡°å‡ç³»æ•°
            adaptive_strategy: è‡ªé€‚åº”ç­–ç•¥ ('loss_based', 'gradient_based', 'hybrid')
        """
        self.params = list(params)
        self.lr = lr
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        self.adaptive_strategy = adaptive_strategy

        # AdamçŠ¶æ€
        self.state = {}
        for param in self.params:
            self.state[param] = {
                'step': 0,
                'exp_avg': torch.zeros_like(param.data),
                'exp_avg_sq': torch.zeros_like(param.data)
            }

        # è‡ªé€‚åº”çŠ¶æ€
        self.loss_history = []
        self.gradient_norm_history = []
        self.learning_rate_history = []

    def step(self, closure=None):
        """æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–"""
        loss = None
        if closure is not None:
            loss = closure()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        total_norm = 0.0
        for param in self.params:
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1. / 2)
        self.gradient_norm_history.append(total_norm)

        # è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
        adaptive_lr = self._adaptive_lr_update(loss, total_norm)

        # æ›´æ–°å‚æ•°
        for param in self.params:
            if param.grad is None:
                continue

            grad = param.grad.data
            state = self.state[param]

            # æƒé‡è¡°å‡
            if self.weight_decay != 0:
                grad = grad.add(param.data, alpha=self.weight_decay)

            # æ›´æ–°çŠ¶æ€
            state['step'] += 1
            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']

            beta1, beta2 = self.betas
            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)

            # åå·®ä¿®æ­£
            bias_correction1 = 1 - beta1 ** state['step']
            bias_correction2 = 1 - beta2 ** state['step']

            # è®¡ç®—æ›´æ–°
            step_size = adaptive_lr / bias_correction1
            denom = (exp_avg_sq.sqrt() / np.sqrt(bias_correction2)).add_(self.eps)

            param.data.addcdiv_(exp_avg, denom, value=-step_size)

        self.learning_rate_history.append(adaptive_lr)
        return loss

    def _adaptive_lr_update(self, loss, gradient_norm):
        """è‡ªé€‚åº”å­¦ä¹ ç‡æ›´æ–°"""
        if self.adaptive_strategy == 'loss_based':
            return self._loss_based_lr(loss)
        elif self.adaptive_strategy == 'gradient_based':
            return self._gradient_based_lr(gradient_norm)
        elif self.adaptive_strategy == 'hybrid':
            return self._hybrid_lr(loss, gradient_norm)
        else:
            return self.lr

    def _loss_based_lr(self, loss):
        """åŸºäºæŸå¤±çš„è‡ªé€‚åº”å­¦ä¹ ç‡"""
        if loss is None or len(self.loss_history) == 0:
            self.loss_history.append(loss.item() if loss is not None else 0.0)
            return self.lr

        current_loss = loss.item()
        self.loss_history.append(current_loss)

        # å¦‚æœæŸå¤±å¢åŠ ï¼Œé™ä½å­¦ä¹ ç‡
        if len(self.loss_history) > 1 and current_loss > self.loss_history[-2]:
            self.lr *= 0.9
        # å¦‚æœæŸå¤±æŒç»­ä¸‹é™ï¼Œå¯ä»¥ç¨å¾®å¢åŠ å­¦ä¹ ç‡
        elif len(self.loss_history) > 5:
            recent_improvement = (self.loss_history[-6] - current_loss) / self.loss_history[-6]
            if recent_improvement > 0.1:
                self.lr *= 1.01

        return max(self.lr, 1e-6)  # æœ€å°å­¦ä¹ ç‡

    def _gradient_based_lr(self, gradient_norm):
        """åŸºäºæ¢¯åº¦çš„è‡ªé€‚åº”å­¦ä¹ ç‡"""
        if len(self.gradient_norm_history) == 0:
            return self.lr

        # æ¢¯åº¦çˆ†ç‚¸æ—¶é™ä½å­¦ä¹ ç‡
        if gradient_norm > 10.0:
            self.lr *= 0.5
        # æ¢¯åº¦æ¶ˆå¤±æ—¶å¢åŠ å­¦ä¹ ç‡
        elif gradient_norm < 0.01:
            self.lr *= 1.1

        return max(self.lr, 1e-6)

    def _hybrid_lr(self, loss, gradient_norm):
        """æ··åˆç­–ç•¥çš„è‡ªé€‚åº”å­¦ä¹ ç‡"""
        loss_lr = self._loss_based_lr(loss)
        grad_lr = self._gradient_based_lr(gradient_norm)
        return (loss_lr + grad_lr) / 2
```

### ç®—æ³• 5.2.2 (è‡ªé€‚åº”æ­£åˆ™åŒ–ä¼˜åŒ–å™¨ / Adaptive Regularization Optimizer)

```python
import torch
import torch.nn as nn
from typing import Dict

class AdaptiveRegularizationOptimizer:
    """è‡ªé€‚åº”æ­£åˆ™åŒ–ä¼˜åŒ–å™¨"""

    def __init__(self, model: nn.Module, base_optimizer,
                 initial_dropout_rate: float = 0.5,
                 initial_weight_decay: float = 1e-4):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”æ­£åˆ™åŒ–ä¼˜åŒ–å™¨

        Args:
            model: æ¨¡å‹
            base_optimizer: åŸºç¡€ä¼˜åŒ–å™¨
            initial_dropout_rate: åˆå§‹Dropoutç‡
            initial_weight_decay: åˆå§‹æƒé‡è¡°å‡
        """
        self.model = model
        self.base_optimizer = base_optimizer
        self.dropout_rate = initial_dropout_rate
        self.weight_decay = initial_weight_decay

        self.train_loss_history = []
        self.val_loss_history = []
        self.overfitting_history = []

    def step(self, train_loss, val_loss=None):
        """æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–"""
        # æ›´æ–°åŸºç¡€ä¼˜åŒ–å™¨
        self.base_optimizer.step()

        # è®°å½•æŸå¤±
        self.train_loss_history.append(train_loss.item() if hasattr(train_loss, 'item') else train_loss)
        if val_loss is not None:
            self.val_loss_history.append(val_loss.item() if hasattr(val_loss, 'item') else val_loss)

            # è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦
            overfitting = train_loss - val_loss if val_loss < train_loss else 0
            self.overfitting_history.append(overfitting)

            # è‡ªé€‚åº”è°ƒæ•´æ­£åˆ™åŒ–
            self._adaptive_regularization_update(overfitting)

    def _adaptive_regularization_update(self, overfitting):
        """è‡ªé€‚åº”æ­£åˆ™åŒ–æ›´æ–°"""
        if len(self.overfitting_history) < 2:
            return

        # å¦‚æœè¿‡æ‹Ÿåˆå¢åŠ ï¼Œå¢å¼ºæ­£åˆ™åŒ–
        if self.overfitting_history[-1] > self.overfitting_history[-2]:
            self.dropout_rate = min(self.dropout_rate * 1.1, 0.9)
            self.weight_decay *= 1.1

            # æ›´æ–°æ¨¡å‹ä¸­çš„Dropoutå±‚
            for module in self.model.modules():
                if isinstance(module, nn.Dropout):
                    module.p = self.dropout_rate
        # å¦‚æœè¿‡æ‹Ÿåˆå‡å°‘ï¼Œå‡å¼±æ­£åˆ™åŒ–
        elif len(self.overfitting_history) > 5:
            recent_avg = sum(self.overfitting_history[-5:]) / 5
            if recent_avg < 0.1:
                self.dropout_rate = max(self.dropout_rate * 0.95, 0.1)
                self.weight_decay *= 0.95

                # æ›´æ–°æ¨¡å‹ä¸­çš„Dropoutå±‚
                for module in self.model.modules():
                    if isinstance(module, nn.Dropout):
                        module.p = self.dropout_rate
```

### ç®—æ³• 5.2.3 (è‡ªé€‚åº”æ•°æ®é‡‡æ ·è®­ç»ƒ / Adaptive Data Sampling Training)

```python
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Callable
import numpy as np

class AdaptiveSampler:
    """è‡ªé€‚åº”æ•°æ®é‡‡æ ·å™¨"""

    def __init__(self, dataset: Dataset, loss_fn: Callable,
                 initial_sampling_rate: float = 1.0,
                 importance_power: float = 1.0):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”é‡‡æ ·å™¨

        Args:
            dataset: æ•°æ®é›†
            loss_fn: æŸå¤±å‡½æ•°
            initial_sampling_rate: åˆå§‹é‡‡æ ·ç‡
            importance_power: é‡è¦æ€§å¹‚æ¬¡
        """
        self.dataset = dataset
        self.loss_fn = loss_fn
        self.sampling_rate = initial_sampling_rate
        self.importance_power = importance_power

        self.sample_losses = {}
        self.sample_importance = {}

    def compute_importance(self, model, device='cuda'):
        """è®¡ç®—æ ·æœ¬é‡è¦æ€§"""
        model.eval()
        importance_scores = []

        with torch.no_grad():
            for idx in range(len(self.dataset)):
                data, target = self.dataset[idx]
                data = data.unsqueeze(0).to(device)
                target = target.unsqueeze(0).to(device)

                output = model(data)
                loss = self.loss_fn(output, target)

                importance = loss.item() ** self.importance_power
                importance_scores.append(importance)
                self.sample_losses[idx] = loss.item()
                self.sample_importance[idx] = importance

        return importance_scores

    def sample_indices(self, num_samples: int):
        """æ ¹æ®é‡è¦æ€§é‡‡æ ·ç´¢å¼•"""
        if len(self.sample_importance) == 0:
            # å¦‚æœè¿˜æ²¡æœ‰è®¡ç®—é‡è¦æ€§ï¼Œéšæœºé‡‡æ ·
            return np.random.choice(len(self.dataset), num_samples, replace=False)

        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        importance_array = np.array([self.sample_importance.get(i, 1.0)
                                    for i in range(len(self.dataset))])
        probabilities = importance_array / importance_array.sum()

        # æ ¹æ®é‡è¦æ€§é‡‡æ ·
        num_samples_to_select = int(num_samples * self.sampling_rate)
        sampled_indices = np.random.choice(
            len(self.dataset),
            num_samples_to_select,
            replace=False,
            p=probabilities
        )

        return sampled_indices
```

---

## ğŸ“Š **å¤æ‚åº¦åˆ†æ / Complexity Analysis**

- **æ—¶é—´å¤æ‚åº¦**: $O(P)$ å…¶ä¸­ $P$ æ˜¯å‚æ•°æ•°é‡
- **ç©ºé—´å¤æ‚åº¦**: $O(P)$

---

## ğŸ’¼ **å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-World Applications**

### æ¡ˆä¾‹1: GNNè®­ç»ƒä¸­çš„è‡ªé€‚åº”ä¼˜åŒ–

- **é—®é¢˜**: GNNè®­ç»ƒä¸ç¨³å®šï¼Œéœ€è¦è‡ªé€‚åº”è°ƒæ•´
- **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡
- **æ•ˆæœ**: è®­ç»ƒç¨³å®šæ€§æé«˜30%ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜20%

---

## ğŸ”— **ç›¸å…³é“¾æ¥ / Related Links**

- [AIç½‘ç»œä¸è‡ªé€‚åº”èŒƒç•´ä¸»ç›®å½•](../../README.md)
- [é«˜çº§ç†è®ºç›®å½•](../README.md)
- [è‡ªé€‚åº”ç½‘ç»œæ¶æ„æœç´¢](01-è‡ªé€‚åº”ç½‘ç»œæ¶æ„æœç´¢.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… **å·²å®Œæˆ**
