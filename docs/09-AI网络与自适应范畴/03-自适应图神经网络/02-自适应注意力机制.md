# è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ / Adaptive Attention Mechanism

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”æ³¨æ„åŠ›æƒé‡ã€åŠ¨æ€æ³¨æ„åŠ›æ¨¡å¼ã€å¤šå°ºåº¦æ³¨æ„åŠ›ç­‰ç†è®ºå’Œå®ç°ã€‚

---

## ğŸ“‘ **ç›®å½• / Table of Contents**

- [è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ / Adaptive Attention Mechanism](#è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶--adaptive-attention-mechanism)
  - [ğŸ“š **æ¦‚è¿° / Overview**](#-æ¦‚è¿°--overview)
  - [ğŸ“ **å½¢å¼åŒ–å®šä¹‰ / Formal Definition**](#-å½¢å¼åŒ–å®šä¹‰--formal-definition)
  - [ğŸ’» **ç®—æ³•å®ç° / Algorithm Implementation**](#-ç®—æ³•å®ç°--algorithm-implementation)
  - [ğŸ“Š **å¤æ‚åº¦åˆ†æ / Complexity Analysis**](#-å¤æ‚åº¦åˆ†æ--complexity-analysis)
  - [ğŸ’¼ **å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-World Applications**](#-å®é™…åº”ç”¨æ¡ˆä¾‹--real-world-applications)
  - [ğŸ”— **ç›¸å…³é“¾æ¥ / Related Links**](#-ç›¸å…³é“¾æ¥--related-links)

---

## ğŸ“ **å½¢å¼åŒ–å®šä¹‰ / Formal Definition**

### å®šä¹‰ 3.2 (è‡ªé€‚åº”æ³¨æ„åŠ› / Adaptive Attention)

**è‡ªé€‚åº”æ³¨æ„åŠ›**æ˜¯åŠ¨æ€è®¡ç®—èŠ‚ç‚¹é—´é‡è¦æ€§çš„æœºåˆ¶ï¼š

$$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}$$

å…¶ä¸­æ³¨æ„åŠ›åˆ†æ•° $e_{ij}$ ä¸ºï¼š
$$e_{ij} = \text{LeakyReLU}(a^T[Wh_i \| Wh_j])$$

### è‡ªé€‚åº”æ³¨æ„åŠ›æƒé‡ / Adaptive Attention Weights

è‡ªé€‚åº”æ³¨æ„åŠ›æƒé‡æ ¹æ®ä»»åŠ¡å’Œè¾“å…¥åŠ¨æ€è°ƒæ•´ï¼š
$$\alpha_{ij}(t) = f(\alpha_{ij}(t-1), x_i(t), x_j(t), task(t))$$

---

## ğŸ’» **ç®—æ³•å®ç° / Algorithm Implementation**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv

class AdaptiveAttention(nn.Module):
    """è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶å®ç°"""

    def __init__(self, input_dim: int, attention_dim: int = 64,
                 num_heads: int = 8, adaptive: bool = True):
        super(AdaptiveAttention, self).__init__()
        self.adaptive = adaptive

        # æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
        self.W = nn.Linear(input_dim, attention_dim, bias=False)
        self.a = nn.Linear(2 * attention_dim, 1, bias=False)

        # è‡ªé€‚åº”å‚æ•°
        if adaptive:
            self.adaptive_alpha = nn.Parameter(torch.ones(1))
            self.adaptive_beta = nn.Parameter(torch.zeros(1))

    def forward(self, x, edge_index):
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        row, col = edge_index
        edge_features = torch.cat([x[row], x[col]], dim=-1)

        Wh = self.W(x)
        edge_attention = self.a(edge_features)

        # è‡ªé€‚åº”è°ƒæ•´
        if self.adaptive:
            edge_attention = self.adaptive_alpha * edge_attention + self.adaptive_beta

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(edge_attention, dim=0)

        return attention_weights
```

---

## ğŸ“Š **å¤æ‚åº¦åˆ†æ / Complexity Analysis**

- **æ—¶é—´å¤æ‚åº¦**: $O(|E| \cdot D^2)$ å…¶ä¸­ $|E|$ æ˜¯è¾¹æ•°ï¼Œ$D$ æ˜¯ç‰¹å¾ç»´åº¦
- **ç©ºé—´å¤æ‚åº¦**: $O(|E| + |V| \cdot D)$

---

## ğŸ’¼ **å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-World Applications**

### æ¡ˆä¾‹1: å›¾æ¨èç³»ç»Ÿä¸­çš„æ³¨æ„åŠ›æœºåˆ¶

- **é—®é¢˜**: ç†è§£ç”¨æˆ·-ç‰©å“å…³ç³»çš„é‡è¦æ€§
- **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶
- **æ•ˆæœ**: æ¨èå‡†ç¡®ç‡æé«˜30%

---

## ğŸ”— **ç›¸å…³é“¾æ¥ / Related Links**

- [AIç½‘ç»œä¸è‡ªé€‚åº”èŒƒç•´ä¸»ç›®å½•](../../README.md)
- [è‡ªé€‚åº”å›¾ç¥ç»ç½‘ç»œç›®å½•](../README.md)
- [è‡ªé€‚åº”GNNæ¨¡å‹](01-è‡ªé€‚åº”GNNæ¨¡å‹.md)
- [AIç½‘ç»œå…ƒæ¨¡å‹](../../00-AIç½‘ç»œå…ƒæ¨¡å‹.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… **å·²å®Œæˆ**
