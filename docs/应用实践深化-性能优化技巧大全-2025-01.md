# åº”ç”¨å®è·µæ·±åŒ– - æ€§èƒ½ä¼˜åŒ–æŠ€å·§å¤§å…¨ / Application Practice Deepening - Performance Optimization Tips Collection

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›PGTã€Emmaã€GraphGPTã€GPSã€Mamba2äº”ä¸ªä¸“é¢˜çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼ŒåŒ…æ‹¬è®­ç»ƒä¼˜åŒ–ã€æ¨ç†ä¼˜åŒ–ã€å†…å­˜ä¼˜åŒ–å’Œç³»ç»Ÿä¼˜åŒ–ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## ğŸš€ **ä¸€ã€è®­ç»ƒæ€§èƒ½ä¼˜åŒ– / Training Performance Optimization**

### 1.1 æ•°æ®åŠ è½½ä¼˜åŒ–

**å¤šè¿›ç¨‹æ•°æ®åŠ è½½**:
```python
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,
    pin_memory=True,
    prefetch_factor=2
)
```

**æ··åˆç²¾åº¦è®­ç»ƒ**:
```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
with autocast():
    output = model(batch)
    loss = criterion(output, batch.y)
scaler.scale(loss).backward()
```

### 1.2 æ¢¯åº¦ç´¯ç§¯

```python
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## âš¡ **äºŒã€æ¨ç†æ€§èƒ½ä¼˜åŒ– / Inference Performance Optimization**

### 2.1 æ¨¡å‹é‡åŒ–

```python
model_quantized = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### 2.2 æ¨¡å‹ç¼–è¯‘

```python
model_compiled = torch.compile(model, mode='max-autotune')
```

---

## ğŸ’¾ **ä¸‰ã€å†…å­˜ä¼˜åŒ– / Memory Optimization**

### 3.1 æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
from torch.utils.checkpoint import checkpoint
output = checkpoint(model.forward, x)
```

### 3.2 CPUå¸è½½

```python
large_tensor = x.cpu()
small_tensor = x[:100].cuda()
```

---

## ğŸ”§ **å››ã€åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ– / Distributed Training Optimization**

### 4.1 é€šä¿¡ä¼˜åŒ–

```python
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks
model.register_comm_hook(None, default_hooks.fp16_compress_hook)
```

---

## ğŸ“Š **äº”ã€ä¸“é¢˜ç‰¹å®šä¼˜åŒ– / Topic-Specific Optimization**

### 5.1 PGTä¼˜åŒ–

çº¿æ€§æ³¨æ„åŠ›å®ç°O(n)å¤æ‚åº¦ï¼Œé€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚

### 5.2 Emmaä¼˜åŒ–

æºèŠ‚ç‚¹åˆ†å—å‡å°‘é€šä¿¡å¼€é”€40-60%ã€‚

### 5.3 GraphGPTä¼˜åŒ–

ç¼“å­˜åºåˆ—åŒ–ç»“æœï¼Œå‡å°‘80-90%åºåˆ—åŒ–æ—¶é—´ã€‚

### 5.4 GPSä¼˜åŒ–

è‡ªé€‚åº”èåˆæƒé‡ï¼Œæ€§èƒ½æå‡5-10%ã€‚

### 5.5 Mamba2ä¼˜åŒ–

å¹¶è¡ŒS4è®¡ç®—ï¼Œé€Ÿåº¦æå‡2-3å€ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… å®Œæˆ


## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›PGTã€Emmaã€GraphGPTã€GPSã€Mamba2äº”ä¸ªä¸“é¢˜çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼ŒåŒ…æ‹¬è®­ç»ƒä¼˜åŒ–ã€æ¨ç†ä¼˜åŒ–ã€å†…å­˜ä¼˜åŒ–å’Œç³»ç»Ÿä¼˜åŒ–ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## ğŸš€ **ä¸€ã€è®­ç»ƒæ€§èƒ½ä¼˜åŒ– / Training Performance Optimization**

### 1.1 æ•°æ®åŠ è½½ä¼˜åŒ–

**å¤šè¿›ç¨‹æ•°æ®åŠ è½½**:

```python
# ä¼˜åŒ–å‰
dataloader = DataLoader(dataset, batch_size=32, num_workers=0)

# ä¼˜åŒ–å
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,  # å¢åŠ workeræ•°é‡
    pin_memory=True,  # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
    prefetch_factor=2,  # é¢„å–æ•°æ®
    persistent_workers=True  # ä¿æŒworkerè¿›ç¨‹
)
```

**æ•°æ®é¢„å¤„ç†ä¼˜åŒ–**:

```python
# ä¼˜åŒ–å‰ï¼šæ¯æ¬¡è¿­ä»£éƒ½é¢„å¤„ç†
for batch in dataloader:
    batch = preprocess(batch)  # æ…¢
    output = model(batch)

# ä¼˜åŒ–åï¼šé¢„å¤„ç†æ”¾åœ¨æ•°æ®åŠ è½½æ—¶
class PreprocessedDataset(Dataset):
    def __init__(self, data):
        self.data = [preprocess(item) for item in data]  # é¢„å¤„ç†ä¸€æ¬¡

    def __getitem__(self, idx):
        return self.data[idx]  # ç›´æ¥è¿”å›é¢„å¤„ç†åçš„æ•°æ®
```

### 1.2 æ¨¡å‹è®­ç»ƒä¼˜åŒ–

**æ··åˆç²¾åº¦è®­ç»ƒ**:

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    # ä½¿ç”¨æ··åˆç²¾åº¦
    with autocast():
        output = model(batch)
        loss = criterion(output, batch.y)

    # ç¼©æ”¾æ¢¯åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

    # æ€§èƒ½æå‡ï¼š30-50%
```

**æ¢¯åº¦ç´¯ç§¯**:

```python
# æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps

optimizer.zero_grad()
for i, batch in enumerate(dataloader):
    output = model(batch)
    loss = criterion(output, batch.y) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æ¢¯åº¦æ£€æŸ¥ç‚¹**:

```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœå†…å­˜
model.gradient_checkpointing_enable()

# å†…å­˜èŠ‚çœï¼š40-50%ï¼Œé€Ÿåº¦é™ä½ï¼š10-20%
```

### 1.3 ä¼˜åŒ–å™¨ä¼˜åŒ–

**AdamWä¼˜åŒ–å™¨è°ƒä¼˜**:

```python
# ä¼˜åŒ–å‰
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# ä¼˜åŒ–å
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0.01  # æƒé‡è¡°å‡
)

# æ€§èƒ½æå‡ï¼š5-10%
```

**å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–**:

```python
# ä½™å¼¦é€€ç«è°ƒåº¦
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=100,
    eta_min=1e-6
)

# æˆ–ä½¿ç”¨OneCycleLR
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=1e-3,
    total_steps=1000,
    pct_start=0.1
)

# æ€§èƒ½æå‡ï¼š10-15%
```

---

## âš¡ **äºŒã€æ¨ç†æ€§èƒ½ä¼˜åŒ– / Inference Performance Optimization**

### 2.1 æ¨¡å‹å‹ç¼©

**é‡åŒ–å‹ç¼©**:

```python
# åŠ¨æ€é‡åŒ–
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},
    dtype=torch.qint8
)

# æ€§èƒ½æå‡ï¼š2-4å€åŠ é€Ÿï¼Œå†…å­˜å‡å°‘ï¼š50-75%

# é™æ€é‡åŒ–
model.eval()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
# æ ¡å‡†æ•°æ®
torch.quantization.convert(model, inplace=True)

# æ€§èƒ½æå‡ï¼š3-5å€åŠ é€Ÿï¼Œå†…å­˜å‡å°‘ï¼š75%
```

**çŸ¥è¯†è’¸é¦**:

```python
# ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰è®­ç»ƒå°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰
teacher_model = LargeModel()
student_model = SmallModel()

# è’¸é¦æŸå¤±
def distillation_loss(student_output, teacher_output, labels, temperature=3.0):
    # è½¯æ ‡ç­¾æŸå¤±
    soft_loss = F.kl_div(
        F.log_softmax(student_output / temperature, dim=1),
        F.softmax(teacher_output / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature ** 2)

    # ç¡¬æ ‡ç­¾æŸå¤±
    hard_loss = F.cross_entropy(student_output, labels)

    return 0.7 * soft_loss + 0.3 * hard_loss

# æ¨¡å‹å¤§å°å‡å°‘ï¼š50-70%ï¼Œæ€§èƒ½ä¿æŒï¼š95-98%
```

**æ¨¡å‹å‰ªæ**:

```python
import torch.nn.utils.prune as prune

# ç»“æ„åŒ–å‰ªæ
prune.ln_structured(
    model.layer,
    name='weight',
    amount=0.5,
    n=2,
    dim=0
)

# éç»“æ„åŒ–å‰ªæ
prune.l1_unstructured(
    model.layer,
    name='weight',
    amount=0.3
)

# æ¨¡å‹å¤§å°å‡å°‘ï¼š30-50%ï¼Œæ€§èƒ½ä¿æŒï¼š90-95%
```

### 2.2 æ‰¹å¤„ç†ä¼˜åŒ–

**åŠ¨æ€æ‰¹å¤„ç†**:

```python
def dynamic_batch_inference(model, data_list, max_batch_size=64):
    """åŠ¨æ€æ‰¹å¤„ç†æ¨ç†"""
    results = []

    # æŒ‰å¤§å°æ’åº
    sorted_data = sorted(data_list, key=len, reverse=True)

    # æ‰¹å¤„ç†
    batch = []
    batch_size = 0

    for item in sorted_data:
        item_size = len(item)
        if batch_size + item_size > max_batch_size and batch:
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            results.extend(model(torch.stack(batch)))
            batch = []
            batch_size = 0

        batch.append(item)
        batch_size += item_size

    # å¤„ç†æœ€åä¸€æ‰¹
    if batch:
        results.extend(model(torch.stack(batch)))

    return results

# ååé‡æå‡ï¼š2-3å€
```

**å¼‚æ­¥æ¨ç†**:

```python
import asyncio
import torch.cuda.streams as streams

async def async_inference(model, data_queue, result_queue):
    """å¼‚æ­¥æ¨ç†"""
    stream = streams.Stream()

    while True:
        data = await data_queue.get()
        if data is None:
            break

        with streams.stream(stream):
            result = model(data)
            result_queue.put_nowait(result)

# å»¶è¿Ÿé™ä½ï¼š30-50%
```

### 2.3 æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–

**TorchScriptç¼–è¯‘**:

```python
# ç¼–è¯‘æ¨¡å‹
model_scripted = torch.jit.script(model)

# æˆ–ä½¿ç”¨trace
example_input = torch.randn(1, 10, 768)
model_traced = torch.jit.trace(model, example_input)

# æ€§èƒ½æå‡ï¼š10-30%
```

**Torch Compileï¼ˆPyTorch 2.0+ï¼‰**:

```python
# ä½¿ç”¨torch.compile
model_compiled = torch.compile(model, mode='max-autotune')

# æ€§èƒ½æå‡ï¼š20-50%
```

---

## ğŸ’¾ **ä¸‰ã€å†…å­˜ä¼˜åŒ– / Memory Optimization**

### 3.1 æ¿€æ´»æ£€æŸ¥ç‚¹

```python
from torch.utils.checkpoint import checkpoint

# ä½¿ç”¨æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
def forward_with_checkpoint(self, x):
    return checkpoint(self.forward, x)

# å†…å­˜èŠ‚çœï¼š40-50%ï¼Œé€Ÿåº¦é™ä½ï¼š10-20%
```

### 3.2 æ¢¯åº¦ç´¯ç§¯

```python
# æ¢¯åº¦ç´¯ç§¯å‡å°‘å†…å­˜å³°å€¼
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    output = model(batch)
    loss = criterion(output, batch.y) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# å†…å­˜èŠ‚çœï¼š50-75%
```

### 3.3 CPUå¸è½½

```python
# å°†éƒ¨åˆ†æ•°æ®ç§»åˆ°CPU
def forward(self, x):
    # å¤§å¼ é‡åœ¨CPU
    large_tensor = x.cpu()

    # å°å¼ é‡åœ¨GPU
    small_tensor = x[:100].cuda()

    # è®¡ç®—
    result = self.compute(small_tensor, large_tensor)

    return result

# å†…å­˜èŠ‚çœï¼š30-50%
```

---

## ğŸ”§ **å››ã€åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ– / Distributed Training Optimization**

### 4.1 é€šä¿¡ä¼˜åŒ–

**æ¢¯åº¦å‹ç¼©**:

```python
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks

# ä½¿ç”¨FP16å‹ç¼©
model.register_comm_hook(
    None,
    default_hooks.fp16_compress_hook
)

# é€šä¿¡é‡å‡å°‘ï¼š50%ï¼Œæ€§èƒ½æå‡ï¼š10-20%
```

**å¼‚æ­¥é€šä¿¡**:

```python
# ä½¿ç”¨å¼‚æ­¥allreduce
from torch.distributed import all_reduce

# å¼‚æ­¥é€šä¿¡
handle = all_reduce(gradients, async_op=True)
# ç»§ç»­è®¡ç®—
# ...
handle.wait()

# æ€§èƒ½æå‡ï¼š15-25%
```

### 4.2 æ•°æ®å¹¶è¡Œä¼˜åŒ–

**æ•°æ®åˆ†ç‰‡**:

```python
# æ•°æ®åˆ†ç‰‡å‡å°‘å†…å­˜
from torch.nn.parallel import DistributedDataParallel as DDP

model = DDP(model, device_ids=[rank])

# æ¯ä¸ªGPUåªå­˜å‚¨éƒ¨åˆ†æ•°æ®
# å†…å­˜èŠ‚çœï¼š50-75%
```

---

## ğŸ“Š **äº”ã€ç³»ç»Ÿçº§ä¼˜åŒ– / System-Level Optimization**

### 5.1 GPUåˆ©ç”¨ç‡ä¼˜åŒ–

**å¤šæµå¤„ç†**:

```python
import torch.cuda.streams as streams

stream1 = streams.Stream()
stream2 = streams.Stream()

# å¹¶è¡Œå¤„ç†
with streams.stream(stream1):
    result1 = model1(data1)

with streams.stream(stream2):
    result2 = model2(data2)

# GPUåˆ©ç”¨ç‡æå‡ï¼š20-40%
```

**æ‰¹å¤§å°è‡ªåŠ¨è°ƒæ•´**:

```python
def auto_batch_size(model, dataloader, start_batch_size=32):
    """è‡ªåŠ¨è°ƒæ•´æ‰¹å¤§å°"""
    batch_size = start_batch_size

    while True:
        try:
            # å°è¯•å½“å‰æ‰¹å¤§å°
            batch = next(iter(DataLoader(
                dataloader.dataset,
                batch_size=batch_size
            )))
            model(batch)
            break
        except RuntimeError as e:
            if 'out of memory' in str(e):
                batch_size //= 2
                torch.cuda.empty_cache()
            else:
                raise

    return batch_size

# è‡ªåŠ¨æ‰¾åˆ°æœ€å¤§æ‰¹å¤§å°
optimal_batch_size = auto_batch_size(model, dataloader)
```

### 5.2 I/Oä¼˜åŒ–

**æ•°æ®é¢„å–**:

```python
# ä½¿ç”¨é¢„å–é˜Ÿåˆ—
from queue import Queue
import threading

def data_prefetcher(dataloader, queue, maxsize=10):
    """æ•°æ®é¢„å–å™¨"""
    for batch in dataloader:
        queue.put(batch)
    queue.put(None)

queue = Queue(maxsize=10)
thread = threading.Thread(
    target=data_prefetcher,
    args=(dataloader, queue)
)
thread.start()

# ä½¿ç”¨é¢„å–çš„æ•°æ®
while True:
    batch = queue.get()
    if batch is None:
        break
    # å¤„ç†batch

# I/Oç­‰å¾…æ—¶é—´å‡å°‘ï¼š50-70%
```

---

## ğŸ¯ **å…­ã€ä¸“é¢˜ç‰¹å®šä¼˜åŒ– / Topic-Specific Optimization**

### 6.1 PGTä¼˜åŒ–

**çº¿æ€§æ³¨æ„åŠ›ä¼˜åŒ–**:

```python
# ä½¿ç”¨ç‰¹å¾æ˜ å°„å®ç°çº¿æ€§å¤æ‚åº¦
def linear_attention(q, k, v):
    # ç‰¹å¾æ˜ å°„
    q = F.elu(q) + 1
    k = F.elu(k) + 1

    # çº¿æ€§å¤æ‚åº¦è®¡ç®—
    kv = torch.einsum('bshd,bshv->bhdv', k, v)
    z = torch.einsum('bshd,bhdv->bshv', q, kv)

    # å½’ä¸€åŒ–
    normalizer = torch.einsum('bshd->bsh', q).unsqueeze(-1) + 1e-8
    z = z / normalizer

    return z

# å¤æ‚åº¦ï¼šO(n) è€Œä¸æ˜¯ O(nÂ²)
```

### 6.2 Emmaä¼˜åŒ–

**æºèŠ‚ç‚¹åˆ†å—ä¼˜åŒ–**:

```python
# æŒ‰åº¦æ’åºåˆ†å—
def degree_based_blocking(graph, block_size=10000):
    degrees = graph.degree
    sorted_indices = torch.argsort(degrees, descending=True)

    blocks = []
    for i in range(0, len(sorted_indices), block_size):
        blocks.append(sorted_indices[i:i+block_size])

    return blocks

# é€šä¿¡å¼€é”€å‡å°‘ï¼š40-60%
```

### 6.3 GraphGPTä¼˜åŒ–

**å›¾åºåˆ—åŒ–ä¼˜åŒ–**:

```python
# ç¼“å­˜åºåˆ—åŒ–ç»“æœ
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_sequence_graph(graph_hash, method='bfs'):
    """ç¼“å­˜åºåˆ—åŒ–ç»“æœ"""
    return sequence_graph(graph, method)

# åºåˆ—åŒ–æ—¶é—´å‡å°‘ï¼š80-90%
```

### 6.4 GPSä¼˜åŒ–

**å±€éƒ¨-å…¨å±€èåˆä¼˜åŒ–**:

```python
# è‡ªé€‚åº”èåˆæƒé‡
class AdaptiveFusion(nn.Module):
    def __init__(self):
        super().__init__()
        self.fusion_weight = nn.Parameter(torch.tensor(0.5))

    def forward(self, local_out, global_out):
        # è‡ªé€‚åº”èåˆ
        fused = (self.fusion_weight * local_out +
                (1 - self.fusion_weight) * global_out)
        return fused

# æ€§èƒ½æå‡ï¼š5-10%
```

### 6.5 Mamba2ä¼˜åŒ–

**S4çŠ¶æ€ç©ºé—´ä¼˜åŒ–**:

```python
# å¹¶è¡ŒåŒ–S4è®¡ç®—
def parallel_s4_forward(self, x):
    """å¹¶è¡ŒS4å‰å‘ä¼ æ’­"""
    # ä½¿ç”¨å¹¶è¡Œæ‰«æç®—æ³•
    # å¤æ‚åº¦ï¼šO(n log n) è€Œä¸æ˜¯ O(nÂ²)
    return parallel_scan(x, self.A, self.B, self.C)

# é€Ÿåº¦æå‡ï¼š2-3å€
```

---

## ğŸ“ˆ **ä¸ƒã€æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜ / Performance Monitoring and Tuning**

### 7.1 æ€§èƒ½åˆ†æ

```python
import torch.profiler as profiler

# æ€§èƒ½åˆ†æ
with profiler.profile(
    activities=[
        profiler.ProfilerActivity.CPU,
        profiler.ProfilerActivity.CUDA
    ],
    record_shapes=True,
    profile_memory=True
) as prof:
    output = model(batch)

# ç”ŸæˆæŠ¥å‘Š
print(prof.key_averages().table(sort_by="cuda_time_total"))
prof.export_chrome_trace("trace.json")
```

### 7.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def benchmark_model(model, dataloader, num_iterations=100):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    model.eval()

    # é¢„çƒ­
    for _ in range(10):
        _ = model(next(iter(dataloader)))

    # æµ‹è¯•
    import time
    times = []

    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= num_iterations:
                break

            start = time.time()
            _ = model(batch)
            torch.cuda.synchronize()
            elapsed = time.time() - start
            times.append(elapsed)

    avg_time = sum(times) / len(times)
    throughput = len(dataloader.dataset) / (avg_time * num_iterations)

    return {
        'avg_time': avg_time,
        'throughput': throughput,
        'fps': 1.0 / avg_time
    }
```

---

## ğŸ“ **å…«ã€ä¼˜åŒ–æŠ€å·§æ€»ç»“ / Optimization Tips Summary**

### 8.1 å¿«é€Ÿä¼˜åŒ–æ¸…å•

- [ ] âœ… ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæå‡30-50%ï¼‰
- [ ] âœ… å¢åŠ æ•°æ®åŠ è½½workeræ•°é‡ï¼ˆæå‡20-40%ï¼‰
- [ ] âœ… ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆå†…å­˜èŠ‚çœ50-75%ï¼‰
- [ ] âœ… å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå†…å­˜èŠ‚çœ40-50%ï¼‰
- [ ] âœ… ä½¿ç”¨torch.compileï¼ˆæå‡20-50%ï¼‰
- [ ] âœ… æ¨¡å‹é‡åŒ–ï¼ˆåŠ é€Ÿ2-4å€ï¼‰
- [ ] âœ… æ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆååé‡æå‡2-3å€ï¼‰
- [ ] âœ… é€šä¿¡ä¼˜åŒ–ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒæå‡10-20%ï¼‰

### 8.2 ä¼˜åŒ–ä¼˜å…ˆçº§

**é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰**:
1. æ··åˆç²¾åº¦è®­ç»ƒ
2. æ•°æ®åŠ è½½ä¼˜åŒ–
3. æ‰¹å¤§å°ä¼˜åŒ–

**ä¸­ä¼˜å…ˆçº§ï¼ˆçŸ­æœŸå®æ–½ï¼‰**:
1. æ¨¡å‹ç¼–è¯‘
2. æ¢¯åº¦ç´¯ç§¯
3. é€šä¿¡ä¼˜åŒ–

**ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰**:
1. æ¨¡å‹å‹ç¼©
2. çŸ¥è¯†è’¸é¦
3. ç³»ç»Ÿçº§ä¼˜åŒ–

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… å®Œæˆ
