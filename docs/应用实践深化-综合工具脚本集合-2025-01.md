# åº”ç”¨å®è·µæ·±åŒ– - ç»¼åˆå·¥å…·è„šæœ¬é›†åˆ / Application Practice Deepening - Comprehensive Tool Scripts Collection

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›æœ€æ–°ç ”ç©¶ä¸“é¢˜ï¼ˆPGTã€NSDI 2025åˆ†å¸ƒå¼ç³»ç»Ÿã€LLM-å›¾èåˆæ¨¡å‹ã€GPSæ¶æ„æœ€æ–°è¿›å±•ã€Mamba2ï¼‰çš„ç»¼åˆå·¥å…·è„šæœ¬é›†åˆï¼ŒåŒ…æ‹¬æ€§èƒ½è¯„ä¼°ã€æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²è‡ªåŠ¨åŒ–ç­‰å®ç”¨å·¥å…·ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§
**æœ€æ–°æ›´æ–°**: 2025å¹´1æœˆ - æ•´åˆNSDI 2025æœ€æ–°ç³»ç»Ÿå·¥å…·è„šæœ¬

---

## ğŸ› ï¸ **ä¸€ã€æ€§èƒ½è¯„ä¼°å·¥å…· / Performance Evaluation Tools**

### 1.1 ç»¼åˆæ€§èƒ½è¯„ä¼°è„šæœ¬

```python
#!/usr/bin/env python3
"""
ç»¼åˆæ€§èƒ½è¯„ä¼°å·¥å…·

è¯„ä¼°PGTã€Emmaã€GraphGPTã€GPSã€Mamba2ç­‰æ¨¡å‹çš„ç»¼åˆæ€§èƒ½
"""

import torch
import torch.nn as nn
import numpy as np
import time
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass
from pathlib import Path

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    accuracy: float
    f1_score: float
    auc_roc: float
    training_time: float
    inference_time: float
    memory_usage: float
    throughput: float

class ComprehensiveEvaluator:
    """ç»¼åˆæ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self, device='cuda'):
        self.device = device
        self.results = {}

    def evaluate_model(self,
                      model: nn.Module,
                      dataloader: torch.utils.data.DataLoader,
                      task_type: str = 'classification') -> PerformanceMetrics:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        å‚æ•°:
            model: æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            task_type: ä»»åŠ¡ç±»å‹

        è¿”å›:
            metrics: æ€§èƒ½æŒ‡æ ‡
        """
        model.eval()

        # è®­ç»ƒæ—¶é—´
        train_time = self._measure_training_time(model, dataloader)

        # æ¨ç†æ—¶é—´
        inference_time = self._measure_inference_time(model, dataloader)

        # å†…å­˜å ç”¨
        memory_usage = self._measure_memory_usage(model, dataloader)

        # å‡†ç¡®ç‡
        if task_type == 'classification':
            accuracy, f1_score = self._evaluate_classification(model, dataloader)
            auc_roc = 0.0
        elif task_type == 'link_prediction':
            accuracy, auc_roc = self._evaluate_link_prediction(model, dataloader)
            f1_score = 0.0
        else:
            accuracy = f1_score = auc_roc = 0.0

        # ååé‡
        throughput = len(dataloader.dataset) / inference_time

        return PerformanceMetrics(
            accuracy=accuracy,
            f1_score=f1_score,
            auc_roc=auc_roc,
            training_time=train_time,
            inference_time=inference_time,
            memory_usage=memory_usage,
            throughput=throughput
        )

    def _measure_training_time(self, model, dataloader):
        """æµ‹é‡è®­ç»ƒæ—¶é—´"""
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        criterion = nn.CrossEntropyLoss()

        start_time = time.time()
        for batch in dataloader:
            batch = batch.to(self.device)
            optimizer.zero_grad()
            output = model(batch)
            loss = criterion(output, batch.y)
            loss.backward()
            optimizer.step()
        elapsed_time = time.time() - start_time

        return elapsed_time

    def _measure_inference_time(self, model, dataloader):
        """æµ‹é‡æ¨ç†æ—¶é—´"""
        model.eval()
        times = []

        with torch.no_grad():
            for batch in dataloader:
                batch = batch.to(self.device)
                start_time = time.time()
                _ = model(batch)
                elapsed_time = time.time() - start_time
                times.append(elapsed_time)

        return np.mean(times)

    def _measure_memory_usage(self, model, dataloader):
        """æµ‹é‡å†…å­˜å ç”¨"""
        import psutil
        import GPUtil

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        if torch.cuda.is_available():
            gpus = GPUtil.getGPUs()
            initial_gpu_memory = gpus[0].memoryUsed if gpus else 0
        else:
            initial_gpu_memory = 0

        # è¿è¡Œä¸€ä¸ªbatch
        model.eval()
        with torch.no_grad():
            batch = next(iter(dataloader))
            batch = batch.to(self.device)
            _ = model(batch)

        peak_memory = process.memory_info().rss / 1024 / 1024
        if torch.cuda.is_available():
            gpus = GPUtil.getGPUs()
            peak_gpu_memory = gpus[0].memoryUsed if gpus else 0
        else:
            peak_gpu_memory = 0

        return {
            'cpu_memory_mb': peak_memory - initial_memory,
            'gpu_memory_mb': peak_gpu_memory - initial_gpu_memory
        }

    def _evaluate_classification(self, model, dataloader):
        """è¯„ä¼°åˆ†ç±»ä»»åŠ¡"""
        from sklearn.metrics import accuracy_score, f1_score

        predictions = []
        labels = []

        model.eval()
        with torch.no_grad():
            for batch in dataloader:
                batch = batch.to(self.device)
                output = model(batch)
                pred = output.argmax(dim=-1)
                predictions.extend(pred.cpu().numpy())
                labels.extend(batch.y.cpu().numpy())

        accuracy = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='macro')

        return accuracy, f1

    def _evaluate_link_prediction(self, model, dataloader):
        """è¯„ä¼°é“¾æ¥é¢„æµ‹ä»»åŠ¡"""
        from sklearn.metrics import roc_auc_score, accuracy_score

        scores = []
        labels = []

        model.eval()
        with torch.no_grad():
            for batch in dataloader:
                batch = batch.to(self.device)
                score = model.predict_link(batch)
                scores.extend(score.cpu().numpy())
                labels.extend(batch.edge_label.cpu().numpy())

        auc = roc_auc_score(labels, scores)
        accuracy = accuracy_score(labels, (np.array(scores) > 0.5).astype(int))

        return accuracy, auc

    def generate_report(self, output_path: str):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = {
            'summary': self._generate_summary(),
            'detailed_results': self.results,
            'comparison': self._generate_comparison(),
            'recommendations': self._generate_recommendations()
        }

        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    evaluator = ComprehensiveEvaluator()

    # è¯„ä¼°PGTæ¨¡å‹
    pgt_model = load_pgt_model()
    pgt_metrics = evaluator.evaluate_model(pgt_model, pgt_dataloader, 'classification')
    evaluator.results['PGT'] = pgt_metrics

    # è¯„ä¼°Emmaæ¨¡å‹
    emma_model = load_emma_model()
    emma_metrics = evaluator.evaluate_model(emma_model, emma_dataloader, 'classification')
    evaluator.results['Emma'] = emma_metrics

    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_report('performance_report.json')
```

### 1.2 æ‰¹é‡æ€§èƒ½å¯¹æ¯”è„šæœ¬

```python
#!/usr/bin/env python3
"""
æ‰¹é‡æ€§èƒ½å¯¹æ¯”å·¥å…·

å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List

class BatchComparisonTool:
    """æ‰¹é‡å¯¹æ¯”å·¥å…·"""

    def __init__(self):
        self.comparison_data = []

    def add_model_results(self, model_name: str, metrics: Dict):
        """æ·»åŠ æ¨¡å‹ç»“æœ"""
        metrics['model'] = model_name
        self.comparison_data.append(metrics)

    def generate_comparison_table(self) -> pd.DataFrame:
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        df = pd.DataFrame(self.comparison_data)
        return df

    def generate_comparison_chart(self, metric: str, output_path: str):
        """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
        df = self.generate_comparison_table()

        plt.figure(figsize=(10, 6))
        sns.barplot(data=df, x='model', y=metric)
        plt.title(f'{metric} Comparison')
        plt.xlabel('Model')
        plt.ylabel(metric)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()

    def generate_radar_chart(self, metrics: List[str], output_path: str):
        """ç”Ÿæˆé›·è¾¾å›¾"""
        import math

        df = self.generate_comparison_table()
        num_metrics = len(metrics)
        angles = [n / float(num_metrics) * 2 * math.pi for n in range(num_metrics)]
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

        for idx, row in df.iterrows():
            values = [row[metric] for metric in metrics]
            values += values[:1]
            ax.plot(angles, values, 'o-', linewidth=2, label=row['model'])
            ax.fill(angles, values, alpha=0.25)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        plt.title('Model Performance Radar Chart')
        plt.savefig(output_path)
        plt.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    tool = BatchComparisonTool()

    # æ·»åŠ æ¨¡å‹ç»“æœ
    tool.add_model_results('PGT', {
        'accuracy': 0.92,
        'f1_score': 0.91,
        'training_time': 80,
        'inference_time': 0.05
    })

    tool.add_model_results('Emma', {
        'accuracy': 0.88,
        'f1_score': 0.87,
        'training_time': 60,
        'inference_time': 0.03
    })

    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    comparison_df = tool.generate_comparison_table()
    print(comparison_df)

    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    tool.generate_comparison_chart('accuracy', 'accuracy_comparison.png')

    # ç”Ÿæˆé›·è¾¾å›¾
    tool.generate_radar_chart(['accuracy', 'f1_score', 'training_time', 'inference_time'],
                              'radar_chart.png')
```

---

## ğŸš€ **äºŒã€æ¨¡å‹è®­ç»ƒè‡ªåŠ¨åŒ–å·¥å…· / Automated Training Tools**

### 2.1 è‡ªåŠ¨åŒ–è®­ç»ƒè„šæœ¬

```python
#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–è®­ç»ƒå·¥å…·

è‡ªåŠ¨æ‰§è¡Œæ¨¡å‹è®­ç»ƒï¼ŒåŒ…æ‹¬è¶…å‚æ•°æœç´¢ã€æ¨¡å‹é€‰æ‹©ã€è®­ç»ƒç›‘æ§ç­‰
"""

import torch
import torch.nn as nn
import optuna
from typing import Dict, Any
import logging
from pathlib import Path

class AutomatedTrainer:
    """è‡ªåŠ¨åŒ–è®­ç»ƒå™¨"""

    def __init__(self,
                 model_class: type,
                 train_dataloader,
                 val_dataloader,
                 device='cuda'):
        self.model_class = model_class
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = device
        self.best_model = None
        self.best_score = 0.0

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def hyperparameter_search(self,
                             n_trials: int = 100,
                             study_name: str = 'hyperparameter_search'):
        """
        è¶…å‚æ•°æœç´¢

        å‚æ•°:
            n_trials: è¯•éªŒæ¬¡æ•°
            study_name: ç ”ç©¶åç§°
        """
        study = optuna.create_study(
            direction='maximize',
            study_name=study_name,
            storage=f'sqlite:///{study_name}.db',
            load_if_exists=True
        )

        study.optimize(
            self._objective,
            n_trials=n_trials,
            show_progress_bar=True
        )

        self.logger.info(f"æœ€ä½³è¶…å‚æ•°: {study.best_params}")
        self.logger.info(f"æœ€ä½³åˆ†æ•°: {study.best_value}")

        return study.best_params

    def _objective(self, trial):
        """ç›®æ ‡å‡½æ•°"""
        # è¶…å‚æ•°å»ºè®®
        hidden_dim = trial.suggest_int('hidden_dim', 256, 1024, step=128)
        num_layers = trial.suggest_int('num_layers', 4, 12)
        num_heads = trial.suggest_int('num_heads', 4, 16)
        dropout = trial.suggest_float('dropout', 0.1, 0.5)
        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
        batch_size = trial.suggest_int('batch_size', 16, 64, step=8)

        # åˆ›å»ºæ¨¡å‹
        model = self.model_class(
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            num_heads=num_heads,
            dropout=dropout
        ).to(self.device)

        # è®­ç»ƒ
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒå‡ ä¸ªepoch
        for epoch in range(5):
            model.train()
            for batch in self.train_dataloader:
                batch = batch.to(self.device)
                optimizer.zero_grad()
                output = model(batch)
                loss = criterion(output, batch.y)
                loss.backward()
                optimizer.step()

        # éªŒè¯
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in self.val_dataloader:
                batch = batch.to(self.device)
                output = model(batch)
                pred = output.argmax(dim=-1)
                correct += (pred == batch.y).sum().item()
                total += batch.y.size(0)

        accuracy = correct / total

        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if accuracy > self.best_score:
            self.best_score = accuracy
            self.best_model = model.state_dict()

        return accuracy

    def train_with_best_params(self, best_params: Dict[str, Any], num_epochs: int = 100):
        """ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒ"""
        model = self.model_class(**best_params).to(self.device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'])
        criterion = nn.CrossEntropyLoss()

        best_val_acc = 0.0

        for epoch in range(num_epochs):
            # è®­ç»ƒ
            model.train()
            train_loss = 0.0
            for batch in self.train_dataloader:
                batch = batch.to(self.device)
                optimizer.zero_grad()
                output = model(batch)
                loss = criterion(output, batch.y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()

            # éªŒè¯
            model.eval()
            val_correct = 0
            val_total = 0
            with torch.no_grad():
                for batch in self.val_dataloader:
                    batch = batch.to(self.device)
                    output = model(batch)
                    pred = output.argmax(dim=-1)
                    val_correct += (pred == batch.y).sum().item()
                    val_total += batch.y.size(0)

            val_acc = val_correct / val_total

            self.logger.info(
                f"Epoch {epoch+1}/{num_epochs}, "
                f"Train Loss: {train_loss/len(self.train_dataloader):.4f}, "
                f"Val Acc: {val_acc:.4f}"
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'best_model.pth')

        return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    trainer = AutomatedTrainer(
        model_class=PGTModel,
        train_dataloader=train_loader,
        val_dataloader=val_loader
    )

    # è¶…å‚æ•°æœç´¢
    best_params = trainer.hyperparameter_search(n_trials=100)

    # ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒ
    final_model = trainer.train_with_best_params(best_params, num_epochs=100)
```

### 2.2 åˆ†å¸ƒå¼è®­ç»ƒè‡ªåŠ¨åŒ–è„šæœ¬

```python
#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒè‡ªåŠ¨åŒ–å·¥å…·

è‡ªåŠ¨é…ç½®å’Œç®¡ç†åˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡
"""

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
import os

class DistributedTrainingManager:
    """åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self,
                 backend: str = 'nccl',
                 init_method: str = 'env://'):
        self.backend = backend
        self.init_method = init_method

    def setup_distributed(self, rank: int, world_size: int):
        """è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ"""
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'

        dist.init_process_group(
            backend=self.backend,
            init_method=self.init_method,
            rank=rank,
            world_size=world_size
        )

        torch.cuda.set_device(rank)

    def cleanup_distributed(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
        dist.destroy_process_group()

    def train_distributed(self,
                         model: nn.Module,
                         dataloader: torch.utils.data.DataLoader,
                         num_epochs: int = 100):
        """åˆ†å¸ƒå¼è®­ç»ƒ"""
        # åŒ…è£…æ¨¡å‹
        model = DDP(model, device_ids=[torch.cuda.current_device()])

        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        criterion = nn.CrossEntropyLoss()

        for epoch in range(num_epochs):
            model.train()
            for batch in dataloader:
                batch = batch.to(torch.cuda.current_device())
                optimizer.zero_grad()
                output = model(batch)
                loss = criterion(output, batch.y)
                loss.backward()
                optimizer.step()

            # åŒæ­¥
            dist.barrier()

            if dist.get_rank() == 0:
                print(f"Epoch {epoch+1}/{num_epochs} completed")

def run_distributed_training(rank: int, world_size: int, model, dataloader):
    """è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ"""
    manager = DistributedTrainingManager()
    manager.setup_distributed(rank, world_size)

    manager.train_distributed(model, dataloader)

    manager.cleanup_distributed()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    world_size = 4  # 4ä¸ªGPU

    mp.spawn(
        run_distributed_training,
        args=(world_size, model, dataloader),
        nprocs=world_size,
        join=True
    )
```

---

## ğŸ¯ **ä¸‰ã€éƒ¨ç½²è‡ªåŠ¨åŒ–å·¥å…· / Deployment Automation Tools**

### 3.1 æ¨¡å‹æœåŠ¡åŒ–è„šæœ¬

```python
#!/usr/bin/env python3
"""
æ¨¡å‹æœåŠ¡åŒ–å·¥å…·

è‡ªåŠ¨å°†æ¨¡å‹è½¬æ¢ä¸ºæœåŠ¡ï¼Œæ”¯æŒREST APIå’ŒgRPC
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import uvicorn
from typing import List, Dict

app = FastAPI(title="Graph Model Service")

class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚"""
    data: List[Dict]
    batch_size: int = 32

class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”"""
    predictions: List[float]
    confidence: List[float]

class ModelService:
    """æ¨¡å‹æœåŠ¡"""

    def __init__(self, model_path: str, device: str = 'cuda'):
        self.device = device
        self.model = self._load_model(model_path)
        self.model.eval()

    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        model = torch.load(model_path)
        model.to(self.device)
        return model

    def predict(self, data: List[Dict]) -> PredictionResponse:
        """é¢„æµ‹"""
        # é¢„å¤„ç†æ•°æ®
        processed_data = self._preprocess(data)

        # æ‰¹å¤„ç†é¢„æµ‹
        predictions = []
        confidences = []

        with torch.no_grad():
            for batch in self._batch_data(processed_data, batch_size=32):
                batch = batch.to(self.device)
                output = self.model(batch)
                probs = torch.softmax(output, dim=-1)
                pred = output.argmax(dim=-1)

                predictions.extend(pred.cpu().numpy().tolist())
                confidences.extend(probs.max(dim=-1)[0].cpu().numpy().tolist())

        return PredictionResponse(
            predictions=predictions,
            confidence=confidences
        )

    def _preprocess(self, data: List[Dict]):
        """é¢„å¤„ç†æ•°æ®"""
        # å®ç°æ•°æ®é¢„å¤„ç†é€»è¾‘
        pass

    def _batch_data(self, data, batch_size: int):
        """æ‰¹å¤„ç†æ•°æ®"""
        for i in range(0, len(data), batch_size):
            yield data[i:i+batch_size]

# å…¨å±€æ¨¡å‹æœåŠ¡å®ä¾‹
model_service = None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global model_service
    model_service = ModelService(model_path='best_model.pth')

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """é¢„æµ‹æ¥å£"""
    try:
        result = model_service.predict(request.data)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model_loaded": model_service is not None}

if __name__ == '__main__':
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3.2 Dockeréƒ¨ç½²è„šæœ¬

```dockerfile
# Dockerfileç¤ºä¾‹
FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

WORKDIR /app

# å®‰è£…Pythonå’Œä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ¨¡å‹å’Œä»£ç 
COPY model.pth /models/
COPY app.py .
COPY model_service.py .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨æœåŠ¡
CMD ["python", "app.py"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  model-service:
    build: .
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - MODEL_PATH=/models/model.pth
      - BATCH_SIZE=32
    volumes:
      - ./models:/models
    restart: unless-stopped
```

---

## ğŸ“Š **å››ã€ç›‘æ§å’Œæ—¥å¿—å·¥å…· / Monitoring and Logging Tools**

### 4.1 æ€§èƒ½ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
"""
æ€§èƒ½ç›‘æ§å·¥å…·

å®æ—¶ç›‘æ§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æ€§èƒ½
"""

import time
import psutil
import GPUtil
import threading
from collections import deque
from typing import Dict, List
import matplotlib.pyplot as plt

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, interval: float = 1.0):
        self.interval = interval
        self.monitoring = False
        self.metrics = {
            'cpu_usage': deque(maxlen=1000),
            'memory_usage': deque(maxlen=1000),
            'gpu_usage': deque(maxlen=1000),
            'gpu_memory': deque(maxlen=1000),
            'timestamp': deque(maxlen=1000)
        }

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            timestamp = time.time()

            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=None)
            self.metrics['cpu_usage'].append(cpu_percent)

            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.metrics['memory_usage'].append(memory.percent)

            # GPUä½¿ç”¨ç‡
            if torch.cuda.is_available():
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu_usage = gpus[0].load * 100
                    gpu_memory = gpus[0].memoryUsed / gpus[0].memoryTotal * 100
                    self.metrics['gpu_usage'].append(gpu_usage)
                    self.metrics['gpu_memory'].append(gpu_memory)

            self.metrics['timestamp'].append(timestamp)

            time.sleep(self.interval)

    def get_current_metrics(self) -> Dict:
        """è·å–å½“å‰æŒ‡æ ‡"""
        return {
            'cpu_usage': self.metrics['cpu_usage'][-1] if self.metrics['cpu_usage'] else 0,
            'memory_usage': self.metrics['memory_usage'][-1] if self.metrics['memory_usage'] else 0,
            'gpu_usage': self.metrics['gpu_usage'][-1] if self.metrics['gpu_usage'] else 0,
            'gpu_memory': self.metrics['gpu_memory'][-1] if self.metrics['gpu_memory'] else 0
        }

    def plot_metrics(self, output_path: str):
        """ç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        timestamps = list(self.metrics['timestamp'])

        # CPUä½¿ç”¨ç‡
        axes[0, 0].plot(timestamps, list(self.metrics['cpu_usage']))
        axes[0, 0].set_title('CPU Usage')
        axes[0, 0].set_ylabel('Percentage')

        # å†…å­˜ä½¿ç”¨ç‡
        axes[0, 1].plot(timestamps, list(self.metrics['memory_usage']))
        axes[0, 1].set_title('Memory Usage')
        axes[0, 1].set_ylabel('Percentage')

        # GPUä½¿ç”¨ç‡
        if self.metrics['gpu_usage']:
            axes[1, 0].plot(timestamps, list(self.metrics['gpu_usage']))
            axes[1, 0].set_title('GPU Usage')
            axes[1, 0].set_ylabel('Percentage')

        # GPUå†…å­˜
        if self.metrics['gpu_memory']:
            axes[1, 1].plot(timestamps, list(self.metrics['gpu_memory']))
            axes[1, 1].set_title('GPU Memory')
            axes[1, 1].set_ylabel('Percentage')

        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    monitor = PerformanceMonitor(interval=1.0)
    monitor.start_monitoring()

    # è¿è¡Œè®­ç»ƒ
    # ... è®­ç»ƒä»£ç  ...

    monitor.stop_monitoring()
    monitor.plot_metrics('performance_metrics.png')
```

---

## ğŸ“ **äº”ã€æ€»ç»“ / Summary**

### 5.1 å·¥å…·æ¸…å•

1. âœ… **æ€§èƒ½è¯„ä¼°å·¥å…·**: ç»¼åˆæ€§èƒ½è¯„ä¼°ã€æ‰¹é‡å¯¹æ¯”
2. âœ… **è®­ç»ƒè‡ªåŠ¨åŒ–å·¥å…·**: è¶…å‚æ•°æœç´¢ã€åˆ†å¸ƒå¼è®­ç»ƒ
3. âœ… **éƒ¨ç½²è‡ªåŠ¨åŒ–å·¥å…·**: æ¨¡å‹æœåŠ¡åŒ–ã€Dockeréƒ¨ç½²
4. âœ… **ç›‘æ§å·¥å…·**: æ€§èƒ½ç›‘æ§ã€æ—¥å¿—è®°å½•

### 5.2 ä½¿ç”¨å»ºè®®

1. **æ ¹æ®éœ€æ±‚é€‰æ‹©å·¥å…·**: æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·
2. **ç»„åˆä½¿ç”¨**: å¤šä¸ªå·¥å…·å¯ä»¥ç»„åˆä½¿ç”¨
3. **è‡ªå®šä¹‰æ‰©å±•**: æ ¹æ®å®é™…éœ€æ±‚æ‰©å±•å·¥å…·åŠŸèƒ½
4. **æ–‡æ¡£å®Œå–„**: åŠæ—¶æ›´æ–°å·¥å…·æ–‡æ¡£

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ - æ•´åˆNSDI 2025æœ€æ–°ç³»ç»Ÿå·¥å…·è„šæœ¬
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… å®Œæˆ
