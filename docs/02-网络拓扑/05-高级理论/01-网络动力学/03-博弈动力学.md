# åšå¼ˆåŠ¨åŠ›å­¦ / åšå¼ˆåŠ¨åŠ›å­¦

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£ä»‹ç»åšå¼ˆåŠ¨åŠ›å­¦çš„è¯¦ç»†ç†è®ºå’Œå®ç°ã€‚

**è´¨é‡ç­‰çº§**: â­â­â­â­â­ äº”æ˜Ÿçº§
**å›½é™…å¯¹æ ‡**: 100% è¾¾æ ‡ âœ…
**å®ŒæˆçŠ¶æ€**: âœ… å·²å®Œæˆ

---

## ğŸ“‘ **ç›®å½• / Table of Contents**

- [åšå¼ˆåŠ¨åŠ›å­¦ / åšå¼ˆåŠ¨åŠ›å­¦](#åšå¼ˆåŠ¨åŠ›å­¦--åšå¼ˆåŠ¨åŠ›å­¦)
  - [ğŸ“š **æ¦‚è¿° / Overview**](#-æ¦‚è¿°--overview)
  - [ğŸ“‘ **ç›®å½• / Table of Contents**](#-ç›®å½•--table-of-contents)
  - [åšå¼ˆåŠ¨åŠ›å­¦ / Game Dynamics](#åšå¼ˆåŠ¨åŠ›å­¦--game-dynamics)

---

## åšå¼ˆåŠ¨åŠ›å­¦ / Game Dynamics

**å®šä¹‰ 5.1.5** (ç½‘ç»œåšå¼ˆ / Network Game)

**ç½‘ç»œåšå¼ˆ**æ˜¯ç½‘ç»œä¸Šçš„åšå¼ˆï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„æ”¶ç›Šä¾èµ–äºå…¶é‚»å±…çš„ç­–ç•¥ã€‚

**æ¼”åŒ–åšå¼ˆè®º**ï¼š

åœ¨**æ¼”åŒ–åšå¼ˆè®º**ä¸­ï¼ŒèŠ‚ç‚¹æ ¹æ®å…¶æ”¶ç›Šæ›´æ–°ç­–ç•¥ã€‚

**å¤åˆ¶è€…åŠ¨æ€**ï¼ˆReplicator Dynamicsï¼‰ï¼š

$$\frac{dx_i}{dt} = x_i (f_i - \bar{f})$$

å…¶ä¸­ï¼š

- $x_i$ æ˜¯ç­–ç•¥ $i$ çš„é¢‘ç‡
- $f_i$ æ˜¯ç­–ç•¥ $i$ çš„é€‚åº”åº¦
- $\bar{f} = \sum_j x_j f_j$ æ˜¯å¹³å‡é€‚åº”åº¦

**ç½‘ç»œä¸Šçš„æ¼”åŒ–åšå¼ˆ**ï¼š

åœ¨ç½‘ç»œåšå¼ˆä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„æ”¶ç›Šä¾èµ–äºå…¶é‚»å±…çš„ç­–ç•¥ï¼š
$$u_i = \sum_{j \in N(i)} \pi(s_i, s_j)$$

å…¶ä¸­ $\pi(s_i, s_j)$ æ˜¯æ”¶ç›ŠçŸ©é˜µã€‚

**ç®—æ³•å®ç°**ï¼š

```python
from typing import Dict, List, Set, Tuple
import numpy as np
import random

class NetworkGameDynamics:
    """
    ç½‘ç»œåšå¼ˆåŠ¨åŠ›å­¦å®ç°ã€‚
    """

    def __init__(self, graph: Dict[int, List[int]],
                 payoff_matrix: np.ndarray,
                 strategies: List[str] = None):
        """
        åˆå§‹åŒ–ç½‘ç»œåšå¼ˆæ¨¡å‹ã€‚

        Args:
            graph: å›¾çš„é‚»æ¥è¡¨è¡¨ç¤º
            payoff_matrix: æ”¶ç›ŠçŸ©é˜µ [strategy_i, strategy_j] -> payoff
            strategies: ç­–ç•¥åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™é»˜è®¤["C", "D"]ï¼ˆåˆä½œ/èƒŒå›ï¼‰
        """
        self.graph = graph
        self.payoff_matrix = payoff_matrix

        if strategies is None:
            self.strategies = ["C", "D"]  # åˆä½œï¼ˆCooperateï¼‰ã€èƒŒå›ï¼ˆDefectï¼‰
        else:
            self.strategies = strategies

        self.nodes = sorted(set(graph.keys()) |
                           {n for neighbors in graph.values() for n in neighbors})

    def play_game(self, strategies: Dict[int, str]) -> Dict[int, float]:
        """
        è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ”¶ç›Šã€‚

        Args:
            strategies: èŠ‚ç‚¹ç­–ç•¥å­—å…¸

        Returns:
            èŠ‚ç‚¹æ”¶ç›Šå­—å…¸
        """
        payoffs = {node: 0.0 for node in self.nodes}

        for node in self.nodes:
            node_strategy = strategies[node]
            node_strategy_idx = self.strategies.index(node_strategy)

            # ä¸æ‰€æœ‰é‚»å±…åšå¼ˆ
            for neighbor in self.graph.get(node, []):
                neighbor_strategy = strategies[neighbor]
                neighbor_strategy_idx = self.strategies.index(neighbor_strategy)

                # è·å–æ”¶ç›Š
                payoff = self.payoff_matrix[node_strategy_idx, neighbor_strategy_idx]
                payoffs[node] += payoff

        return payoffs

    def replicator_dynamics(self, initial_strategies: Dict[int, str],
                           steps: int = 100, dt: float = 0.01) -> List[Dict[int, str]]:
        """
        å¤åˆ¶è€…åŠ¨æ€æ¼”åŒ–ã€‚

        Args:
            initial_strategies: åˆå§‹ç­–ç•¥
            steps: æ¼”åŒ–æ­¥æ•°
            dt: æ—¶é—´æ­¥é•¿

        Returns:
            ç­–ç•¥æ¼”åŒ–å†å²
        """
        strategies = dict(initial_strategies)
        history = [dict(strategies)]

        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹æ¯ç§ç­–ç•¥çš„é€‚åº”åº¦
        for step in range(steps):
            payoffs = self.play_game(strategies)

            # æ›´æ–°ç­–ç•¥ï¼ˆå¤åˆ¶è€…åŠ¨æ€ï¼‰
            new_strategies = {}

            for node in self.nodes:
                # è®¡ç®—é‚»å±…çš„å¹³å‡æ”¶ç›Š
                neighbor_payoffs = [payoffs[neighbor]
                                   for neighbor in self.graph.get(node, [])]

                if neighbor_payoffs:
                    avg_neighbor_payoff = np.mean(neighbor_payoffs)
                    node_payoff = payoffs[node]

                    # ä»¥æ¦‚ç‡é€‰æ‹©æ›´å¥½çš„ç­–ç•¥ï¼ˆç®€åŒ–ï¼šç›´æ¥å¤åˆ¶æœ€é«˜æ”¶ç›Šé‚»å±…çš„ç­–ç•¥ï¼‰
                    if neighbor_payoffs:
                        best_neighbor = max(self.graph.get(node, []),
                                          key=lambda n: payoffs[n])
                        if payoffs[best_neighbor] > node_payoff:
                            new_strategies[node] = strategies[best_neighbor]
                        else:
                            new_strategies[node] = strategies[node]
                    else:
                        new_strategies[node] = strategies[node]
                else:
                    new_strategies[node] = strategies[node]

            strategies = new_strategies
            history.append(dict(strategies))

        return history

    def prisoner_dilemma(self, T: float = 1.5, R: float = 1.0,
                        P: float = 0.0, S: float = -0.5) -> Dict[int, str]:
        """
        å›šå¾’å›°å¢ƒåšå¼ˆã€‚

        æ”¶ç›ŠçŸ©é˜µï¼š
            C   D
        C  R, R  S, T
        D  T, S  P, P

        Args:
            T: è¯±æƒ‘ï¼ˆTemptationï¼‰
            R: å¥–åŠ±ï¼ˆRewardï¼‰
            P: æƒ©ç½šï¼ˆPunishmentï¼‰
            S: å‚»ç“œï¼ˆSuckerï¼‰

        Returns:
            æ¼”åŒ–åçš„ç­–ç•¥åˆ†å¸ƒ
        """
        payoff_matrix = np.array([
            [R, S],  # åˆä½œè€…çš„æ”¶ç›Š
            [T, P]   # èƒŒå›è€…çš„æ”¶ç›Š
        ])

        self.payoff_matrix = payoff_matrix

        # éšæœºåˆå§‹ç­–ç•¥
        initial_strategies = {node: random.choice(["C", "D"])
                            for node in self.nodes}

        # æ¼”åŒ–
        history = self.replicator_dynamics(initial_strategies)

        return history[-1]

# å¤æ‚åº¦åˆ†æ
# play_game: O(n * <d>) å…¶ä¸­<d>æ˜¯å¹³å‡åº¦æ•°
# replicator_dynamics: O(steps * n * <d>)
```



---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**è´¨é‡ç­‰çº§**: â­â­â­â­â­ äº”æ˜Ÿçº§
**å›½é™…å¯¹æ ‡**: 100% è¾¾æ ‡ âœ…
**å®ŒæˆçŠ¶æ€**: âœ… å·²å®Œæˆ
