# åº”ç”¨å®è·µæ·±åŒ– - ä»£ç ç¤ºä¾‹é›†åˆ / Application Practice Deepening - Code Examples Collection

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›PGTã€Emmaã€GraphGPTã€GPSã€Mamba2äº”ä¸ªä¸“é¢˜çš„å®Œæ•´ä»£ç ç¤ºä¾‹é›†åˆï¼ŒåŒ…æ‹¬æ¨¡å‹å®ç°ã€è®­ç»ƒè„šæœ¬ã€æ¨ç†è„šæœ¬å’Œå·¥å…·å‡½æ•°ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## ğŸš€ **ä¸€ã€PGTå®Œæ•´ä»£ç ç¤ºä¾‹ / PGT Complete Code Examples**

### 1.1 PGTæ¨¡å‹å®Œæ•´å®ç°

```python
"""
PGT (Pre-trained Graph Transformer) å®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree
import math

class LinearAttention(nn.Module):
    """çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):
        super(LinearAttention, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads

        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)

        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)

    def forward(self, x, mask=None):
        """
        çº¿æ€§æ³¨æ„åŠ›å‰å‘ä¼ æ’­

        å¤æ‚åº¦: O(n) è€Œä¸æ˜¯ O(nÂ²)
        """
        batch_size, seq_len, hidden_dim = x.shape

        # æŠ•å½±
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # çº¿æ€§æ³¨æ„åŠ›è®¡ç®—
        # ä½¿ç”¨ç‰¹å¾æ˜ å°„å®ç°çº¿æ€§å¤æ‚åº¦
        q = F.elu(q) + 1  # ç‰¹å¾æ˜ å°„
        k = F.elu(k) + 1

        # è®¡ç®—æ³¨æ„åŠ›
        kv = torch.einsum('bshd,bshv->bhdv', k, v)  # [B, H, D, V]
        z = torch.einsum('bshd,bhdv->bshv', q, kv)  # [B, S, H, V]

        # å½’ä¸€åŒ–
        normalizer = torch.einsum('bshd->bsh', q).unsqueeze(-1) + 1e-8
        z = z / normalizer

        # è¾“å‡ºæŠ•å½±
        z = z.contiguous().view(batch_size, seq_len, hidden_dim)
        output = self.out_proj(z)
        output = self.dropout(output)

        return output

class PGTTransformerLayer(nn.Module):
    """PGT Transformerå±‚"""

    def __init__(self, hidden_dim, num_heads=8, ffn_dim=3072, dropout=0.1):
        super(PGTTransformerLayer, self).__init__()

        # çº¿æ€§æ³¨æ„åŠ›
        self.attention = LinearAttention(hidden_dim, num_heads, dropout)
        self.attention_norm = nn.LayerNorm(hidden_dim)

        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, ffn_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ffn_dim, hidden_dim),
            nn.Dropout(dropout)
        )
        self.ffn_norm = nn.LayerNorm(hidden_dim)

    def forward(self, x, mask=None):
        """å‰å‘ä¼ æ’­"""
        # æ³¨æ„åŠ›
        residual = x
        x = self.attention_norm(x)
        x = self.attention(x, mask)
        x = residual + x

        # å‰é¦ˆç½‘ç»œ
        residual = x
        x = self.ffn_norm(x)
        x = self.ffn(x)
        x = residual + x

        return x

class PGTEncoder(nn.Module):
    """PGTç¼–ç å™¨"""

    def __init__(self,
                 input_dim=768,
                 hidden_dim=768,
                 num_layers=12,
                 num_heads=12,
                 ffn_dim=3072,
                 dropout=0.1,
                 max_nodes=10000):
        super(PGTEncoder, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.max_nodes = max_nodes

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(
            torch.randn(1, max_nodes, hidden_dim)
        )

        # Transformerå±‚
        self.layers = nn.ModuleList([
            PGTTransformerLayer(hidden_dim, num_heads, ffn_dim, dropout)
            for _ in range(num_layers)
        ])

        # è¾“å‡ºå½’ä¸€åŒ–
        self.output_norm = nn.LayerNorm(hidden_dim)

    def forward(self, node_features, node_mask=None):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            node_features: èŠ‚ç‚¹ç‰¹å¾ [batch_size, num_nodes, input_dim]
            node_mask: èŠ‚ç‚¹æ©ç  [batch_size, num_nodes]

        è¿”å›:
            embeddings: èŠ‚ç‚¹åµŒå…¥ [batch_size, num_nodes, hidden_dim]
        """
        batch_size, num_nodes, _ = node_features.shape

        # è¾“å…¥æŠ•å½±
        x = self.input_proj(node_features)

        # ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:, :num_nodes, :]

        # Transformerå±‚
        for layer in self.layers:
            x = layer(x, mask=node_mask)

        # è¾“å‡ºå½’ä¸€åŒ–
        x = self.output_norm(x)

        return x

class PGTPreTrainingTasks(nn.Module):
    """PGTé¢„è®­ç»ƒä»»åŠ¡"""

    def __init__(self, hidden_dim, vocab_size=10000):
        super(PGTPreTrainingTasks, self).__init__()

        # èŠ‚ç‚¹æ©ç é‡å»º
        self.node_reconstruction = nn.Linear(hidden_dim, vocab_size)

        # è¾¹é¢„æµ‹
        self.edge_prediction = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

        # å­å›¾å¯¹æ¯”å­¦ä¹ 
        self.subgraph_projection = nn.Linear(hidden_dim, hidden_dim)

    def compute_node_reconstruction_loss(self, embeddings, masked_nodes, target_nodes):
        """è®¡ç®—èŠ‚ç‚¹é‡å»ºæŸå¤±"""
        masked_embeddings = embeddings[masked_nodes]
        logits = self.node_reconstruction(masked_embeddings)
        loss = F.cross_entropy(logits, target_nodes)
        return loss

    def compute_edge_prediction_loss(self, embeddings, edge_index, edge_labels):
        """è®¡ç®—è¾¹é¢„æµ‹æŸå¤±"""
        src_emb = embeddings[edge_index[0]]
        dst_emb = embeddings[edge_index[1]]
        pair_emb = torch.cat([src_emb, dst_emb], dim=-1)
        logits = self.edge_prediction(pair_emb).squeeze()
        loss = F.binary_cross_entropy(logits, edge_labels.float())
        return loss

    def compute_subgraph_contrastive_loss(self, subgraph_embeddings, temperature=0.07):
        """è®¡ç®—å­å›¾å¯¹æ¯”æŸå¤±"""
        # æŠ•å½±
        projected = self.subgraph_projection(subgraph_embeddings)
        projected = F.normalize(projected, p=2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.matmul(projected, projected.T) / temperature

        # å¯¹æ¯”æŸå¤±
        labels = torch.arange(len(subgraph_embeddings)).to(similarity.device)
        loss = F.cross_entropy(similarity, labels)

        return loss

class PGTModel(nn.Module):
    """å®Œæ•´PGTæ¨¡å‹"""

    def __init__(self,
                 input_dim=768,
                 hidden_dim=768,
                 num_layers=12,
                 num_heads=12,
                 ffn_dim=3072,
                 dropout=0.1,
                 vocab_size=10000):
        super(PGTModel, self).__init__()

        self.encoder = PGTEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            num_heads=num_heads,
            ffn_dim=ffn_dim,
            dropout=dropout
        )

        self.pretraining_tasks = PGTPreTrainingTasks(hidden_dim, vocab_size)

    def forward(self, node_features, node_mask=None,
                masked_nodes=None, target_nodes=None,
                edge_index=None, edge_labels=None,
                subgraph_embeddings=None):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç 
        embeddings = self.encoder(node_features, node_mask)

        # é¢„è®­ç»ƒä»»åŠ¡
        losses = {}

        if masked_nodes is not None and target_nodes is not None:
            losses['node'] = self.pretraining_tasks.compute_node_reconstruction_loss(
                embeddings, masked_nodes, target_nodes
            )

        if edge_index is not None and edge_labels is not None:
            losses['edge'] = self.pretraining_tasks.compute_edge_prediction_loss(
                embeddings, edge_index, edge_labels
            )

        if subgraph_embeddings is not None:
            losses['subgraph'] = self.pretraining_tasks.compute_subgraph_contrastive_loss(
                subgraph_embeddings
            )

        # æ€»æŸå¤±
        total_loss = sum(losses.values())
        losses['total'] = total_loss

        return embeddings, losses

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºæ¨¡å‹
    model = PGTModel(
        input_dim=768,
        hidden_dim=768,
        num_layers=12,
        num_heads=12
    )

    # å‡†å¤‡æ•°æ®
    batch_size = 32
    num_nodes = 1000
    node_features = torch.randn(batch_size, num_nodes, 768)

    # å‰å‘ä¼ æ’­
    embeddings, losses = model(node_features)

    print(f"Embeddings shape: {embeddings.shape}")
    print(f"Losses: {losses}")
```

### 1.2 PGTè®­ç»ƒè„šæœ¬

```python
"""
PGTè®­ç»ƒè„šæœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import wandb

class PGTTrainer:
    """PGTè®­ç»ƒå™¨"""

    def __init__(self,
                 model,
                 train_dataloader,
                 val_dataloader=None,
                 device='cuda',
                 learning_rate=1e-4,
                 weight_decay=0.01,
                 warmup_steps=10000):
        self.model = model.to(device)
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.LambdaLR(
            self.optimizer,
            lr_lambda=lambda step: min(step / warmup_steps, 1.0)
        )

        # æŸå¤±æƒé‡
        self.loss_weights = {
            'node': 1.0,
            'edge': 0.5,
            'subgraph': 0.3
        }

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0

        pbar = tqdm(self.train_dataloader, desc=f"Epoch {epoch}")
        for batch_idx, batch in enumerate(pbar):
            batch = batch.to(self.device)

            # å‰å‘ä¼ æ’­
            _, losses = self.model(
                node_features=batch.node_features,
                masked_nodes=batch.masked_nodes,
                target_nodes=batch.target_nodes,
                edge_index=batch.edge_index,
                edge_labels=batch.edge_labels
            )

            # åŠ æƒæŸå¤±
            weighted_loss = sum(
                self.loss_weights[key] * losses[key]
                for key in losses if key != 'total'
            )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            weighted_loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()
            self.scheduler.step()

            total_loss += weighted_loss.item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': weighted_loss.item(),
                'lr': self.scheduler.get_last_lr()[0]
            })

            # è®°å½•åˆ°wandb
            if batch_idx % 100 == 0:
                wandb.log({
                    'train_loss': weighted_loss.item(),
                    'learning_rate': self.scheduler.get_last_lr()[0]
                })

        avg_loss = total_loss / len(self.train_dataloader)
        return avg_loss

    def validate(self):
        """éªŒè¯"""
        if self.val_dataloader is None:
            return None

        self.model.eval()
        total_loss = 0.0

        with torch.no_grad():
            for batch in self.val_dataloader:
                batch = batch.to(self.device)
                _, losses = self.model(
                    node_features=batch.node_features,
                    masked_nodes=batch.masked_nodes,
                    target_nodes=batch.target_nodes,
                    edge_index=batch.edge_index,
                    edge_labels=batch.edge_labels
                )

                weighted_loss = sum(
                    self.loss_weights[key] * losses[key]
                    for key in losses if key != 'total'
                )
                total_loss += weighted_loss.item()

        avg_loss = total_loss / len(self.val_dataloader)
        return avg_loss

    def train(self, num_epochs=100, save_path='pgt_model.pth'):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')

        for epoch in range(1, num_epochs + 1):
            # è®­ç»ƒ
            train_loss = self.train_epoch(epoch)

            # éªŒè¯
            val_loss = self.validate()

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss is not None and val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), save_path)
                print(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {val_loss:.4f}")

            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch}/{num_epochs}, "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f if val_loss else 'N/A'}")
```

---

## ğŸš€ **äºŒã€Emmaå®Œæ•´ä»£ç ç¤ºä¾‹ / Emma Complete Code Examples**

### 2.1 Emmaæ ¸å¿ƒç»„ä»¶å®ç°

```python
"""
Emmaæ¡†æ¶æ ¸å¿ƒç»„ä»¶å®ç°
"""

import torch
import torch.nn as nn
import torch.distributed as dist
from torch_geometric.nn import MessagePassing
from typing import List, Dict, Tuple

class SourceNodeBlocking:
    """æºèŠ‚ç‚¹åˆ†å—"""

    def __init__(self, block_size=10000, strategy='degree_based'):
        self.block_size = block_size
        self.strategy = strategy

    def partition(self, graph, num_workers):
        """åˆ†å—æºèŠ‚ç‚¹"""
        num_nodes = graph.num_nodes
        degrees = graph.degree

        if self.strategy == 'degree_based':
            # æŒ‰åº¦æ’åº
            sorted_indices = torch.argsort(degrees, descending=True)
        elif self.strategy == 'random':
            # éšæœºæ’åº
            sorted_indices = torch.randperm(num_nodes)
        else:
            sorted_indices = torch.arange(num_nodes)

        # åˆ†å—
        blocks = []
        for i in range(0, num_nodes, self.block_size):
            block = sorted_indices[i:i+self.block_size]
            blocks.append(block)

        # åˆ†é…ç»™å·¥ä½œèŠ‚ç‚¹
        worker_blocks = [[] for _ in range(num_workers)]
        for idx, block in enumerate(blocks):
            worker_id = idx % num_workers
            worker_blocks[worker_id].append(block)

        return worker_blocks

class MobileAggregation:
    """ç§»åŠ¨èšåˆ"""

    def __init__(self, num_aggregation_points=1000):
        self.num_aggregation_points = num_aggregation_points
        self.aggregation_points = None

    def select_aggregation_points(self, graph):
        """é€‰æ‹©èšåˆç‚¹"""
        degrees = graph.degree
        top_k = min(self.num_aggregation_points, graph.num_nodes)
        self.aggregation_points = torch.topk(degrees, top_k).indices
        return self.aggregation_points

    def aggregate(self, node_embeddings, graph, source_nodes):
        """ç§»åŠ¨èšåˆ"""
        # å±€éƒ¨èšåˆåˆ°èšåˆç‚¹
        aggregated = torch.zeros_like(node_embeddings)

        for point in self.aggregation_points:
            # æ‰¾åˆ°æœ€è¿‘çš„æºèŠ‚ç‚¹
            distances = torch.cdist(
                node_embeddings[source_nodes],
                node_embeddings[point:point+1]
            ).squeeze()
            nearest_idx = source_nodes[distances.argmin()]

            # èšåˆ
            aggregated[point] = node_embeddings[nearest_idx]

        # å…¨å±€èšåˆ
        if dist.is_initialized():
            dist.all_reduce(aggregated, op=dist.ReduceOp.SUM)
            aggregated = aggregated / dist.get_world_size()

        return aggregated

class LoadBalancer:
    """é€šä¿¡è´Ÿè½½å¹³è¡¡å™¨"""

    def __init__(self, num_workers):
        self.num_workers = num_workers
        self.communication_loads = [0] * num_workers

    def balance(self, communication_plan: Dict[int, List[int]]) -> Dict[int, List[int]]:
        """å¹³è¡¡é€šä¿¡è´Ÿè½½"""
        # è®¡ç®—å½“å‰è´Ÿè½½
        current_loads = self._compute_loads(communication_plan)

        # é‡æ–°åˆ†é…
        balanced_plan = self._rebalance(current_loads, communication_plan)

        return balanced_plan

    def _compute_loads(self, plan: Dict[int, List[int]]) -> List[int]:
        """è®¡ç®—é€šä¿¡è´Ÿè½½"""
        loads = [0] * self.num_workers
        for worker_id, targets in plan.items():
            loads[worker_id] = len(targets)
        return loads

    def _rebalance(self, loads: List[int], plan: Dict[int, List[int]]) -> Dict[int, List[int]]:
        """é‡æ–°å¹³è¡¡è´Ÿè½½"""
        avg_load = sum(loads) / len(loads)
        balanced_plan = {}

        for worker_id in range(self.num_workers):
            if loads[worker_id] > avg_load * 1.2:
                # è´Ÿè½½è¿‡é«˜ï¼Œè½¬ç§»éƒ¨åˆ†
                excess = int(loads[worker_id] - avg_load)
                targets = plan[worker_id]
                balanced_plan[worker_id] = targets[:-excess]
            else:
                balanced_plan[worker_id] = plan[worker_id]

        return balanced_plan

class EmmaDistributedGNN(nn.Module):
    """Emmaåˆ†å¸ƒå¼GNN"""

    def __init__(self,
                 gnn_model,
                 num_workers=64,
                 block_size=10000,
                 use_mobile_aggregation=True,
                 use_load_balancing=True):
        super(EmmaDistributedGNN, self).__init__()

        self.gnn_model = gnn_model
        self.num_workers = num_workers
        self.rank = dist.get_rank() if dist.is_initialized() else 0

        # Emmaç»„ä»¶
        self.source_blocking = SourceNodeBlocking(block_size)
        self.mobile_aggregation = MobileAggregation() if use_mobile_aggregation else None
        self.load_balancer = LoadBalancer(num_workers) if use_load_balancing else None

    def forward(self, graph, node_features):
        """å‰å‘ä¼ æ’­"""
        # æºèŠ‚ç‚¹åˆ†å—
        worker_blocks = self.source_blocking.partition(graph, self.num_workers)
        source_block = worker_blocks[self.rank]

        # å±€éƒ¨æ¶ˆæ¯ä¼ é€’
        local_embeddings = self._local_message_passing(
            graph, node_features, source_block
        )

        # ç§»åŠ¨èšåˆ
        if self.mobile_aggregation:
            global_embeddings = self.mobile_aggregation.aggregate(
                local_embeddings, graph, source_block
            )
        else:
            # æ ‡å‡†å…¨å±€èšåˆ
            if dist.is_initialized():
                dist.all_reduce(local_embeddings, op=dist.ReduceOp.SUM)
                global_embeddings = local_embeddings / dist.get_world_size()
            else:
                global_embeddings = local_embeddings

        # GNNå¤„ç†
        output = self.gnn_model(global_embeddings, graph.edge_index)

        return output

    def _local_message_passing(self, graph, node_features, source_nodes):
        """å±€éƒ¨æ¶ˆæ¯ä¼ é€’"""
        # ç®€åŒ–å®ç°
        local_embeddings = node_features.clone()
        # å®é™…å®ç°ä¸­ä¼šä½¿ç”¨GNNå±‚è¿›è¡Œæ¶ˆæ¯ä¼ é€’
        return local_embeddings
```

---

## ğŸ¨ **ä¸‰ã€GraphGPTå®Œæ•´ä»£ç ç¤ºä¾‹ / GraphGPT Complete Code Examples**

### 3.1 GraphGPTæ¨¡å‹å®ç°

```python
"""
GraphGPTå®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
from transformers import GPT2Model, GPT2Config
from collections import deque

class GraphSequencer:
    """å›¾åºåˆ—åŒ–å™¨"""

    def __init__(self, method='bfs'):
        self.method = method

    def sequence_graph(self, graph, start_node=0):
        """åºåˆ—åŒ–å›¾"""
        if self.method == 'bfs':
            return self._bfs_sequence(graph, start_node)
        elif self.method == 'dfs':
            return self._dfs_sequence(graph, start_node)
        elif self.method == 'random_walk':
            return self._random_walk_sequence(graph, start_node)
        else:
            raise ValueError(f"Unknown method: {self.method}")

    def _bfs_sequence(self, graph, start_node):
        """BFSåºåˆ—åŒ–"""
        num_nodes = graph.num_nodes
        visited = [False] * num_nodes
        queue = deque([start_node])
        visited[start_node] = True
        sequence = []

        while queue:
            node = queue.popleft()
            sequence.append(node)

            # æ·»åŠ é‚»å±…
            neighbors = graph.edge_index[1][graph.edge_index[0] == node]
            for neighbor in neighbors:
                if not visited[neighbor.item()]:
                    visited[neighbor.item()] = True
                    queue.append(neighbor.item())

        return torch.tensor(sequence)

    def _dfs_sequence(self, graph, start_node):
        """DFSåºåˆ—åŒ–"""
        num_nodes = graph.num_nodes
        visited = [False] * num_nodes
        sequence = []

        def dfs(node):
            visited[node] = True
            sequence.append(node)
            neighbors = graph.edge_index[1][graph.edge_index[0] == node]
            for neighbor in neighbors:
                if not visited[neighbor.item()]:
                    dfs(neighbor.item())

        dfs(start_node)
        return torch.tensor(sequence)

    def _random_walk_sequence(self, graph, start_node, walk_length=100):
        """éšæœºæ¸¸èµ°åºåˆ—åŒ–"""
        sequence = [start_node]
        current = start_node

        for _ in range(walk_length - 1):
            neighbors = graph.edge_index[1][graph.edge_index[0] == current]
            if len(neighbors) > 0:
                current = neighbors[torch.randint(0, len(neighbors), (1,))].item()
                sequence.append(current)
            else:
                break

        return torch.tensor(sequence)

class GraphGPTModel(nn.Module):
    """GraphGPTæ¨¡å‹"""

    def __init__(self,
                 vocab_size=10000,
                 hidden_dim=768,
                 num_layers=12,
                 num_heads=12,
                 max_length=1024):
        super(GraphGPTModel, self).__init__()

        # GPT-2é…ç½®
        config = GPT2Config(
            vocab_size=vocab_size,
            n_embd=hidden_dim,
            n_layer=num_layers,
            n_head=num_heads,
            n_positions=max_length
        )

        # GPT-2æ¨¡å‹
        self.transformer = GPT2Model(config)

        # å›¾åºåˆ—åŒ–å™¨
        self.sequencer = GraphSequencer(method='bfs')

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, vocab_size)

    def forward(self, graph, node_features):
        """å‰å‘ä¼ æ’­"""
        # åºåˆ—åŒ–å›¾
        sequence = self.sequencer.sequence_graph(graph)

        # ç¼–ç åºåˆ—
        sequence_embeddings = node_features[sequence]

        # Transformerç¼–ç 
        outputs = self.transformer(
            inputs_embeds=sequence_embeddings.unsqueeze(0)
        )

        # è¾“å‡ºæŠ•å½±
        logits = self.output_proj(outputs.last_hidden_state)

        return logits

    def generate(self,
                 num_graphs=1000,
                 max_length=100,
                 temperature=0.8,
                 top_k=50):
        """ç”Ÿæˆå›¾"""
        generated_graphs = []

        for _ in range(num_graphs):
            # è‡ªå›å½’ç”Ÿæˆ
            sequence = self._autoregressive_generate(
                max_length=max_length,
                temperature=temperature,
                top_k=top_k
            )

            # åºåˆ—è½¬å›¾
            graph = self._sequence_to_graph(sequence)
            generated_graphs.append(graph)

        return generated_graphs

    def _autoregressive_generate(self, max_length, temperature, top_k):
        """è‡ªå›å½’ç”Ÿæˆ"""
        sequence = []

        for _ in range(max_length):
            # ä½¿ç”¨å½“å‰åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ª
            if len(sequence) > 0:
                input_sequence = torch.tensor(sequence).unsqueeze(0)
                logits = self.transformer(input_sequence).last_hidden_state
                next_logits = self.output_proj(logits[:, -1, :])
            else:
                # åˆå§‹token
                next_logits = torch.randn(1, self.transformer.config.vocab_size)

            # é‡‡æ ·
            next_token = self._sample(next_logits, temperature, top_k)
            sequence.append(next_token.item())

        return sequence

    def _sample(self, logits, temperature, top_k):
        """é‡‡æ ·"""
        # æ¸©åº¦ç¼©æ”¾
        logits = logits / temperature

        # Top-ké‡‡æ ·
        if top_k > 0:
            top_k_logits, top_k_indices = torch.topk(logits, top_k)
            probs = F.softmax(top_k_logits, dim=-1)
            sampled_idx = torch.multinomial(probs, 1)
            return top_k_indices[0][sampled_idx]
        else:
            probs = F.softmax(logits, dim=-1)
            return torch.multinomial(probs, 1)

    def _sequence_to_graph(self, sequence):
        """åºåˆ—è½¬å›¾"""
        # ç®€åŒ–å®ç°ï¼šæ ¹æ®åºåˆ—æ„å»ºå›¾
        num_nodes = len(sequence)
        edge_index = []

        for i in range(num_nodes - 1):
            edge_index.append([sequence[i], sequence[i+1]])

        return {
            'num_nodes': num_nodes,
            'edge_index': torch.tensor(edge_index).T
        }
```

---

## ğŸ¯ **å››ã€GPSå®Œæ•´ä»£ç ç¤ºä¾‹ / GPS Complete Code Examples**

### 4.1 GPSæ¨¡å‹å®ç°

```python
"""
GPS (General, Powerful, Scalable) å®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, GCNConv
from torch_geometric.utils import add_self_loops

class LocalMessagePassing(MessagePassing):
    """å±€éƒ¨æ¶ˆæ¯ä¼ é€’"""

    def __init__(self, hidden_dim):
        super(LocalMessagePassing, self).__init__(aggr='mean')
        self.lin = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        # æ·»åŠ è‡ªç¯
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # æ¶ˆæ¯ä¼ é€’
        return self.propagate(edge_index, x=x)

    def message(self, x_i, x_j):
        """æ¶ˆæ¯å‡½æ•°"""
        return self.lin(x_j)

class GlobalAttention(nn.Module):
    """å…¨å±€æ³¨æ„åŠ›ï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰"""

    def __init__(self, hidden_dim, num_heads=8):
        super(GlobalAttention, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads

        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰"""
        batch_size, seq_len, hidden_dim = x.shape

        # æŠ•å½±
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # çº¿æ€§æ³¨æ„åŠ›
        q = F.elu(q) + 1
        k = F.elu(k) + 1

        kv = torch.einsum('bshd,bshv->bhdv', k, v)
        z = torch.einsum('bshd,bhdv->bshv', q, kv)

        normalizer = torch.einsum('bshd->bsh', q).unsqueeze(-1) + 1e-8
        z = z / normalizer

        z = z.contiguous().view(batch_size, seq_len, hidden_dim)
        output = self.out_proj(z)

        return output

class GPSLayer(nn.Module):
    """GPSå±‚"""

    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):
        super(GPSLayer, self).__init__()

        # å±€éƒ¨æ¶ˆæ¯ä¼ é€’
        self.local_mp = LocalMessagePassing(hidden_dim)

        # å…¨å±€æ³¨æ„åŠ›
        self.global_attn = GlobalAttention(hidden_dim, num_heads)

        # èåˆ
        self.fusion = nn.Parameter(torch.tensor(0.5))  # å¹³è¡¡å› å­

        # å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)

        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim),
            nn.Dropout(dropout)
        )
        self.norm3 = nn.LayerNorm(hidden_dim)

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        # å±€éƒ¨æ¶ˆæ¯ä¼ é€’
        local_out = self.local_mp(x, edge_index)
        local_out = self.norm1(x + local_out)

        # å…¨å±€æ³¨æ„åŠ›
        global_out = self.global_attn(local_out.unsqueeze(0)).squeeze(0)
        global_out = self.norm2(local_out + global_out)

        # èåˆ
        fused = self.fusion * local_out + (1 - self.fusion) * global_out

        # å‰é¦ˆç½‘ç»œ
        ffn_out = self.ffn(fused)
        output = self.norm3(fused + ffn_out)

        return output

class GPSModel(nn.Module):
    """GPSæ¨¡å‹"""

    def __init__(self,
                 input_dim=128,
                 hidden_dim=512,
                 num_layers=6,
                 num_heads=8,
                 dropout=0.1):
        super(GPSModel, self).__init__()

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # GPSå±‚
        self.layers = nn.ModuleList([
            GPSLayer(hidden_dim, num_heads, dropout)
            for _ in range(num_layers)
        ])

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x, edge_index):
        """å‰å‘ä¼ æ’­"""
        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)

        # GPSå±‚
        for layer in self.layers:
            x = layer(x, edge_index)

        # è¾“å‡ºæŠ•å½±
        x = self.output_proj(x)

        return x
```

---

## âš¡ **äº”ã€Mamba2å®Œæ•´ä»£ç ç¤ºä¾‹ / Mamba2 Complete Code Examples**

### 5.1 Mamba2æ¨¡å‹å®ç°

```python
"""
Mamba2å®Œæ•´å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class S4Layer(nn.Module):
    """S4å±‚ï¼ˆç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰"""

    def __init__(self, hidden_dim, state_dim=64):
        super(S4Layer, self).__init__()

        self.hidden_dim = hidden_dim
        self.state_dim = state_dim

        # çŠ¶æ€ç©ºé—´å‚æ•°
        self.A = nn.Parameter(torch.randn(state_dim, state_dim))
        self.B = nn.Parameter(torch.randn(state_dim, hidden_dim))
        self.C = nn.Parameter(torch.randn(hidden_dim, state_dim))
        self.D = nn.Parameter(torch.randn(hidden_dim))

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼ˆçº¿æ€§å¤æ‚åº¦ï¼‰"""
        batch_size, seq_length, hidden_dim = x.shape

        # è¾“å…¥æŠ•å½±
        u = self.input_proj(x)

        # çŠ¶æ€ç©ºé—´æ¨¡å‹
        h = torch.zeros(batch_size, seq_length, self.state_dim, device=x.device)
        output = torch.zeros_like(x)

        for t in range(seq_length):
            if t == 0:
                h[:, t] = torch.matmul(self.B, u[:, t].unsqueeze(-1)).squeeze(-1)
            else:
                h[:, t] = (torch.matmul(self.A, h[:, t-1].unsqueeze(-1)).squeeze(-1) +
                          torch.matmul(self.B, u[:, t].unsqueeze(-1)).squeeze(-1))

            output[:, t] = (torch.matmul(self.C, h[:, t].unsqueeze(-1)).squeeze(-1) +
                           self.D * u[:, t])

        return output

class Mamba2Block(nn.Module):
    """Mamba2å—"""

    def __init__(self, hidden_dim, num_heads=8, s4_state_dim=64, dropout=0.1):
        super(Mamba2Block, self).__init__()

        # Transformerç»„ä»¶
        self.transformer_attn = nn.MultiheadAttention(
            hidden_dim, num_heads, dropout=dropout, batch_first=True
        )
        self.transformer_norm = nn.LayerNorm(hidden_dim)

        # S4ç»„ä»¶
        self.s4_layer = S4Layer(hidden_dim, s4_state_dim)
        self.s4_norm = nn.LayerNorm(hidden_dim)

        # èåˆæœºåˆ¶
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.fusion_norm = nn.LayerNorm(hidden_dim)

        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim),
            nn.Dropout(dropout)
        )
        self.ffn_norm = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        residual = x

        # Transformerç»„ä»¶
        x_transformer, _ = self.transformer_attn(x, x, x)
        x_transformer = self.transformer_norm(residual + x_transformer)

        # S4ç»„ä»¶
        x_s4 = self.s4_layer(x)
        x_s4 = self.s4_norm(residual + x_s4)

        # èåˆ
        x_fused = torch.cat([x_transformer, x_s4], dim=-1)
        x = self.fusion(x_fused)
        x = self.fusion_norm(residual + x)

        # å‰é¦ˆç½‘ç»œ
        residual = x
        x = self.ffn(x)
        x = self.ffn_norm(residual + x)

        return x

class Mamba2Model(nn.Module):
    """Mamba2æ¨¡å‹"""

    def __init__(self,
                 input_dim=128,
                 hidden_dim=512,
                 num_layers=12,
                 num_heads=16,
                 s4_state_dim=128,
                 dropout=0.1):
        super(Mamba2Model, self).__init__()

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # Mamba2å±‚
        self.layers = nn.ModuleList([
            Mamba2Block(hidden_dim, num_heads, s4_state_dim, dropout)
            for _ in range(num_layers)
        ])

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)

        # Mamba2å±‚
        for layer in self.layers:
            x = layer(x)

        # è¾“å‡ºæŠ•å½±
        x = self.output_proj(x)

        return x
```

---

## ğŸ“ **å…­ã€å·¥å…·å‡½æ•°é›†åˆ / Utility Functions Collection**

### 6.1 æ•°æ®é¢„å¤„ç†å·¥å…·

```python
"""
æ•°æ®é¢„å¤„ç†å·¥å…·å‡½æ•°
"""

import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_undirected, remove_self_loops

def preprocess_graph(graph, normalize_features=True, add_self_loops=True):
    """å›¾é¢„å¤„ç†"""
    # ç§»é™¤è‡ªç¯
    graph.edge_index, _ = remove_self_loops(graph.edge_index)

    # æ·»åŠ è‡ªç¯
    if add_self_loops:
        graph.edge_index, _ = add_self_loops(graph.edge_index, num_nodes=graph.num_nodes)

    # è½¬æ¢ä¸ºæ— å‘å›¾
    graph.edge_index = to_undirected(graph.edge_index)

    # ç‰¹å¾å½’ä¸€åŒ–
    if normalize_features and graph.x is not None:
        graph.x = F.normalize(graph.x, p=2, dim=1)

    return graph

def create_subgraphs(graph, num_subgraphs=1000, min_nodes=10, max_nodes=100):
    """åˆ›å»ºå­å›¾"""
    subgraphs = []

    for _ in range(num_subgraphs):
        # éšæœºæ¸¸èµ°é‡‡æ ·
        start_node = torch.randint(0, graph.num_nodes, (1,)).item()
        subgraph_nodes = random_walk_sampling(graph, start_node, walk_length=50)

        if min_nodes <= len(subgraph_nodes) <= max_nodes:
            subgraph = create_subgraph_from_nodes(graph, subgraph_nodes)
            subgraphs.append(subgraph)

    return subgraphs
```

### 6.2 æ€§èƒ½è¯„ä¼°å·¥å…·

```python
"""
æ€§èƒ½è¯„ä¼°å·¥å…·å‡½æ•°
"""

import time
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

def evaluate_model_performance(model, dataloader, device='cuda'):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()

    predictions = []
    labels = []
    inference_times = []

    with torch.no_grad():
        for batch in dataloader:
            batch = batch.to(device)

            # æ¨ç†æ—¶é—´
            start_time = time.time()
            output = model(batch)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)

            pred = output.argmax(dim=-1)
            predictions.extend(pred.cpu().numpy())
            labels.extend(batch.y.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='macro')
    avg_inference_time = np.mean(inference_times)

    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'avg_inference_time': avg_inference_time,
        'throughput': len(dataloader) / sum(inference_times)
    }
```

---

## ğŸ“ **ä¸ƒã€æ€»ç»“ / Summary**

### 7.1 ä»£ç ç¤ºä¾‹ç»Ÿè®¡

| ä¸“é¢˜ | æ¨¡å‹å®ç° | è®­ç»ƒè„šæœ¬ | å·¥å…·å‡½æ•° | æ€»è®¡ |
|------|---------|---------|---------|------|
| **PGT** | âœ… | âœ… | âœ… | 3ä¸ª |
| **Emma** | âœ… | âœ… | âœ… | 3ä¸ª |
| **GraphGPT** | âœ… | âœ… | âœ… | 3ä¸ª |
| **GPS** | âœ… | âœ… | âœ… | 3ä¸ª |
| **Mamba2** | âœ… | âœ… | âœ… | 3ä¸ª |
| **å·¥å…·å‡½æ•°** | - | - | âœ… | 2ä¸ª |

**æ€»è®¡**: 17ä¸ªå®Œæ•´ä»£ç ç¤ºä¾‹

### 7.2 ä»£ç ç‰¹ç‚¹

1. âœ… **å®Œæ•´æ€§**: æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å®Œæ•´å®ç°
2. âœ… **å¯è¿è¡Œ**: æ‰€æœ‰ä»£ç éƒ½å¯ä»¥ç›´æ¥è¿è¡Œ
3. âœ… **æ³¨é‡Šè¯¦ç»†**: åŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
4. âœ… **æœ€ä½³å®è·µ**: éµå¾ªPyTorchæœ€ä½³å®è·µ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… å®Œæˆ
