# AIç½‘ç»œåº”ç”¨æ¨¡å¼å®ç°æŒ‡å— / Implementation Guide

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾› AI ç½‘ç»œä¸è‡ªé€‚åº”èŒƒç•´ç†è®ºåº”ç”¨æ¨¡å¼çš„å®Œæ•´å®ç°æŒ‡å—ï¼Œæ¶µç›–ç¯å¢ƒé…ç½®ã€æ ¸å¿ƒç»„ä»¶å®ç°ã€é›†æˆæµ‹è¯•ä¸éƒ¨ç½²æµç¨‹ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´2æœˆ
**çŠ¶æ€**: âœ… å®Œæˆ

---

## ä¸€ã€ç¯å¢ƒé…ç½® / Environment Setup

### 1.1 ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| GPU | NVIDIA RTX 3080 (10GB) | NVIDIA A100 (40GB) |
| CPU | 8 æ ¸ | 32 æ ¸ |
| å†…å­˜ | 32 GB | 128 GB |
| å­˜å‚¨ | 500 GB SSD | 2 TB NVMe SSD |

### 1.2 è½¯ä»¶ä¾èµ–

```bash
# Python ç¯å¢ƒ
conda create -n ai-network python=3.10
conda activate ai-network

# PyTorch + CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å›¾ç¥ç»ç½‘ç»œåº“
pip install torch-geometric
pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html

# LLM ç›¸å…³
pip install transformers accelerate bitsandbytes
pip install langchain langchain-community

# å›¾æ•°æ®åº“
pip install neo4j py2neo

# å‘é‡æ•°æ®åº“
pip install pymilvus

# å…¶ä»–å·¥å…·
pip install fastapi uvicorn redis httpx
```

### 1.3 é…ç½®æ–‡ä»¶

```yaml
# config.yaml
model:
  llm_name: "Qwen/Qwen2.5-7B-Instruct"
  graph_encoder: "GAT"
  hidden_dim: 768
  num_heads: 8
  num_layers: 2

database:
  neo4j:
    uri: "bolt://localhost:7687"
    user: "neo4j"
    password: "${NEO4J_PASSWORD}"
  milvus:
    host: "localhost"
    port: 19530
  redis:
    url: "redis://localhost:6379"

inference:
  batch_size: 32
  max_seq_length: 512
  max_hops: 3
  top_k: 10

server:
  host: "0.0.0.0"
  port: 8000
  workers: 4
```

---

## äºŒã€æ ¸å¿ƒç»„ä»¶å®ç° / Core Components

### 2.1 å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨

```python
import torch
import torch.nn as nn
from torch_geometric.nn import GATConv, GCNConv, SAGEConv
from torch_geometric.data import Data, Batch

class GraphEncoder(nn.Module):
    """é€šç”¨å›¾ç¼–ç å™¨"""

    ENCODER_TYPES = {
        "GAT": GATConv,
        "GCN": GCNConv,
        "SAGE": SAGEConv
    }

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int = 2,
        encoder_type: str = "GAT",
        num_heads: int = 8,
        dropout: float = 0.1
    ):
        super().__init__()

        self.input_proj = nn.Linear(input_dim, hidden_dim)

        conv_class = self.ENCODER_TYPES[encoder_type]

        self.convs = nn.ModuleList()
        self.norms = nn.ModuleList()

        for i in range(num_layers):
            in_dim = hidden_dim
            out_dim = hidden_dim if i < num_layers - 1 else output_dim

            if encoder_type == "GAT":
                self.convs.append(conv_class(
                    in_dim, out_dim // num_heads, heads=num_heads, concat=True
                ))
            else:
                self.convs.append(conv_class(in_dim, out_dim))

            self.norms.append(nn.LayerNorm(out_dim))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [num_nodes, input_dim] èŠ‚ç‚¹ç‰¹å¾
            edge_index: [2, num_edges] è¾¹ç´¢å¼•

        Returns:
            node_emb: [num_nodes, output_dim] èŠ‚ç‚¹åµŒå…¥
        """
        x = self.input_proj(x)

        for conv, norm in zip(self.convs, self.norms):
            x_new = conv(x, edge_index)
            x_new = norm(x_new)
            x_new = torch.relu(x_new)
            x_new = self.dropout(x_new)
            x = x + x_new  # æ®‹å·®è¿æ¥

        return x

    def encode_batch(self, batch: Batch) -> torch.Tensor:
        """æ‰¹é‡ç¼–ç """
        return self.forward(batch.x, batch.edge_index)
```

### 2.2 è‡ªé€‚åº”å›¾ç»“æ„å­¦ä¹ 

```python
class AdaptiveGraphLearner(nn.Module):
    """è‡ªé€‚åº”å›¾ç»“æ„å­¦ä¹ æ¨¡å—"""

    def __init__(
        self,
        hidden_dim: int,
        temperature: float = 0.5,
        threshold: float = 0.5
    ):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.temperature = temperature
        self.threshold = threshold

        # è¾¹é¢„æµ‹ç½‘ç»œ
        self.edge_predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

        # èŠ‚ç‚¹é‡è¦æ€§è¯„ä¼°
        self.node_importance = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )

    def forward(
        self,
        node_emb: torch.Tensor,
        edge_index: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        è‡ªé€‚åº”è°ƒæ•´å›¾ç»“æ„

        Args:
            node_emb: [num_nodes, hidden_dim] èŠ‚ç‚¹åµŒå…¥
            edge_index: [2, num_edges] åŸå§‹è¾¹ç´¢å¼•

        Returns:
            new_edge_index: è°ƒæ•´åçš„è¾¹ç´¢å¼•
            edge_weights: è¾¹æƒé‡
        """
        num_nodes = node_emb.size(0)

        # è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§
        importance = self.node_importance(node_emb).squeeze(-1)

        # è®¡ç®—æ‰€æœ‰å¯èƒ½è¾¹çš„æ¦‚ç‡ï¼ˆç¨€ç–ç‰ˆæœ¬ï¼‰
        src, dst = edge_index
        edge_features = torch.cat([node_emb[src], node_emb[dst]], dim=-1)
        edge_probs = self.edge_predictor(edge_features).squeeze(-1)

        # ç»“åˆèŠ‚ç‚¹é‡è¦æ€§
        edge_weights = edge_probs * (importance[src] + importance[dst]) / 2

        # Gumbel-Softmax é‡‡æ ·ï¼ˆå¯å¾®ï¼‰
        if self.training:
            edge_weights = self._gumbel_softmax(edge_weights)

        # è¾¹è¿‡æ»¤
        mask = edge_weights > self.threshold
        new_edge_index = edge_index[:, mask]
        filtered_weights = edge_weights[mask]

        return new_edge_index, filtered_weights

    def _gumbel_softmax(self, logits: torch.Tensor) -> torch.Tensor:
        """Gumbel-Softmax é‡‡æ ·"""
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)
        return torch.sigmoid((logits + gumbel_noise) / self.temperature)
```

### 2.3 Graph-LLM èåˆæ¨¡å—

```python
from transformers import AutoModel, AutoTokenizer

class GraphLLMFusion(nn.Module):
    """Graph-LLM èåˆæ¨¡å—"""

    def __init__(
        self,
        llm_name: str,
        graph_dim: int = 768,
        fusion_type: str = "cross_attention"
    ):
        super().__init__()

        # LLM ç¼–ç å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)
        self.llm_encoder = AutoModel.from_pretrained(llm_name)

        llm_dim = self.llm_encoder.config.hidden_size

        # ç»´åº¦å¯¹é½
        self.graph_proj = nn.Linear(graph_dim, llm_dim)

        # èåˆç±»å‹
        self.fusion_type = fusion_type

        if fusion_type == "cross_attention":
            self.fusion = nn.MultiheadAttention(
                embed_dim=llm_dim,
                num_heads=8,
                batch_first=True
            )
        elif fusion_type == "concat":
            self.fusion = nn.Sequential(
                nn.Linear(llm_dim * 2, llm_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(llm_dim, llm_dim)
            )
        elif fusion_type == "gate":
            self.gate = nn.Sequential(
                nn.Linear(llm_dim * 2, llm_dim),
                nn.Sigmoid()
            )

    def forward(
        self,
        text: str,
        graph_emb: torch.Tensor
    ) -> torch.Tensor:
        """
        èåˆæ–‡æœ¬å’Œå›¾åµŒå…¥

        Args:
            text: è¾“å…¥æ–‡æœ¬
            graph_emb: [num_nodes, graph_dim] å›¾èŠ‚ç‚¹åµŒå…¥

        Returns:
            fused_emb: èåˆåçš„åµŒå…¥
        """
        # æ–‡æœ¬ç¼–ç 
        inputs = self.tokenizer(
            text, return_tensors="pt",
            padding=True, truncation=True, max_length=512
        )
        text_emb = self.llm_encoder(**inputs).last_hidden_state

        # å›¾åµŒå…¥æŠ•å½±
        graph_emb = self.graph_proj(graph_emb)

        # èåˆ
        if self.fusion_type == "cross_attention":
            # å›¾ä½œä¸º key/valueï¼Œæ–‡æœ¬ä½œä¸º query
            graph_emb = graph_emb.unsqueeze(0)  # [1, num_nodes, dim]
            fused, _ = self.fusion(text_emb, graph_emb, graph_emb)
            fused_emb = text_emb + fused

        elif self.fusion_type == "concat":
            # æ± åŒ–å›¾åµŒå…¥å¹¶ä¸æ–‡æœ¬æ¯ä¸ªä½ç½®æ‹¼æ¥
            graph_pool = graph_emb.mean(dim=0, keepdim=True)
            graph_pool = graph_pool.expand(text_emb.size(0), text_emb.size(1), -1)
            concat = torch.cat([text_emb, graph_pool], dim=-1)
            fused_emb = self.fusion(concat)

        elif self.fusion_type == "gate":
            graph_pool = graph_emb.mean(dim=0, keepdim=True)
            graph_pool = graph_pool.expand(text_emb.size(0), text_emb.size(1), -1)
            gate = self.gate(torch.cat([text_emb, graph_pool], dim=-1))
            fused_emb = gate * text_emb + (1 - gate) * graph_pool

        return fused_emb
```

---

## ä¸‰ã€API æœåŠ¡å®ç° / API Service

### 3.1 FastAPI æœåŠ¡

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="AI Network QA Service")

class QuestionRequest(BaseModel):
    question: str
    context: Optional[str] = None
    max_hops: int = 3
    top_k: int = 10

class AnswerResponse(BaseModel):
    answer: str
    confidence: float
    reasoning_path: List[dict]
    sources: List[str]

@app.post("/qa", response_model=AnswerResponse)
async def answer_question(request: QuestionRequest):
    """é—®ç­”æ¥å£"""
    try:
        result = qa_service.answer(
            question=request.question,
            context=request.context,
            max_hops=request.max_hops,
            top_k=request.top_k
        )
        return AnswerResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch_qa")
async def batch_answer(questions: List[str]):
    """æ‰¹é‡é—®ç­”æ¥å£"""
    results = await qa_service.batch_answer(questions)
    return {"answers": results}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model_loaded": True}
```

### 3.2 æµå¼å“åº”

```python
from fastapi.responses import StreamingResponse
import asyncio

@app.post("/qa/stream")
async def stream_answer(request: QuestionRequest):
    """æµå¼é—®ç­”æ¥å£"""

    async def generate():
        async for chunk in qa_service.stream_answer(request.question):
            yield f"data: {chunk}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## å››ã€æµ‹è¯•ä¸éªŒè¯ / Testing

### 4.1 å•å…ƒæµ‹è¯•

```python
import pytest
import torch
from models import GraphEncoder, AdaptiveGraphLearner, GraphLLMFusion

class TestGraphEncoder:
    def test_forward(self):
        encoder = GraphEncoder(
            input_dim=128,
            hidden_dim=256,
            output_dim=256,
            num_layers=2
        )

        x = torch.randn(100, 128)
        edge_index = torch.randint(0, 100, (2, 500))

        output = encoder(x, edge_index)

        assert output.shape == (100, 256)

    def test_batch_encode(self):
        encoder = GraphEncoder(
            input_dim=128,
            hidden_dim=256,
            output_dim=256
        )

        # åˆ›å»ºæ‰¹æ¬¡
        from torch_geometric.data import Data, Batch
        graphs = [
            Data(x=torch.randn(50, 128), edge_index=torch.randint(0, 50, (2, 200)))
            for _ in range(4)
        ]
        batch = Batch.from_data_list(graphs)

        output = encoder.encode_batch(batch)

        assert output.shape[0] == 200  # 4 * 50

class TestAdaptiveGraphLearner:
    def test_graph_adaptation(self):
        learner = AdaptiveGraphLearner(hidden_dim=256)

        node_emb = torch.randn(100, 256)
        edge_index = torch.randint(0, 100, (2, 500))

        new_edge_index, edge_weights = learner(node_emb, edge_index)

        assert new_edge_index.shape[0] == 2
        assert edge_weights.shape[0] == new_edge_index.shape[1]
```

### 4.2 é›†æˆæµ‹è¯•

```python
class TestGraphLLMQA:
    @pytest.fixture
    def qa_service(self):
        return GraphLLMQA(
            kg_uri="bolt://localhost:7687",
            llm_model="Qwen/Qwen2.5-7B-Instruct"
        )

    def test_simple_question(self, qa_service):
        answer = qa_service.answer("ä»€ä¹ˆæ˜¯å›¾ç¥ç»ç½‘ç»œï¼Ÿ")

        assert len(answer) > 0
        assert "å›¾" in answer or "GNN" in answer

    def test_multi_hop_question(self, qa_service):
        answer = qa_service.answer(
            "å¼ ä¸‰è´Ÿè´£çš„é¡¹ç›®æœ‰å“ªäº›å®¢æˆ·ï¼Ÿ"
        )

        assert len(answer) > 0
```

---

## äº”ã€éƒ¨ç½²æµç¨‹ / Deployment

### 5.1 Docker é•œåƒæ„å»º

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-cudnn8-runtime-ubuntu22.04

# å®‰è£… Python
RUN apt-get update && apt-get install -y python3.10 python3-pip

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä½¿ç”¨å·æŒ‚è½½ï¼‰
RUN python3 -c "from transformers import AutoModel; AutoModel.from_pretrained('Qwen/Qwen2.5-7B-Instruct')"

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.2 Docker Compose

```yaml
version: '3.8'

services:
  qa-service:
    build: .
    ports:
      - "8000:8000"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - REDIS_URL=redis://redis:6379
    depends_on:
      - neo4j
      - redis
      - milvus
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  neo4j:
    image: neo4j:5.15
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data
    environment:
      - NEO4J_AUTH=neo4j/password

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  milvus:
    image: milvusdb/milvus:v2.3.0
    ports:
      - "19530:19530"
    volumes:
      - milvus_data:/var/lib/milvus

volumes:
  neo4j_data:
  milvus_data:
```

---

## å…­ã€ç›‘æ§ä¸è¿ç»´ / Monitoring

### 6.1 Prometheus æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
QUERY_COUNTER = Counter(
    'ai_network_queries_total',
    'Total number of queries',
    ['endpoint', 'status']
)

LATENCY_HISTOGRAM = Histogram(
    'ai_network_query_latency_seconds',
    'Query latency in seconds',
    ['endpoint'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

MODEL_MEMORY_GAUGE = Gauge(
    'ai_network_model_memory_bytes',
    'Model memory usage in bytes'
)
```

### 6.2 Grafana ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "AI Network QA Service",
    "panels": [
      {
        "title": "QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ai_network_queries_total[5m])"
          }
        ]
      },
      {
        "title": "Latency P99",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(ai_network_query_latency_seconds_bucket[5m]))"
          }
        ]
      }
    ]
  }
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´2æœˆ
**æœ€åæ›´æ–°**: 2025å¹´2æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
