# AIç½‘ç»œåº”ç”¨æ¨¡å¼å·¥å…·é›†æˆæŒ‡å— / Tool Integration Guide

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾› AI ç½‘ç»œä¸è‡ªé€‚åº”èŒƒç•´åº”ç”¨æ¨¡å¼æ‰€éœ€å·¥å…·çš„é›†æˆé…ç½®æŒ‡å—ï¼Œæ¶µç›–å›¾ç¥ç»ç½‘ç»œæ¡†æ¶ã€LLM æœåŠ¡ã€å›¾æ•°æ®åº“ã€å‘é‡æ•°æ®åº“ç­‰æ ¸å¿ƒå·¥å…·ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´2æœˆ
**çŠ¶æ€**: âœ… å®Œæˆ

---

## ä¸€ã€å·¥å…·æ ˆæ€»è§ˆ / Tool Stack Overview

| ç±»åˆ« | å·¥å…· | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|------|
| å›¾ç¥ç»ç½‘ç»œ | PyTorch Geometric | 2.4+ | GNN æ¨¡å‹æ„å»º |
| å›¾ç¥ç»ç½‘ç»œ | DGL | 1.1+ | å¤§è§„æ¨¡å›¾è®­ç»ƒ |
| LLM æ¡†æ¶ | Transformers | 4.36+ | LLM åŠ è½½ä¸æ¨ç† |
| LLM ç¼–æ’ | LangChain | 0.1+ | RAG ä¸ Agent |
| å›¾æ•°æ®åº“ | Neo4j | 5.x | çŸ¥è¯†å›¾è°±å­˜å‚¨ |
| å‘é‡æ•°æ®åº“ | Milvus | 2.3+ | å‘é‡æ£€ç´¢ |
| ç¼“å­˜ | Redis | 7.x | æŸ¥è¯¢ç¼“å­˜ |
| æ¶ˆæ¯é˜Ÿåˆ— | Kafka | 3.x | æµå¼å¤„ç† |
| ç›‘æ§ | Prometheus + Grafana | - | æŒ‡æ ‡ç›‘æ§ |

---

## äºŒã€PyTorch Geometric é›†æˆ / PyTorch Geometric Integration

### 2.1 å®‰è£…é…ç½®

```bash
# æ ¹æ® CUDA ç‰ˆæœ¬å®‰è£…
pip install torch-geometric

# å®‰è£…å¯é€‰ä¾èµ–
pip install torch-scatter torch-sparse torch-cluster torch-spline-conv \
    -f https://data.pyg.org/whl/torch-2.1.0+cu118.html
```

### 2.2 åŸºç¡€ä½¿ç”¨

```python
import torch
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, GATConv, SAGEConv

# åˆ›å»ºå›¾æ•°æ®
edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)
x = torch.randn(3, 16)  # 3ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ª16ç»´ç‰¹å¾
data = Data(x=x, edge_index=edge_index)

# å®šä¹‰ GNN æ¨¡å‹
class GNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=4)
        self.conv2 = GATConv(hidden_channels * 4, out_channels, heads=1)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x

model = GNN(16, 32, 8)
out = model(data.x, data.edge_index)
```

### 2.3 å¤§è§„æ¨¡å›¾å¤„ç†

```python
from torch_geometric.loader import NeighborLoader, ClusterLoader

# é‚»å±…é‡‡æ ·åŠ è½½å™¨
loader = NeighborLoader(
    data,
    num_neighbors=[10, 5],  # æ¯å±‚é‡‡æ ·é‚»å±…æ•°
    batch_size=128,
    input_nodes=data.train_mask
)

# å›¾èšç±»åŠ è½½å™¨
from torch_geometric.loader import ClusterData
cluster_data = ClusterData(data, num_parts=100)
cluster_loader = ClusterLoader(cluster_data, batch_size=10)
```

---

## ä¸‰ã€DGL é›†æˆ / DGL Integration

### 3.1 å®‰è£…é…ç½®

```bash
# CUDA 11.8
pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html
```

### 3.2 ä¸ PyG æ•°æ®äº’è½¬

```python
import dgl
from torch_geometric.data import Data

def pyg_to_dgl(pyg_data: Data) -> dgl.DGLGraph:
    """PyG æ•°æ®è½¬ DGL"""
    g = dgl.graph(
        (pyg_data.edge_index[0], pyg_data.edge_index[1]),
        num_nodes=pyg_data.num_nodes
    )
    g.ndata['feat'] = pyg_data.x
    if hasattr(pyg_data, 'y'):
        g.ndata['label'] = pyg_data.y
    return g

def dgl_to_pyg(g: dgl.DGLGraph) -> Data:
    """DGL è½¬ PyG æ•°æ®"""
    src, dst = g.edges()
    edge_index = torch.stack([src, dst])
    return Data(
        x=g.ndata['feat'],
        edge_index=edge_index,
        y=g.ndata.get('label')
    )
```

### 3.3 åˆ†å¸ƒå¼è®­ç»ƒ

```python
import dgl
from dgl.dataloading import DataLoader, NeighborSampler

# åˆ†å¸ƒå¼å›¾åˆ†åŒº
dgl.distributed.partition_graph(
    graph, 'my_graph', num_parts=4,
    out_path='/path/to/partitions'
)

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
# åœ¨æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ
g = dgl.distributed.DistGraph('my_graph')
sampler = NeighborSampler([10, 5])
dataloader = DataLoader(g, train_nids, sampler, batch_size=1024)
```

---

## å››ã€LLM é›†æˆ / LLM Integration

### 4.1 Transformers é›†æˆ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class LLMService:
    """LLM æœåŠ¡å°è£…"""

    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )

    def generate(self, prompt: str, max_tokens: int = 256) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    async def stream_generate(self, prompt: str):
        """æµå¼ç”Ÿæˆ"""
        from transformers import TextIteratorStreamer
        from threading import Thread

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True)

        thread = Thread(target=self.model.generate, kwargs={
            **inputs,
            "streamer": streamer,
            "max_new_tokens": 256
        })
        thread.start()

        for chunk in streamer:
            yield chunk
```

### 4.2 LangChain é›†æˆ

```python
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.vectorstores import Milvus
from langchain.embeddings import HuggingFaceEmbeddings

# åˆ›å»º LLM
llm = HuggingFacePipeline.from_model_id(
    model_id="Qwen/Qwen2.5-7B-Instruct",
    task="text-generation",
    pipeline_kwargs={"max_new_tokens": 256}
)

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh-v1.5")
vectorstore = Milvus(
    embedding_function=embeddings,
    connection_args={"host": "localhost", "port": 19530},
    collection_name="knowledge_base"
)

# åˆ›å»º RAG é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# ä½¿ç”¨
answer = qa_chain.run("ä»€ä¹ˆæ˜¯å›¾ç¥ç»ç½‘ç»œï¼Ÿ")
```

### 4.3 Graph RAG

```python
from langchain.graphs import Neo4jGraph
from langchain.chains import GraphCypherQAChain

# è¿æ¥ Neo4j
graph = Neo4jGraph(
    url="bolt://localhost:7687",
    username="neo4j",
    password="password"
)

# åˆ›å»ºå›¾é—®ç­”é“¾
chain = GraphCypherQAChain.from_llm(
    llm=llm,
    graph=graph,
    verbose=True
)

# ä½¿ç”¨
answer = chain.run("å¼ ä¸‰è´Ÿè´£å“ªäº›é¡¹ç›®ï¼Ÿ")
```

---

## äº”ã€Neo4j é›†æˆ / Neo4j Integration

### 5.1 è¿æ¥é…ç½®

```python
from neo4j import GraphDatabase

class Neo4jService:
    """Neo4j æœåŠ¡å°è£…"""

    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def execute_query(self, query: str, parameters: dict = None):
        with self.driver.session() as session:
            result = session.run(query, parameters or {})
            return [record.data() for record in result]

    def get_subgraph(self, entity_names: list, max_hops: int = 2):
        """è·å–å®ä½“å‘¨å›´å­å›¾"""
        query = f"""
        MATCH path = (start)-[*1..{max_hops}]-(end)
        WHERE start.name IN $names
        RETURN path
        """
        return self.execute_query(query, {"names": entity_names})
```

### 5.2 å›¾åµŒå…¥å¯¼å‡º

```python
def export_graph_for_gnn(neo4j_service) -> Data:
    """ä» Neo4j å¯¼å‡ºå›¾æ•°æ®ç”¨äº GNN"""

    # è·å–æ‰€æœ‰èŠ‚ç‚¹
    nodes = neo4j_service.execute_query("""
        MATCH (n) RETURN id(n) as id, labels(n) as labels, n.embedding as embedding
    """)

    # è·å–æ‰€æœ‰è¾¹
    edges = neo4j_service.execute_query("""
        MATCH (a)-[r]->(b)
        RETURN id(a) as src, id(b) as dst, type(r) as type
    """)

    # æ„å»ºèŠ‚ç‚¹æ˜ å°„
    node_id_map = {n['id']: i for i, n in enumerate(nodes)}

    # æ„å»ºç‰¹å¾çŸ©é˜µ
    x = torch.stack([torch.tensor(n['embedding']) for n in nodes])

    # æ„å»ºè¾¹ç´¢å¼•
    src = [node_id_map[e['src']] for e in edges]
    dst = [node_id_map[e['dst']] for e in edges]
    edge_index = torch.tensor([src, dst], dtype=torch.long)

    return Data(x=x, edge_index=edge_index)
```

---

## å…­ã€Milvus é›†æˆ / Milvus Integration

### 6.1 é›†åˆåˆ›å»º

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# è¿æ¥
connections.connect("default", host="localhost", port=19530)

# å®šä¹‰ Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="entity_id", dtype=DataType.VARCHAR, max_length=256),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields, "Entity embeddings")

# åˆ›å»ºé›†åˆ
collection = Collection("entity_embeddings", schema)

# åˆ›å»ºç´¢å¼•
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",
    "params": {"nlist": 1024}
}
collection.create_index("embedding", index_params)
```

### 6.2 å‘é‡æ£€ç´¢

```python
class MilvusService:
    """Milvus å‘é‡æ£€ç´¢æœåŠ¡"""

    def __init__(self, collection_name: str):
        connections.connect("default", host="localhost", port=19530)
        self.collection = Collection(collection_name)
        self.collection.load()

    def search(self, query_embedding: list, top_k: int = 10):
        """å‘é‡æ£€ç´¢"""
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"nprobe": 16}},
            limit=top_k,
            output_fields=["entity_id", "text"]
        )

        return [
            {
                "entity_id": hit.entity.get("entity_id"),
                "text": hit.entity.get("text"),
                "score": hit.score
            }
            for hit in results[0]
        ]

    def hybrid_search(self, query_embedding: list, filters: str, top_k: int = 10):
        """æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + æ ‡é‡è¿‡æ»¤ï¼‰"""
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "IP", "params": {"nprobe": 16}},
            limit=top_k,
            expr=filters,  # å¦‚ "category == 'product'"
            output_fields=["entity_id", "text"]
        )
        return results
```

---

## ä¸ƒã€Redis ç¼“å­˜é›†æˆ / Redis Integration

```python
import redis
import json
import hashlib
from typing import Optional, Any

class CacheService:
    """Redis ç¼“å­˜æœåŠ¡"""

    def __init__(self, url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(url)
        self.default_ttl = 3600  # 1å°æ—¶

    def _make_key(self, prefix: str, data: Any) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        data_str = json.dumps(data, sort_keys=True)
        hash_val = hashlib.md5(data_str.encode()).hexdigest()
        return f"{prefix}:{hash_val}"

    def get(self, key: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        data = self.redis.get(key)
        return json.loads(data) if data else None

    def set(self, key: str, value: dict, ttl: int = None):
        """è®¾ç½®ç¼“å­˜"""
        self.redis.setex(key, ttl or self.default_ttl, json.dumps(value))

    def cache_query(self, question: str):
        """é—®ç­”ç»“æœç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                key = self._make_key("qa", question)
                cached = self.get(key)
                if cached:
                    return cached

                result = await func(*args, **kwargs)
                self.set(key, result)
                return result
            return wrapper
        return decorator
```

---

## å…«ã€Kafka æµå¼å¤„ç† / Kafka Integration

```python
from kafka import KafkaProducer, KafkaConsumer
import json

class GraphStreamProcessor:
    """å›¾æ›´æ–°æµå¼å¤„ç†"""

    def __init__(self, bootstrap_servers: str = "localhost:9092"):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.consumer = KafkaConsumer(
            'graph_updates',
            bootstrap_servers=bootstrap_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

    def publish_update(self, update: dict):
        """å‘å¸ƒå›¾æ›´æ–°äº‹ä»¶"""
        self.producer.send('graph_updates', update)
        self.producer.flush()

    def process_updates(self, handler):
        """å¤„ç†å›¾æ›´æ–°äº‹ä»¶"""
        for message in self.consumer:
            update = message.value
            handler(update)

    async def stream_process(self, neo4j_service, gnn_model):
        """æµå¼å¤„ç†å›¾æ›´æ–°å¹¶æ›´æ–°åµŒå…¥"""
        for message in self.consumer:
            update = message.value

            if update['type'] == 'node_add':
                # å¢é‡æ›´æ–°èŠ‚ç‚¹åµŒå…¥
                node_features = torch.tensor(update['features'])
                neighbors = neo4j_service.get_neighbors(update['node_id'])
                new_embedding = gnn_model.incremental_update(
                    node_features, neighbors
                )
                # æ›´æ–° Milvus
                milvus_service.upsert(update['node_id'], new_embedding)

            elif update['type'] == 'edge_add':
                # æ›´æ–°å—å½±å“èŠ‚ç‚¹çš„åµŒå…¥
                affected_nodes = [update['src'], update['dst']]
                for node_id in affected_nodes:
                    # é‡æ–°è®¡ç®—åµŒå…¥
                    pass
```

---

## ä¹ã€å®Œæ•´é›†æˆç¤ºä¾‹ / Complete Integration Example

```python
class AINetworkQASystem:
    """AI ç½‘ç»œé—®ç­”ç³»ç»Ÿå®Œæ•´é›†æˆ"""

    def __init__(self, config: dict):
        # åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡
        self.neo4j = Neo4jService(**config['neo4j'])
        self.milvus = MilvusService(config['milvus']['collection'])
        self.cache = CacheService(config['redis']['url'])
        self.llm = LLMService(config['llm']['model'])

        # åŠ è½½ GNN æ¨¡å‹
        self.gnn = GraphEncoder(**config['gnn'])
        self.gnn.load_state_dict(torch.load(config['gnn']['checkpoint']))

        # å›¾-LLM èåˆ
        self.fusion = GraphLLMFusion(config['llm']['model'])

    async def answer(self, question: str) -> dict:
        """å›ç­”é—®é¢˜"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self.cache._make_key("qa", question)
        cached = self.cache.get(cache_key)
        if cached:
            return cached

        # 2. å®ä½“è¯†åˆ«ä¸å‘é‡æ£€ç´¢
        query_emb = self.llm.encode(question)
        relevant_entities = self.milvus.search(query_emb, top_k=10)

        # 3. å­å›¾æ£€ç´¢
        entity_names = [e['entity_id'] for e in relevant_entities]
        subgraph_data = self.neo4j.get_subgraph(entity_names)

        # 4. GNN ç¼–ç 
        graph_data = self._build_graph_tensor(subgraph_data)
        graph_emb = self.gnn(graph_data.x, graph_data.edge_index)

        # 5. å›¾-æ–‡æœ¬èåˆ
        fused_emb = self.fusion(question, graph_emb)

        # 6. LLM ç”Ÿæˆ
        context = self._format_context(subgraph_data)
        prompt = f"åŸºäºä»¥ä¸‹çŸ¥è¯†å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"
        answer = self.llm.generate(prompt)

        # 7. ç¼“å­˜ç»“æœ
        result = {
            "answer": answer,
            "sources": entity_names,
            "confidence": 0.85
        }
        self.cache.set(cache_key, result)

        return result
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´2æœˆ
**æœ€åæ›´æ–°**: 2025å¹´2æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
