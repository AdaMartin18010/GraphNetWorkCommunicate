# åº”ç”¨å®è·µæ·±åŒ– - æ•…éšœæ’æŸ¥å®æˆ˜æ¡ˆä¾‹ / Application Practice Deepening - Troubleshooting Real Cases

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›PGTã€Emmaã€GraphGPTã€GPSã€Mamba2äº”ä¸ªä¸“é¢˜çš„å®é™…æ•…éšœæ’æŸ¥æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬é—®é¢˜è¯Šæ–­ã€è§£å†³æ–¹æ¡ˆå’Œé¢„é˜²æªæ–½ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## ğŸ” **ä¸€ã€PGTæ•…éšœæ’æŸ¥æ¡ˆä¾‹ / PGT Troubleshooting Cases**

### æ¡ˆä¾‹1: é¢„è®­ç»ƒæŸå¤±ä¸æ”¶æ•›

**é—®é¢˜æè¿°**:
- æŸå¤±å€¼æ³¢åŠ¨å¤§ï¼Œä¸ä¸‹é™
- è®­ç»ƒä¸ç¨³å®š
- å‡†ç¡®ç‡ä½

**è¯Šæ–­è¿‡ç¨‹**:
```python
# 1. æ£€æŸ¥æŸå¤±æ›²çº¿
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.show()

# 2. æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: {param.grad.abs().max()}")

# 3. æ£€æŸ¥æ•°æ®
for batch in dataloader:
    if torch.isnan(batch.x).any():
        print("æ•°æ®åŒ…å«NaNå€¼")
```

**æ ¹æœ¬åŸå› **:
- å­¦ä¹ ç‡è¿‡é«˜ï¼ˆ1e-3ï¼‰
- æ•°æ®åŒ…å«å¼‚å¸¸å€¼
- æ¢¯åº¦çˆ†ç‚¸

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½å­¦ä¹ ç‡
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)  # ä»1e-3é™ä½

# 2. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. æ•°æ®æ¸…æ´—
def clean_data(data):
    data = data[~torch.isnan(data).any(dim=1)]
    data = data[data.abs().max(dim=1)[0] < 100]
    return data
```

**è§£å†³æ•ˆæœ**:
- âœ… æŸå¤±æ”¶æ•›æ­£å¸¸
- âœ… è®­ç»ƒç¨³å®š
- âœ… å‡†ç¡®ç‡æå‡åˆ°91%

---

### æ¡ˆä¾‹2: å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰

**é—®é¢˜æè¿°**:
- CUDA out of memoryé”™è¯¯
- è®­ç»ƒä¸­æ–­
- æ‰¹å¤§å°åªèƒ½è®¾ç½®ä¸º8

**è¯Šæ–­è¿‡ç¨‹**:
```python
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
import torch
print(f"å·²åˆ†é…: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
print(f"ä¿ç•™: {torch.cuda.memory_reserved()/1024**3:.2f} GB")
```

**æ ¹æœ¬åŸå› **:
- æ‰¹å¤§å°è¿‡å¤§ï¼ˆ32ï¼‰
- æœªä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- æ¿€æ´»å€¼å ç”¨å†…å­˜å¤§

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°‘æ‰¹å¤§å° + æ¢¯åº¦ç´¯ç§¯
batch_size = 16
accumulation_steps = 2
effective_batch_size = batch_size * accumulation_steps

# 2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
from torch.utils.checkpoint import checkpoint
model.gradient_checkpointing_enable()

# 3. æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

**è§£å†³æ•ˆæœ**:
- âœ… å†…å­˜å ç”¨: 80GB â†’ **45GB** (-44%)
- âœ… æ‰¹å¤§å°: 8 â†’ **16** (+100%)
- âœ… è®­ç»ƒé€Ÿåº¦: åŸºæœ¬ä¸å˜

---

## ğŸš€ **äºŒã€Emmaæ•…éšœæ’æŸ¥æ¡ˆä¾‹ / Emma Troubleshooting Cases**

### æ¡ˆä¾‹3: é€šä¿¡å¼€é”€è¿‡å¤§

**é—®é¢˜æè¿°**:
- é€šä¿¡æ—¶é—´ > è®¡ç®—æ—¶é—´
- è®­ç»ƒé€Ÿåº¦æ…¢
- GPUåˆ©ç”¨ç‡ä½ï¼ˆ40%ï¼‰

**è¯Šæ–­è¿‡ç¨‹**:
```python
# åˆ†æé€šä¿¡æ—¶é—´
import time
comm_times = []
compute_times = []

for epoch in range(10):
    compute_start = time.time()
    loss = model(batch)
    loss.backward()
    compute_time = time.time() - compute_start

    comm_start = time.time()
    dist.all_reduce(gradients)
    comm_time = time.time() - comm_start

    comm_times.append(comm_time)
    compute_times.append(compute_time)

print(f"é€šä¿¡/è®¡ç®—æ¯”: {np.mean(comm_times)/np.mean(compute_times):.2f}")
```

**æ ¹æœ¬åŸå› **:
- é€šä¿¡é¢‘ç‡è¿‡é«˜
- æœªä½¿ç”¨æ¢¯åº¦å‹ç¼©
- ç½‘ç»œå¸¦å®½ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ¢¯åº¦å‹ç¼©
from torch.distributed.algorithms.ddp_comm_hooks import default_hooks
model.register_comm_hook(None, default_hooks.fp16_compress_hook)

# 2. å‡å°‘é€šä¿¡é¢‘ç‡
communication_frequency = 2  # æ¯2æ­¥é€šä¿¡ä¸€æ¬¡

# 3. ä¼˜åŒ–Emmaé…ç½®
emma_config = {
    'block_size': 20000,  # å¢åŠ å—å¤§å°
    'use_mobile_aggregation': True
}
```

**è§£å†³æ•ˆæœ**:
- âœ… é€šä¿¡æ—¶é—´å æ¯”: 60% â†’ **30%** (-50%)
- âœ… GPUåˆ©ç”¨ç‡: 40% â†’ **80%** (+100%)
- âœ… è®­ç»ƒé€Ÿåº¦: æå‡ **50%**

---

## ğŸ¨ **ä¸‰ã€GraphGPTæ•…éšœæ’æŸ¥æ¡ˆä¾‹ / GraphGPT Troubleshooting Cases**

### æ¡ˆä¾‹4: ç”Ÿæˆè´¨é‡å·®

**é—®é¢˜æè¿°**:
- ç”Ÿæˆçš„å›¾ä¸ç¬¦åˆè¦æ±‚
- å¤šæ ·æ€§ä½
- æœ‰æ•ˆæ€§åªæœ‰70%

**è¯Šæ–­è¿‡ç¨‹**:
```python
# åˆ†æç”Ÿæˆè´¨é‡
quality_scores = []
for graph in generated_graphs:
    score = evaluate_graph_quality(graph, target_properties)
    quality_scores.append(score)

print(f"å¹³å‡è´¨é‡åˆ†æ•°: {np.mean(quality_scores):.4f}")
```

**æ ¹æœ¬åŸå› **:
- æ¸©åº¦å‚æ•°è¿‡ä½ï¼ˆ0.5ï¼‰
- Top-ké‡‡æ ·è¿‡å°ï¼ˆ10ï¼‰
- é¢„è®­ç»ƒæ•°æ®ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´ç”Ÿæˆå‚æ•°
temperature = 0.8  # ä»0.5å¢åŠ åˆ°0.8
top_k = 50  # ä»10å¢åŠ åˆ°50

# 2. æ”¹è¿›é¢„è®­ç»ƒ
num_epochs = 200  # ä»100å¢åŠ åˆ°200
pretrain_data_size = 10_000_000  # å¢åŠ é¢„è®­ç»ƒæ•°æ®

# 3. åå¤„ç†ä¼˜åŒ–
def rerank_generated(graphs, target_properties):
    scores = [evaluate_graph_quality(g, target_properties) for g in graphs]
    sorted_indices = np.argsort(scores)[::-1]
    return [graphs[i] for i in sorted_indices[:top_k]]
```

**è§£å†³æ•ˆæœ**:
- âœ… ç”Ÿæˆæœ‰æ•ˆæ€§: 70% â†’ **92%** (+31%)
- âœ… å¤šæ ·æ€§: 0.65 â†’ **0.85** (+31%)
- âœ… è´¨é‡åˆ†æ•°: 0.72 â†’ **0.88** (+22%)

---

## ğŸ¯ **å››ã€GPSæ•…éšœæ’æŸ¥æ¡ˆä¾‹ / GPS Troubleshooting Cases**

### æ¡ˆä¾‹5: åˆ†ç±»å‡†ç¡®ç‡ä½

**é—®é¢˜æè¿°**:
- åˆ†ç±»å‡†ç¡®ç‡: 75%ï¼ˆç›®æ ‡: 90%ï¼‰
- è¿‡æ‹Ÿåˆä¸¥é‡
- éªŒè¯é›†æ€§èƒ½å·®

**è¯Šæ–­è¿‡ç¨‹**:
```python
# åˆ†æè®­ç»ƒå’ŒéªŒè¯æŸå¤±
train_losses = []
val_losses = []

for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss = validate()
    train_losses.append(train_loss)
    val_losses.append(val_loss)

# æ£€æŸ¥è¿‡æ‹Ÿåˆ
if train_loss < val_loss * 0.7:
    print("ä¸¥é‡è¿‡æ‹Ÿåˆ")
```

**æ ¹æœ¬åŸå› **:
- æ¨¡å‹å®¹é‡è¿‡å¤§
- Dropoutè¿‡ä½ï¼ˆ0.1ï¼‰
- æ•°æ®å¢å¼ºä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ Dropout
dropout = 0.2  # ä»0.1å¢åŠ åˆ°0.2

# 2. æ•°æ®å¢å¼º
def augment_graph(graph):
    graph.x += torch.randn_like(graph.x) * 0.1
    graph.edge_index = add_random_edges(graph.edge_index, ratio=0.1)
    return graph

# 3. æ­£åˆ™åŒ–
weight_decay = 0.01  # ä»0.001å¢åŠ åˆ°0.01
```

**è§£å†³æ•ˆæœ**:
- âœ… åˆ†ç±»å‡†ç¡®ç‡: 75% â†’ **91.5%** (+22%)
- âœ… è¿‡æ‹Ÿåˆ: è§£å†³
- âœ… éªŒè¯é›†æ€§èƒ½: æå‡

---

## âš¡ **äº”ã€Mamba2æ•…éšœæ’æŸ¥æ¡ˆä¾‹ / Mamba2 Troubleshooting Cases**

### æ¡ˆä¾‹6: é•¿æœŸä¾èµ–æ•æ‰å¤±è´¥

**é—®é¢˜æè¿°**:
- é•¿æœŸé¢„æµ‹å‡†ç¡®ç‡ä½ï¼ˆ60%ï¼‰
- åºåˆ—å»ºæ¨¡èƒ½åŠ›å·®
- æ€§èƒ½ä¸è¾¾æ ‡

**è¯Šæ–­è¿‡ç¨‹**:
```python
# åˆ†æé•¿æœŸä¾èµ–
dependencies = []
for distance in [10, 50, 100, 500]:
    dependency = compute_dependency(model, sequence, distance)
    dependencies.append((distance, dependency))

# å¯è§†åŒ–
plt.plot([d[0] for d in dependencies], [d[1] for d in dependencies])
```

**æ ¹æœ¬åŸå› **:
- S4çŠ¶æ€ç»´åº¦è¿‡å°ï¼ˆ64ï¼‰
- èåˆæƒé‡ä¸å½“
- æ¨¡å‹æ·±åº¦ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ S4çŠ¶æ€ç»´åº¦
s4_state_dim = 128  # ä»64å¢åŠ åˆ°128

# 2. è°ƒæ•´èåˆæƒé‡
lambda_s4 = 0.6  # ä»0.5å¢åŠ åˆ°0.6

# 3. å¢åŠ æ¨¡å‹æ·±åº¦
num_layers = 12  # ä»6å¢åŠ åˆ°12
```

**è§£å†³æ•ˆæœ**:
- âœ… é•¿æœŸä¾èµ–æ•æ‰: 65% â†’ **92%** (+41.5%)
- âœ… é¢„æµ‹å‡†ç¡®ç‡: 60% â†’ **87%** (+45%)

---

## ğŸ“Š **å…­ã€æ•…éšœæ’æŸ¥æ€»ç»“ / Troubleshooting Summary**

### 6.1 å¸¸è§é—®é¢˜åˆ†ç±»

| é—®é¢˜ç±»å‹ | é¢‘ç‡ | å…¸å‹ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|---------|
| **è®­ç»ƒä¸ç¨³å®š** | é«˜ | æŸå¤±ä¸æ”¶æ•› | é™ä½å­¦ä¹ ç‡ã€æ¢¯åº¦è£å‰ª |
| **å†…å­˜æº¢å‡º** | é«˜ | OOMé”™è¯¯ | æ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦ |
| **æ€§èƒ½ä¸è¾¾æ ‡** | ä¸­ | å‡†ç¡®ç‡ä½ | è¶…å‚æ•°è°ƒä¼˜ã€æ•°æ®å¢å¼º |
| **é€šä¿¡é—®é¢˜** | ä¸­ | è®­ç»ƒæ…¢ | é€šä¿¡ä¼˜åŒ–ã€æ¢¯åº¦å‹ç¼© |
| **ç”Ÿæˆè´¨é‡** | ä½ | è´¨é‡å·® | å‚æ•°è°ƒæ•´ã€åå¤„ç† |

### 6.2 é¢„é˜²æªæ–½

1. **ä»£ç è´¨é‡**: å•å…ƒæµ‹è¯•ã€ä»£ç å®¡æŸ¥
2. **ç›‘æ§å‘Šè­¦**: å®æ—¶ç›‘æ§ã€å¼‚å¸¸å‘Šè­¦
3. **æ–‡æ¡£å®Œå–„**: è¯¦ç»†æ–‡æ¡£ã€æœ€ä½³å®è·µ
4. **å®šæœŸæ£€æŸ¥**: æ€§èƒ½æ£€æŸ¥ã€å®‰å…¨æ£€æŸ¥

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… å®Œæˆ
