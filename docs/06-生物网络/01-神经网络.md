# 生物网络 - 神经网络

## 1. 神经元模型

### 1.1 神经元基本结构

**定义 1.1** (神经元)
**神经元**是神经系统的基本功能单位，形式化为：
$$\mathcal{N} = \langle D, S, A, T \rangle$$

其中：

- $D$ 是树突 (Dendrites)
- $S$ 是胞体 (Soma)
- $A$ 是轴突 (Axon)
- $T$ 是突触 (Synapses)

**定义 1.2** (神经元状态)
**神经元状态**描述神经元的激活水平：
$$s(t) = f\left(\sum_{i=1}^n w_i x_i(t) + b\right)$$

其中：

- $x_i(t)$ 是第 $i$ 个输入在时间 $t$ 的值
- $w_i$ 是第 $i$ 个输入的权重
- $b$ 是偏置项
- $f$ 是激活函数

### 1.2 激活函数

**定义 1.3** (激活函数)
**激活函数**将神经元的输入映射到输出：
$$f: \mathbb{R} \to \mathbb{R}$$

**定义 1.4** (Sigmoid函数)
**Sigmoid函数**是常用的激活函数：
$$f(x) = \frac{1}{1 + e^{-x}}$$

**性质 1.1** (Sigmoid性质)

- 输出范围：$f(x) \in (0, 1)$
- 单调递增：$f'(x) > 0$
- 对称性：$f(-x) = 1 - f(x)$

**定义 1.5** (ReLU函数)
**ReLU函数**是线性整流函数：
$$f(x) = \max(0, x)$$

**定义 1.6** (Tanh函数)
**Tanh函数**是双曲正切函数：
$$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

## 2. 神经网络结构

### 2.1 前馈神经网络

**定义 2.1** (前馈神经网络)
**前馈神经网络**是信息单向传播的网络：
$$\mathcal{NN} = \langle L, W, B, F \rangle$$

其中：

- $L = \{L_1, L_2, \ldots, L_n\}$ 是层集
- $W = \{W_1, W_2, \ldots, W_{n-1}\}$ 是权重矩阵集
- $B = \{B_1, B_2, \ldots, B_n\}$ 是偏置向量集
- $F = \{F_1, F_2, \ldots, F_n\}$ 是激活函数集

**定义 2.2** (层)
**层**是神经元的集合：
$$L_i = \{n_{i,1}, n_{i,2}, \ldots, n_{i,m_i}\}$$

其中 $m_i$ 是第 $i$ 层的神经元数。

**算法 2.1** (前向传播)

```text
输入：输入数据 x，网络参数 W, B, F
输出：网络输出 y

1. 初始化：a[0] = x
2. 前向传播：for i = 1 to n do
   a. 计算线性组合：z[i] = W[i] * a[i-1] + B[i]
   b. 应用激活函数：a[i] = F[i](z[i])
3. 返回输出：return a[n]
```

### 2.2 循环神经网络

**定义 2.3** (循环神经网络)
**循环神经网络** (RNN) 是具有记忆能力的网络：
$$h_t = f(W_h h_{t-1} + W_x x_t + b)$$

其中：

- $h_t$ 是时间步 $t$ 的隐藏状态
- $x_t$ 是时间步 $t$ 的输入
- $W_h, W_x$ 是权重矩阵
- $b$ 是偏置向量

**定义 2.4** (LSTM)
**长短期记忆网络** (LSTM) 是改进的RNN：
$$\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
h_t &= o_t * \tanh(C_t)
\end{align}$$

其中：
- $f_t$ 是遗忘门
- $i_t$ 是输入门
- $o_t$ 是输出门
- $C_t$ 是细胞状态

## 3. 生物通信机制

### 3.1 突触传递

**定义 3.1** (突触)
**突触**是神经元之间的连接点：
$$\mathcal{S} = \langle P, V, R, T \rangle$$

其中：
- $P$ 是突触前膜
- $V$ 是突触间隙
- $R$ 是突触后膜
- $T$ 是神经递质

**定义 3.2** (突触强度)
**突触强度**决定信号传递的强度：
$$w_{ij} = \alpha \cdot \text{neurotransmitter}_{ij} \cdot \text{receptor}_{ij}$$

其中：
- $\alpha$ 是调节因子
- $\text{neurotransmitter}_{ij}$ 是神经递质浓度
- $\text{receptor}_{ij}$ 是受体密度

### 3.2 动作电位

**定义 3.3** (动作电位)
**动作电位**是神经元的电信号：
$$V(t) = V_{rest} + \Delta V \cdot e^{-t/\tau}$$

其中：
- $V_{rest}$ 是静息电位
- $\Delta V$ 是电位变化幅度
- $\tau$ 是时间常数

**定义 3.4** (霍奇金-赫胥黎模型)
**霍奇金-赫胥黎模型**描述动作电位的产生：
$$C \frac{dV}{dt} = I_{ext} - I_{Na} - I_K - I_L$$

其中：
- $C$ 是膜电容
- $I_{ext}$ 是外部电流
- $I_{Na}, I_K, I_L$ 是离子电流

## 4. 学习机制

### 4.1 赫布学习

**定义 4.1** (赫布学习规则)
**赫布学习规则**描述突触强度的变化：
$$\Delta w_{ij} = \eta \cdot x_i \cdot y_j$$

其中：
- $\Delta w_{ij}$ 是权重变化
- $\eta$ 是学习率
- $x_i$ 是突触前神经元活动
- $y_j$ 是突触后神经元活动

**算法 4.1** (赫布学习算法)
```
输入：神经元对 (i, j)，学习率 η
输出：更新的权重

1. 计算神经元活动：x_i = get_activity(i)
2. 计算神经元活动：y_j = get_activity(j)
3. 更新权重：Δw_ij = η * x_i * y_j
4. 应用更新：w_ij = w_ij + Δw_ij
5. 返回新权重：return w_ij
```

### 4.2 反向传播

**定义 4.2** (反向传播)
**反向传播**是计算梯度的算法：
$$\frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial w_{ij}}$$

**算法 4.2** (反向传播算法)
```
输入：训练数据 (x, y)，网络参数
输出：更新的网络参数

1. 前向传播：compute_forward_pass(x)
2. 计算误差：E = compute_error(y_pred, y)
3. 反向传播：for layer = n downto 1 do
   a. 计算梯度：grad = compute_gradient(layer)
   b. 更新权重：update_weights(layer, grad)
4. 返回更新后的参数：return updated_parameters
```

## 5. 网络拓扑

### 5.1 小世界网络

**定义 5.1** (小世界网络)
**小世界网络**具有高聚类系数和短平均路径长度：
$$\mathcal{SW} = \langle V, E, C, L \rangle$$

其中：
- $C$ 是聚类系数
- $L$ 是平均路径长度

**性质 5.1** (小世界性质)
- 高聚类系数：$C \gg C_{random}$
- 短平均路径长度：$L \approx L_{random}$

**算法 5.1** (Watts-Strogatz模型)
```
输入：节点数 n，平均度 k，重连概率 p
输出：小世界网络

1. 构造规则网络：G = create_regular_graph(n, k)
2. 重连边：for each edge e in G do
   a. 以概率 p 重连：if random() < p then
      i. 选择新端点：new_end = random_node()
      ii. 重连边：rewire_edge(e, new_end)
3. 返回网络：return G
```

### 5.2 无标度网络

**定义 5.2** (无标度网络)
**无标度网络**的度分布遵循幂律分布：
$$P(k) \sim k^{-\gamma}$$

其中 $\gamma$ 是幂律指数。

**算法 5.2** (Barabási-Albert模型)
```
输入：节点数 n，每步添加的边数 m
输出：无标度网络

1. 初始化：G = create_empty_graph()
2. 添加初始节点：add_initial_nodes(m)
3. 增长网络：for i = m+1 to n do
   a. 添加新节点：add_node(i)
   b. 优先连接：for j = 1 to m do
      i. 选择目标节点：target = preferential_attachment()
      ii. 添加边：add_edge(i, target)
4. 返回网络：return G
```

## 6. 同步与振荡

### 6.1 神经元同步

**定义 6.1** (神经元同步)
**神经元同步**是多个神经元同时激活的现象：
$$\text{Sync}(t) = \frac{1}{N} \sum_{i=1}^N \delta(t - t_i)$$

其中 $t_i$ 是第 $i$ 个神经元的激活时间。

**定义 6.2** (Kuramoto模型)
**Kuramoto模型**描述相位同步：
$$\frac{d\theta_i}{dt} = \omega_i + \frac{K}{N} \sum_{j=1}^N \sin(\theta_j - \theta_i)$$

其中：
- $\theta_i$ 是第 $i$ 个神经元的相位
- $\omega_i$ 是自然频率
- $K$ 是耦合强度

### 6.2 振荡网络

**定义 6.3** (振荡网络)
**振荡网络**产生周期性活动模式：
$$x_i(t) = A_i \sin(\omega_i t + \phi_i)$$

**算法 6.1** (振荡网络模拟)
```
输入：网络参数，初始条件
输出：振荡模式

1. 初始化：set_initial_conditions()
2. 时间演化：for t = 0 to T do
   a. 更新相位：for each neuron i do
      i. 计算耦合项：coupling = compute_coupling(i)
      ii. 更新相位：θ_i(t+1) = θ_i(t) + ω_i + coupling
   b. 计算活动：compute_activity(t)
3. 返回振荡模式：return oscillation_pattern
```

## 7. 信息编码

### 7.1 频率编码

**定义 7.1** (频率编码)
**频率编码**通过神经元发放频率传递信息：
$$r = \frac{N_{spikes}}{T}$$

其中：
- $r$ 是发放频率
- $N_{spikes}$ 是发放次数
- $T$ 是时间窗口

**定义 7.2** (时间编码)
**时间编码**通过精确的发放时间传递信息：
$$I = \sum_{i=1}^N \log_2 \frac{1}{P(t_i)}$$

其中 $t_i$ 是第 $i$ 个发放时间。

### 7.2 群体编码

**定义 7.3** (群体编码)
**群体编码**通过多个神经元的联合活动表示信息：
$$P(s|r) = \frac{1}{Z} \exp\left(\sum_i h_i r_i + \sum_{i<j} J_{ij} r_i r_j\right)$$

其中：
- $s$ 是刺激
- $r$ 是神经元响应
- $h_i, J_{ij}$ 是参数

## 8. 可塑性

### 8.1 突触可塑性

**定义 8.1** (长时程增强)
**长时程增强** (LTP) 是突触强度的持久增强：
$$\Delta w = \eta \cdot \text{Ca}^{2+} \cdot f(\text{timing})$$

**定义 8.2** (长时程抑制)
**长时程抑制** (LTD) 是突触强度的持久减弱：
$$\Delta w = -\eta \cdot \text{Ca}^{2+} \cdot f(\text{timing})$$

### 8.2 结构可塑性

**定义 8.3** (结构可塑性)
**结构可塑性**是神经元连接结构的动态变化：
$$\frac{dN_{syn}}{dt} = \alpha \cdot \text{activity} - \beta \cdot N_{syn}$$

其中：
- $N_{syn}$ 是突触数量
- $\alpha, \beta$ 是生长和消除率

## 1.7 生物神经网络的结构化梳理、主要定理、极值、语义模型与自动化验证

### 1.7.1 结构化梳理
- 神经元、突触、连接矩阵、神经回路、功能模块、模体、异质性、分层、动力学、稳态、吸引子等
- 属性：连通性、模体频率、稳态数、鲁棒性、复杂性

### 1.7.2 主要定理与极值

**定理 1.7.2.1（Hopfield网络稳态定理）**
对称权重的Hopfield网络收敛于有限个稳态（吸引子）。

**定理 1.7.2.2（模体极值）**
神经网络中三元环等模体的频率极大化提升信息处理能力。

**定理 1.7.2.3（鲁棒性极值）**
适度异质性与冗余可极大提升神经网络的鲁棒性。

### 1.7.3 形式语义模型
- 神经网络$N=(V,E,W)$，动力学方程$dx/dt=f(x,W)$，模体$M$，属性映射等
- 性质可用微分方程、属性逻辑、一阶/概率逻辑公式表达
- 神经网络算法、模体检测、动力学分析等可形式化为范畴上的函子或逻辑推理过程

### 1.7.4 保持性与极值定理

**定理 1.7.4.1（同态下模体/稳态保持）**
神经网络同态$h:N_1\to N_2$保持模体结构与稳态数不减。

**定理 1.7.4.2（极值保持性）**
最大模体频率、最小稳态数等极值性质在结构保持映射下不减弱。

### 1.7.5 自动化验证建议
- Coq/Lean等定理证明器可形式化神经网络结构、模体、稳态、鲁棒性等定理。
- Rust/Go代码可实现模体检测、动力学仿真、稳态分析与自动化验证。

## 多模态表达与可视化

- **神经网络结构图**：用Cytoscape/NetworkX展示神经元连接模式。
- **动态演化动画**：用biological_network_visualizer.py生成神经网络演化动态图。
- **自动化脚本建议**：
  - `scripts/biological_network_visualizer.py`：输入神经网络数据，输出结构图、动画。
- **示例**：
  - Mermaid神经网络结构：
    ```mermaid
    graph TD;
      Input-->Hidden;
      Hidden-->Output;
    ```

---

*本文档提供了生物神经网络的基础理论和模型，为生物网络通信系统的研究提供了理论基础。*
