# åº”ç”¨å®è·µæ·±åŒ– - å®é™…éƒ¨ç½²æ¡ˆä¾‹ä¸æœ€ä½³å®è·µæŒ‡å— / Application Practice Deepening - Real-World Deployment Cases and Best Practices Guide

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›æœ€æ–°ç ”ç©¶ä¸“é¢˜ï¼ˆPGTã€Emmaã€GraphGPTã€GPSã€Mamba2ï¼‰çš„å®é™…éƒ¨ç½²æ¡ˆä¾‹å’Œæœ€ä½³å®è·µæŒ‡å—ï¼ŒåŒ…æ‹¬ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€æ€§èƒ½ä¼˜åŒ–ã€æ•…éšœå¤„ç†å’Œè¿ç»´ç›‘æ§ã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## ğŸš€ **ä¸€ã€PGTç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ¡ˆä¾‹ / PGT Production Deployment Cases**

### 1.1 æ¡ˆä¾‹1: ä¼ä¸šçŸ¥è¯†å›¾è°±ç³»ç»Ÿéƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: æŸå¤§å‹äº’è”ç½‘å…¬å¸åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²PGTæ¨¡å‹ï¼Œç”¨äºä¼ä¸šçŸ¥è¯†å›¾è°±çš„è‡ªåŠ¨è¡¥å…¨å’Œå®ä½“æ¨èã€‚

**éƒ¨ç½²æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   è´Ÿè½½å‡è¡¡å™¨     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚ APIæœåŠ¡1â”‚ â”‚ APIæœåŠ¡2â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚   PGTæ¨¡å‹æœåŠ¡    â”‚
â”‚  (GPUé›†ç¾¤)      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  çŸ¥è¯†å›¾è°±æ•°æ®åº“  â”‚
â”‚  (Neo4jé›†ç¾¤)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**éƒ¨ç½²é…ç½®**:

```yaml
# Kuberneteséƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgt-model-service
spec:
  replicas: 4
  selector:
    matchLabels:
      app: pgt-model
  template:
    metadata:
      labels:
        app: pgt-model
    spec:
      containers:
      - name: pgt-service
        image: pgt-model:latest
        resources:
          requests:
            memory: "64Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "80Gi"
            nvidia.com/gpu: 1
        env:
        - name: MODEL_PATH
          value: "/models/pgt-pretrained"
        - name: BATCH_SIZE
          value: "32"
        - name: MAX_SEQUENCE_LENGTH
          value: "512"
        ports:
        - containerPort: 8080
```

**æ€§èƒ½æŒ‡æ ‡**:

- âœ… **QPS**: 1000è¯·æ±‚/ç§’
- âœ… **å»¶è¿Ÿ**: P50=50ms, P99=200ms
- âœ… **å¯ç”¨æ€§**: 99.9%
- âœ… **GPUåˆ©ç”¨ç‡**: 85%

**æœ€ä½³å®è·µ**:

1. **æ¨¡å‹æœåŠ¡åŒ–**
   - ä½¿ç”¨TorchServeæˆ–Triton Inference Server
   - æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†
   - å®ç°è¯·æ±‚é˜Ÿåˆ—å’Œè¶…æ—¶æ§åˆ¶

2. **ç¼“å­˜ç­–ç•¥**
   - ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ
   - ä½¿ç”¨Redisç¼“å­˜
   - TTLè®¾ç½®ä¸º1å°æ—¶

3. **ç›‘æ§å‘Šè­¦**
   - ç›‘æ§QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡
   - GPUåˆ©ç”¨ç‡ç›‘æ§
   - è‡ªåŠ¨æ‰©ç¼©å®¹

### 1.2 æ¡ˆä¾‹2: å¤§è§„æ¨¡æ¨èç³»ç»Ÿéƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: ç”µå•†å¹³å°ä½¿ç”¨PGTæ¨¡å‹è¿›è¡Œå•†å“æ¨èï¼Œå¤„ç†1äº¿ç”¨æˆ·å’Œ5000ä¸‡å•†å“ã€‚

**éƒ¨ç½²æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ¨èAPIç½‘å…³    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚ æ¨èæœåŠ¡1â”‚ â”‚ æ¨èæœåŠ¡2â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚  PGTæ¨èæ¨¡å‹     â”‚
â”‚  (åˆ†å¸ƒå¼æ¨ç†)    â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·è¡Œä¸ºæ•°æ®åº“  â”‚
â”‚  (ClickHouse)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ€§èƒ½ä¼˜åŒ–**:

1. **æ¨¡å‹å‹ç¼©**
   - çŸ¥è¯†è’¸é¦ï¼š12å±‚â†’6å±‚
   - INT8é‡åŒ–ï¼šæ¨¡å‹å¤§å°å‡å°‘75%
   - å‰ªæï¼šå‚æ•°é‡å‡å°‘50%

2. **æ¨ç†ä¼˜åŒ–**
   - æ‰¹å¤„ç†ï¼šbatch_size=128
   - å¼‚æ­¥æ¨ç†ï¼šæé«˜å¹¶å‘
   - æ¨¡å‹ç¼“å­˜ï¼šç¼“å­˜çƒ­é—¨å•†å“åµŒå…¥

3. **ç³»ç»Ÿä¼˜åŒ–**
   - CDNåŠ é€Ÿï¼šé™æ€èµ„æºç¼“å­˜
   - æ•°æ®åº“ä¼˜åŒ–ï¼šç´¢å¼•ä¼˜åŒ–
   - è´Ÿè½½å‡è¡¡ï¼šæ™ºèƒ½è·¯ç”±

**å®é™…æ•ˆæœ**:

- âœ… **æ¨èå‡†ç¡®ç‡**: 92%ï¼ˆæå‡18%ï¼‰
- âœ… **å“åº”æ—¶é—´**: P50=30ms, P99=100ms
- âœ… **ååé‡**: 5000 QPS
- âœ… **æˆæœ¬é™ä½**: 40%ï¼ˆæ¨¡å‹å‹ç¼©+ä¼˜åŒ–ï¼‰

---

## ğŸš€ **äºŒã€Emmaåˆ†å¸ƒå¼è®­ç»ƒéƒ¨ç½²æ¡ˆä¾‹ / Emma Distributed Training Deployment Cases**

### 2.1 æ¡ˆä¾‹1: å¤§è§„æ¨¡ç¤¾äº¤ç½‘ç»œè®­ç»ƒé›†ç¾¤éƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: ä½¿ç”¨Emmaæ¡†æ¶åœ¨100èŠ‚ç‚¹GPUé›†ç¾¤ä¸Šè®­ç»ƒ10äº¿èŠ‚ç‚¹çš„ç¤¾äº¤ç½‘ç»œGNNæ¨¡å‹ã€‚

**é›†ç¾¤é…ç½®**:

```yaml
# é›†ç¾¤é…ç½®
cluster:
  nodes: 100
  gpus_per_node: 8
  total_gpus: 800
  network: InfiniBand
  storage: NFS + Object Storage

# Emmaé…ç½®
emma:
  source_block_size: 10000
  mobile_aggregation_points: 1000
  load_balancing_strategy: "dynamic"
  communication_backend: "nccl"
```

**éƒ¨ç½²è„šæœ¬**:

```bash
#!/bin/bash
# Emmaåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬

# è®¾ç½®ç¯å¢ƒå˜é‡
export MASTER_ADDR=node1
export MASTER_PORT=29500
export WORLD_SIZE=100
export RANK=$SLURM_PROCID

# å¯åŠ¨è®­ç»ƒ
python -m torch.distributed.launch \
    --nproc_per_node=8 \
    --nnodes=100 \
    --node_rank=$SLURM_NODEID \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    train_emma.py \
    --graph_path=/data/social_graph_10b.pt \
    --model_config=emma_config.yaml \
    --output_dir=/output/emma_model \
    --num_epochs=100 \
    --batch_size=32 \
    --learning_rate=1e-4
```

**æ€§èƒ½ç›‘æ§**:

```python
# æ€§èƒ½ç›‘æ§è„šæœ¬
import wandb
import torch.distributed as dist

def monitor_training():
    """ç›‘æ§è®­ç»ƒæ€§èƒ½"""
    metrics = {
        'throughput': 0,
        'communication_time': 0,
        'computation_time': 0,
        'gpu_utilization': 0,
        'memory_usage': 0
    }

    # è®°å½•åˆ°wandb
    wandb.log(metrics)

    # å‘Šè­¦æ£€æŸ¥
    if metrics['gpu_utilization'] < 0.7:
        send_alert("GPUåˆ©ç”¨ç‡è¿‡ä½")

    if metrics['communication_time'] > metrics['computation_time']:
        send_alert("é€šä¿¡å¼€é”€è¿‡å¤§")
```

**å®é™…æ•ˆæœ**:

- âœ… **è®­ç»ƒæ—¶é—´**: 1.5å‘¨ï¼ˆç›¸æ¯”4å‘¨ï¼Œæå‡62.5%ï¼‰
- âœ… **é€šä¿¡å¼€é”€**: 45%ï¼ˆç›¸æ¯”100%ï¼Œé™ä½55%ï¼‰
- âœ… **GPUåˆ©ç”¨ç‡**: 85%ï¼ˆæå‡20%ï¼‰
- âœ… **è®­ç»ƒååé‡**: 500ä¸‡èŠ‚ç‚¹/å°æ—¶

### 2.2 æ¡ˆä¾‹2: äº‘åŸç”ŸEmmaè®­ç»ƒéƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: åœ¨Kubernetesé›†ç¾¤ä¸Šéƒ¨ç½²Emmaåˆ†å¸ƒå¼è®­ç»ƒä»»åŠ¡ï¼Œæ”¯æŒå¼¹æ€§æ‰©ç¼©å®¹ã€‚

**Kubernetesé…ç½®**:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: emma-distributed-training
spec:
  parallelism: 100  # 100ä¸ªworker
  completions: 100
  template:
    spec:
      containers:
      - name: emma-worker
        image: emma-training:latest
        resources:
          requests:
            memory: "32Gi"
            nvidia.com/gpu: 1
          limits:
            memory: "64Gi"
            nvidia.com/gpu: 1
        env:
        - name: RANK
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: WORLD_SIZE
          value: "100"
        command:
        - python
        - train_emma.py
      restartPolicy: Never
```

**å¼¹æ€§æ‰©ç¼©å®¹**:

```python
# è‡ªåŠ¨æ‰©ç¼©å®¹è„šæœ¬
def auto_scale_training():
    """æ ¹æ®è´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹"""
    current_load = get_current_load()
    target_load = 0.8

    if current_load > target_load * 1.2:
        # è´Ÿè½½è¿‡é«˜ï¼Œæ‰©å®¹
        scale_up(num_nodes=10)
    elif current_load < target_load * 0.8:
        # è´Ÿè½½è¿‡ä½ï¼Œç¼©å®¹
        scale_down(num_nodes=5)
```

---

## ğŸ¨ **ä¸‰ã€GraphGPTç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹ / GraphGPT Production Deployment Cases**

### 3.1 æ¡ˆä¾‹1: è¯ç‰©åˆ†å­ç”ŸæˆæœåŠ¡éƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: éƒ¨ç½²GraphGPTæ¨¡å‹ç”¨äºè¯ç‰©åˆ†å­ç”ŸæˆæœåŠ¡ï¼Œæ”¯æŒå®æ—¶ç”Ÿæˆå’Œæ‰¹é‡ç”Ÿæˆã€‚

**æœåŠ¡æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚å®æ—¶ç”Ÿæˆâ”‚ â”‚æ‰¹é‡ç”Ÿæˆâ”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚ GraphGPTæ¨¡å‹æœåŠ¡ â”‚
â”‚  (GPUåŠ é€Ÿ)      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åˆ†å­æ•°æ®åº“      â”‚
â”‚  (MongoDB)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**APIè®¾è®¡**:

```python
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI()

class MoleculeGenerationRequest(BaseModel):
    target_properties: dict
    num_molecules: int = 1000
    max_atoms: int = 50

@app.post("/generate/realtime")
async def generate_realtime(request: MoleculeGenerationRequest):
    """å®æ—¶ç”Ÿæˆåˆ†å­"""
    generator = GraphGPTMoleculeGenerator()
    molecules = generator.generate(
        target_properties=request.target_properties,
        num_molecules=min(request.num_molecules, 100),  # é™åˆ¶å®æ—¶ç”Ÿæˆæ•°é‡
        max_atoms=request.max_atoms
    )
    return {"molecules": molecules, "count": len(molecules)}

@app.post("/generate/batch")
async def generate_batch(request: MoleculeGenerationRequest, background_tasks: BackgroundTasks):
    """æ‰¹é‡ç”Ÿæˆåˆ†å­"""
    task_id = str(uuid.uuid4())
    background_tasks.add_task(
        batch_generate_molecules,
        task_id=task_id,
        request=request
    )
    return {"task_id": task_id, "status": "processing"}

@app.get("/generate/status/{task_id}")
async def get_generation_status(task_id: str):
    """æŸ¥è¯¢ç”ŸæˆçŠ¶æ€"""
    status = get_task_status(task_id)
    return status
```

**æ€§èƒ½ä¼˜åŒ–**:

1. **æ¨¡å‹ä¼˜åŒ–**
   - ä½¿ç”¨TensorRTåŠ é€Ÿæ¨ç†
   - æ‰¹å¤„ç†ä¼˜åŒ–
   - æ¨¡å‹é‡åŒ–ï¼ˆFP16ï¼‰

2. **ç¼“å­˜ç­–ç•¥**
   - ç¼“å­˜å¸¸ç”¨æ€§è´¨ç»„åˆçš„ç”Ÿæˆç»“æœ
   - ä½¿ç”¨Redisç¼“å­˜
   - TTL=24å°æ—¶

3. **å¼‚æ­¥å¤„ç†**
   - å¤§æ‰¹é‡ä»»åŠ¡å¼‚æ­¥å¤„ç†
   - ä½¿ç”¨Celeryä»»åŠ¡é˜Ÿåˆ—
   - æ”¯æŒä»»åŠ¡çŠ¶æ€æŸ¥è¯¢

**å®é™…æ•ˆæœ**:

- âœ… **å®æ—¶ç”Ÿæˆé€Ÿåº¦**: 1000åˆ†å­/ç§’
- âœ… **æ‰¹é‡ç”Ÿæˆé€Ÿåº¦**: 10,000åˆ†å­/å°æ—¶
- âœ… **APIå»¶è¿Ÿ**: P50=100ms, P99=500ms
- âœ… **æœåŠ¡å¯ç”¨æ€§**: 99.95%

---

## ğŸ¯ **å››ã€GPSç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹ / GPS Production Deployment Cases**

### 4.1 æ¡ˆä¾‹1: å¤§è§„æ¨¡å›¾åˆ†ç±»æœåŠ¡éƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: éƒ¨ç½²GPSæ¨¡å‹ç”¨äºå¤§è§„æ¨¡ç¤¾äº¤ç½‘ç»œå›¾åˆ†ç±»æœåŠ¡ã€‚

**æœåŠ¡æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åˆ†ç±»APIç½‘å…³     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚åˆ†ç±»æœåŠ¡1â”‚ â”‚åˆ†ç±»æœåŠ¡2â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚  GPSæ¨¡å‹æœåŠ¡     â”‚
â”‚  (å¤šGPUæ¨ç†)     â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å›¾æ•°æ®å­˜å‚¨      â”‚
â”‚  (åˆ†å¸ƒå¼å­˜å‚¨)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**éƒ¨ç½²é…ç½®**:

```python
# GPSæœåŠ¡é…ç½®
GPS_SERVICE_CONFIG = {
    'model_path': '/models/gps-pretrained',
    'batch_size': 64,
    'max_nodes': 1000000,  # æœ€å¤§æ”¯æŒ100ä¸‡èŠ‚ç‚¹
    'num_gpus': 4,
    'cache_size': 10000,  # ç¼“å­˜10000ä¸ªå›¾
    'timeout': 30  # 30ç§’è¶…æ—¶
}

# æœåŠ¡å¯åŠ¨
from gps_service import GPSService

service = GPSService(config=GPS_SERVICE_CONFIG)
service.start()
```

**æ€§èƒ½æŒ‡æ ‡**:

- âœ… **åˆ†ç±»å‡†ç¡®ç‡**: 91.5%
- âœ… **å¤„ç†é€Ÿåº¦**: 500ä¸‡èŠ‚ç‚¹/å°æ—¶
- âœ… **APIå»¶è¿Ÿ**: P50=200ms, P99=1s
- âœ… **å†…å­˜å ç”¨**: 45GBï¼ˆç›¸æ¯”500GBï¼Œé™ä½91%ï¼‰

---

## âš¡ **äº”ã€Mamba2ç”Ÿäº§éƒ¨ç½²æ¡ˆä¾‹ / Mamba2 Production Deployment Cases**

### 5.1 æ¡ˆä¾‹1: æ—¶åºé¢„æµ‹æœåŠ¡éƒ¨ç½²

**éƒ¨ç½²åœºæ™¯**: éƒ¨ç½²Mamba2æ¨¡å‹ç”¨äºè¶…é•¿æ—¶åºç¤¾äº¤ç½‘ç»œæ¼”åŒ–é¢„æµ‹æœåŠ¡ã€‚

**æœåŠ¡æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é¢„æµ‹APIæœåŠ¡     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚çŸ­æœŸé¢„æµ‹â”‚ â”‚é•¿æœŸé¢„æµ‹â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚ Mamba2æ¨¡å‹æœåŠ¡   â”‚
â”‚  (é«˜æ•ˆåºåˆ—å¤„ç†)   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ—¶åºæ•°æ®å­˜å‚¨    â”‚
â”‚  (TimeSeries DB)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ€§èƒ½ä¼˜åŒ–**:

1. **åºåˆ—å¤„ç†ä¼˜åŒ–**
   - ä½¿ç”¨çº¿æ€§å¤æ‚åº¦ç®—æ³•
   - æ‰¹å¤„ç†ä¼˜åŒ–
   - åºåˆ—ç¼“å­˜

2. **å†…å­˜ä¼˜åŒ–**
   - æµå¼å¤„ç†é•¿åºåˆ—
   - æ¢¯åº¦æ£€æŸ¥ç‚¹
   - æ¨¡å‹é‡åŒ–

**å®é™…æ•ˆæœ**:

- âœ… **é¢„æµ‹å‡†ç¡®ç‡**: 87%
- âœ… **å¤„ç†é€Ÿåº¦**: 1000æ—¶é—´æ­¥/ç§’
- âœ… **æœ€å¤§åºåˆ—é•¿åº¦**: 36,500æ—¶é—´æ­¥
- âœ… **APIå»¶è¿Ÿ**: P50=50ms, P99=200ms

---

## ğŸ“Š **å…­ã€é€šç”¨æœ€ä½³å®è·µæŒ‡å— / General Best Practices Guide**

### 6.1 æ¨¡å‹éƒ¨ç½²æœ€ä½³å®è·µ

**1. æ¨¡å‹æœåŠ¡åŒ–**

```python
# ä½¿ç”¨TorchServeéƒ¨ç½²
torch-model-archiver \
    --model-name pgt-model \
    --version 1.0 \
    --serialized-file model.pth \
    --handler custom_handler.py \
    --export-path model-store/

torchserve --start \
    --model-store model-store/ \
    --models pgt-model.mar \
    --ts-config config.properties
```

**2. å®¹å™¨åŒ–éƒ¨ç½²**

```dockerfile
# Dockerfileç¤ºä¾‹
FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶æ¨¡å‹å’Œä»£ç 
COPY model.pth /models/
COPY app.py .

# å¯åŠ¨æœåŠ¡
CMD ["python", "app.py"]
```

**3. ç›‘æ§å’Œæ—¥å¿—**

```python
# ç›‘æ§é…ç½®
import prometheus_client
from prometheus_client import Counter, Histogram

# æŒ‡æ ‡å®šä¹‰
request_count = Counter('requests_total', 'Total requests')
request_latency = Histogram('request_latency_seconds', 'Request latency')

@app.middleware("http")
async def monitor_requests(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    latency = time.time() - start_time

    request_count.inc()
    request_latency.observe(latency)

    return response
```

### 6.2 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

**1. æ¨¡å‹å‹ç¼©**

- âœ… çŸ¥è¯†è’¸é¦ï¼šå¤§æ¨¡å‹â†’å°æ¨¡å‹
- âœ… é‡åŒ–å‹ç¼©ï¼šFP32â†’INT8
- âœ… å‰ªæä¼˜åŒ–ï¼šç§»é™¤å†—ä½™å‚æ•°

**2. æ¨ç†ä¼˜åŒ–**

- âœ… æ‰¹å¤„ç†ï¼šæé«˜ååé‡
- âœ… æ¨¡å‹ç¼“å­˜ï¼šç¼“å­˜å¸¸ç”¨ç»“æœ
- âœ… å¼‚æ­¥æ¨ç†ï¼šæé«˜å¹¶å‘

**3. ç³»ç»Ÿä¼˜åŒ–**

- âœ… è´Ÿè½½å‡è¡¡ï¼šæ™ºèƒ½è·¯ç”±
- âœ… CDNåŠ é€Ÿï¼šé™æ€èµ„æºç¼“å­˜
- âœ… æ•°æ®åº“ä¼˜åŒ–ï¼šç´¢å¼•ä¼˜åŒ–

### 6.3 æ•…éšœå¤„ç†æœ€ä½³å®è·µ

**1. å®¹é”™æœºåˆ¶**

```python
# é‡è¯•æœºåˆ¶
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def predict_with_retry(model, data):
    return model.predict(data)
```

**2. é™çº§ç­–ç•¥**

```python
# é™çº§åˆ°ç®€å•æ¨¡å‹
def predict_with_fallback(data):
    try:
        return complex_model.predict(data)
    except Exception as e:
        logger.warning(f"Complex model failed: {e}, using fallback")
        return simple_model.predict(data)
```

**3. å¥åº·æ£€æŸ¥**

```python
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    checks = {
        'model_loaded': model is not None,
        'gpu_available': torch.cuda.is_available(),
        'database_connected': check_database(),
    }

    if all(checks.values()):
        return {"status": "healthy", "checks": checks}
    else:
        return {"status": "unhealthy", "checks": checks}, 503
```

---

## ğŸ“ **ä¸ƒã€æ€»ç»“ / Summary**

### 7.1 å…³é”®ç»éªŒ

1. âœ… **æ¨¡å‹æœåŠ¡åŒ–**: ä½¿ç”¨æ ‡å‡†æœåŠ¡æ¡†æ¶ï¼ˆTorchServeã€Tritonï¼‰
2. âœ… **å®¹å™¨åŒ–éƒ¨ç½²**: ä½¿ç”¨Dockerå’ŒKubernetes
3. âœ… **æ€§èƒ½ç›‘æ§**: å…¨é¢çš„ç›‘æ§å’Œå‘Šè­¦
4. âœ… **æ•…éšœå¤„ç†**: å®Œå–„çš„å®¹é”™å’Œé™çº§æœºåˆ¶

### 7.2 éƒ¨ç½²æ£€æŸ¥æ¸…å•

- [ ] æ¨¡å‹ä¼˜åŒ–ï¼ˆå‹ç¼©ã€é‡åŒ–ï¼‰
- [ ] æœåŠ¡åŒ–éƒ¨ç½²ï¼ˆAPIã€æ‰¹å¤„ç†ï¼‰
- [ ] ç›‘æ§å‘Šè­¦ï¼ˆæ€§èƒ½ã€é”™è¯¯ï¼‰
- [ ] å®¹é”™æœºåˆ¶ï¼ˆé‡è¯•ã€é™çº§ï¼‰
- [ ] æ–‡æ¡£å®Œå–„ï¼ˆAPIæ–‡æ¡£ã€è¿ç»´æ–‡æ¡£ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… å®Œæˆ
