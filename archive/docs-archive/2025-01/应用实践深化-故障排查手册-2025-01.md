# 应用实践深化 - 故障排查手册 / Application Practice Deepening - Troubleshooting Manual

## 📚 **概述 / Overview**

本文档提供最新研究专题（PGT、NSDI 2025分布式系统、LLM-图融合模型、GPS架构最新进展、Mamba2）的故障排查手册，包括常见问题、诊断方法、解决方案和预防措施。

**创建时间**: 2025年1月
**状态**: ✅ 持续更新中
**优先级**: 🔴 P0 - 极高优先级
**最新更新**: 2025年1月 - 整合NSDI 2025最新系统故障排查

---

## 🔍 **一、PGT故障排查 / PGT Troubleshooting**

### 1.1 常见问题

#### 问题1: 预训练损失不收敛

**症状**:

- 损失值波动大
- 损失值不下降
- 训练不稳定

**诊断方法**:

```python
# 检查损失曲线
import matplotlib.pyplot as plt

def diagnose_training_loss(loss_history):
    """诊断训练损失"""
    plt.plot(loss_history)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Curve')
    plt.show()

    # 检查损失值
    if loss_history[-1] > loss_history[0]:
        print("警告: 损失值上升，可能存在问题")

    # 检查损失波动
    loss_std = np.std(loss_history[-10:])
    if loss_std > 0.1:
        print("警告: 损失波动大，可能学习率过高")
```

**解决方案**:

1. **学习率调整**

   ```python
   # 降低学习率
   optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # 从1e-4降低

   # 使用学习率调度
   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
       optimizer, mode='min', factor=0.5, patience=5
   )
   ```

2. **梯度裁剪**

   ```python
   # 添加梯度裁剪
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

3. **数据检查**

   ```python
   # 检查数据质量
   def check_data_quality(dataloader):
       for batch in dataloader:
           # 检查NaN值
           if torch.isnan(batch.x).any():
               print("警告: 数据包含NaN值")

           # 检查Inf值
           if torch.isinf(batch.x).any():
               print("警告: 数据包含Inf值")

           # 检查数据范围
           if batch.x.abs().max() > 100:
               print("警告: 数据值过大，建议归一化")
   ```

#### 问题2: 内存溢出（OOM）

**症状**:

- CUDA out of memory错误
- 训练中断
- 系统卡死

**诊断方法**:

```python
# 检查内存使用
import torch

def check_memory_usage():
    """检查内存使用"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3  # GB
        print(f"已分配内存: {allocated:.2f} GB")
        print(f"保留内存: {reserved:.2f} GB")

        if allocated > 0.9 * reserved:
            print("警告: 内存使用率过高")
```

**解决方案**:

1. **减少批大小**

   ```python
   # 从32减少到16
   batch_size = 16
   ```

2. **使用梯度累积**

   ```python
   # 梯度累积模拟大批量
   accumulation_steps = 4
   effective_batch_size = batch_size * accumulation_steps
   ```

3. **使用混合精度训练**

   ```python
   from torch.cuda.amp import autocast, GradScaler

   scaler = GradScaler()
   for batch in dataloader:
       with autocast():
           loss = model(batch)
       scaler.scale(loss).backward()
       scaler.step(optimizer)
       scaler.update()
   ```

4. **使用梯度检查点**

   ```python
   # 启用梯度检查点
   model.gradient_checkpointing_enable()
   ```

#### 问题3: 训练速度慢

**症状**:

- 训练时间过长
- GPU利用率低
- 吞吐量低

**诊断方法**:

```python
# 性能分析
import torch.profiler as profiler

def profile_training(model, dataloader):
    """性能分析"""
    with profiler.profile(
        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],
        record_shapes=True,
        profile_memory=True
    ) as prof:
        for batch in dataloader:
            loss = model(batch)
            loss.backward()

    # 生成报告
    prof.export_chrome_trace("trace.json")
    print(prof.key_averages().table(sort_by="cuda_time_total"))
```

**解决方案**:

1. **优化数据加载**

   ```python
   # 使用多进程数据加载
   dataloader = DataLoader(
       dataset,
       batch_size=32,
       num_workers=8,  # 增加worker数量
       pin_memory=True,  # 固定内存
       prefetch_factor=2  # 预取数据
   )
   ```

2. **优化模型**

   ```python
   # 使用torch.compile加速
   model = torch.compile(model)
   ```

3. **批处理优化**

   ```python
   # 使用更大的批大小
   batch_size = 64  # 从32增加到64
   ```

---

## 🚀 **二、Emma故障排查 / Emma Troubleshooting**

### 2.1 常见问题

#### 问题1: 通信开销过大

**症状**:

- 通信时间 > 计算时间
- 训练速度慢
- GPU利用率低

**诊断方法**:

```python
# 通信时间分析
import time

def analyze_communication_time():
    """分析通信时间"""
    comm_times = []
    compute_times = []

    for epoch in range(10):
        # 计算时间
        compute_start = time.time()
        loss = model(batch)
        loss.backward()
        compute_time = time.time() - compute_start

        # 通信时间
        comm_start = time.time()
        dist.all_reduce(gradients)
        comm_time = time.time() - comm_start

        comm_times.append(comm_time)
        compute_times.append(compute_time)

    avg_comm_time = np.mean(comm_times)
    avg_compute_time = np.mean(compute_times)

    print(f"平均通信时间: {avg_comm_time:.4f}s")
    print(f"平均计算时间: {avg_compute_time:.4f}s")
    print(f"通信/计算比: {avg_comm_time/avg_compute_time:.2f}")

    if avg_comm_time > avg_compute_time:
        print("警告: 通信开销过大")
```

**解决方案**:

1. **使用Emma的移动聚合**

   ```python
   # 启用移动聚合
   emma_config = {
       'use_mobile_aggregation': True,
       'aggregation_points': 1000
   }
   ```

2. **优化通信拓扑**

   ```python
   # 使用更高效的通信后端
   dist.init_process_group(backend='nccl')  # 使用NCCL
   ```

3. **梯度压缩**

   ```python
   # 使用梯度压缩
   from torch.distributed.algorithms.ddp_comm_hooks import default_hooks

   model.register_comm_hook(None, default_hooks.fp16_compress_hook)
   ```

#### 问题2: 负载不平衡

**症状**:

- 不同节点训练时间差异大
- 资源利用率不均
- 训练速度慢

**诊断方法**:

```python
# 负载分析
def analyze_load_balance():
    """分析负载平衡"""
    node_times = []

    for node_id in range(num_nodes):
        node_start = time.time()
        # 节点训练
        train_on_node(node_id)
        node_time = time.time() - node_start
        node_times.append(node_time)

    avg_time = np.mean(node_times)
    std_time = np.std(node_times)
    imbalance_ratio = std_time / avg_time

    print(f"平均时间: {avg_time:.4f}s")
    print(f"标准差: {std_time:.4f}s")
    print(f"不平衡比: {imbalance_ratio:.2f}")

    if imbalance_ratio > 0.2:
        print("警告: 负载不平衡")
```

**解决方案**:

1. **使用Emma的负载平衡**

   ```python
   # 启用负载平衡
   emma_config = {
       'use_load_balancing': True,
       'balancing_strategy': 'dynamic'
   }
   ```

2. **数据重分配**

   ```python
   # 根据节点负载重新分配数据
   def rebalance_data(data_shards, node_loads):
       """重新平衡数据"""
       total_load = sum(node_loads)
       avg_load = total_load / len(node_loads)

       # 重新分配
       for i, load in enumerate(node_loads):
           if load > avg_load * 1.2:
               # 转移部分数据
               excess = load - avg_load
               transfer_data(i, excess)
   ```

---

## 🎨 **三、GraphGPT故障排查 / GraphGPT Troubleshooting**

### 3.1 常见问题

#### 问题1: 生成质量差

**症状**:

- 生成的图不符合要求
- 生成多样性低
- 生成速度慢

**诊断方法**:

```python
# 生成质量分析
def analyze_generation_quality(generated_graphs, target_properties):
    """分析生成质量"""
    quality_scores = []

    for graph in generated_graphs:
        # 计算质量分数
        score = evaluate_graph_quality(graph, target_properties)
        quality_scores.append(score)

    avg_score = np.mean(quality_scores)
    print(f"平均质量分数: {avg_score:.4f}")

    if avg_score < 0.7:
        print("警告: 生成质量较差")
```

**解决方案**:

1. **调整生成参数**

   ```python
   # 调整温度参数
   temperature = 0.8  # 从0.5增加到0.8，提高多样性

   # 调整top-k采样
   top_k = 50  # 从10增加到50
   ```

2. **改进预训练**

   ```python
   # 使用更多预训练数据
   pretrain_data_size = 10_000_000  # 增加到1000万

   # 增加预训练轮数
   num_epochs = 200  # 从100增加到200
   ```

3. **后处理优化**

   ```python
   # 使用重排序
   def rerank_generated(graphs, target_properties):
       """重排序生成的图"""
       scores = [evaluate_graph_quality(g, target_properties) for g in graphs]
       sorted_indices = np.argsort(scores)[::-1]
       return [graphs[i] for i in sorted_indices[:top_k]]
   ```

---

## 🎯 **四、GPS故障排查 / GPS Troubleshooting**

### 4.1 常见问题

#### 问题1: 分类准确率低

**症状**:

- 分类准确率低于预期
- 模型性能差
- 过拟合

**诊断方法**:

```python
# 模型性能分析
def analyze_model_performance(model, dataloader):
    """分析模型性能"""
    model.eval()
    predictions = []
    labels = []

    with torch.no_grad():
        for batch in dataloader:
            logits = model(batch)
            pred = logits.argmax(dim=-1)
            predictions.extend(pred.cpu().numpy())
            labels.extend(batch.y.cpu().numpy())

    accuracy = accuracy_score(labels, predictions)
    print(f"准确率: {accuracy:.4f}")

    # 混淆矩阵
    cm = confusion_matrix(labels, predictions)
    print("混淆矩阵:")
    print(cm)
```

**解决方案**:

1. **增加模型容量**

   ```python
   # 增加隐藏维度
   hidden_dim = 512  # 从256增加到512

   # 增加层数
   num_layers = 8  # 从6增加到8
   ```

2. **数据增强**

   ```python
   # 使用数据增强
   def augment_graph(graph):
       """图数据增强"""
       # 节点特征增强
       graph.x += torch.randn_like(graph.x) * 0.1

       # 边增强
       graph.edge_index = add_random_edges(graph.edge_index, ratio=0.1)

       return graph
   ```

3. **正则化**

   ```python
   # 增加dropout
   dropout = 0.2  # 从0.1增加到0.2

   # 增加权重衰减
   weight_decay = 0.01  # 从0.001增加到0.01
   ```

---

## ⚡ **五、Mamba2故障排查 / Mamba2 Troubleshooting**

### 5.1 常见问题

#### 问题1: 长期依赖捕捉失败

**症状**:

- 长期预测准确率低
- 序列建模能力差
- 性能不达标

**诊断方法**:

```python
# 长期依赖分析
def analyze_long_term_dependency(model, sequences):
    """分析长期依赖"""
    dependencies = []

    for seq in sequences:
        # 计算不同距离的依赖强度
        for distance in [10, 50, 100, 500]:
            dependency = compute_dependency(model, seq, distance)
            dependencies.append((distance, dependency))

    # 可视化
    distances = [d[0] for d in dependencies]
    strengths = [d[1] for d in dependencies]
    plt.plot(distances, strengths)
    plt.xlabel('Distance')
    plt.ylabel('Dependency Strength')
    plt.title('Long-term Dependency Analysis')
    plt.show()
```

**解决方案**:

1. **增加S4状态维度**

   ```python
   # 增加状态维度
   s4_state_dim = 128  # 从64增加到128
   ```

2. **调整融合权重**

   ```python
   # 增加S4组件权重
   lambda_s4 = 0.6  # 从0.5增加到0.6
   ```

3. **增加模型深度**

   ```python
   # 增加层数
   num_layers = 12  # 从6增加到12
   ```

---

## 📊 **六、通用故障排查流程 / General Troubleshooting Process**

### 6.1 故障排查流程

```
1. 问题识别
   ├─ 收集错误信息
   ├─ 检查日志
   └─ 复现问题

2. 问题诊断
   ├─ 性能分析
   ├─ 资源检查
   └─ 代码检查

3. 解决方案
   ├─ 快速修复
   ├─ 根本解决
   └─ 预防措施

4. 验证测试
   ├─ 功能测试
   ├─ 性能测试
   └─ 回归测试
```

### 6.2 诊断工具

```python
# 综合诊断工具
class ComprehensiveDiagnostic:
    """综合诊断工具"""

    def __init__(self, model, dataloader):
        self.model = model
        self.dataloader = dataloader

    def run_full_diagnosis(self):
        """运行完整诊断"""
        results = {}

        # 1. 模型检查
        results['model'] = self.check_model()

        # 2. 数据检查
        results['data'] = self.check_data()

        # 3. 训练检查
        results['training'] = self.check_training()

        # 4. 性能检查
        results['performance'] = self.check_performance()

        # 5. 资源检查
        results['resources'] = self.check_resources()

        return results

    def check_model(self):
        """检查模型"""
        issues = []

        # 检查参数
        total_params = sum(p.numel() for p in self.model.parameters())
        if total_params == 0:
            issues.append("模型参数为空")

        # 检查NaN参数
        for name, param in self.model.named_parameters():
            if torch.isnan(param).any():
                issues.append(f"参数 {name} 包含NaN值")

        return issues

    def check_data(self):
        """检查数据"""
        issues = []

        for batch in self.dataloader:
            # 检查NaN值
            if torch.isnan(batch.x).any():
                issues.append("数据包含NaN值")

            # 检查数据范围
            if batch.x.abs().max() > 100:
                issues.append("数据值过大")

        return issues

    def check_training(self):
        """检查训练"""
        issues = []

        # 检查梯度
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    issues.append(f"梯度 {name} 包含NaN值")
                if param.grad.abs().max() > 100:
                    issues.append(f"梯度 {name} 值过大")

        return issues

    def check_performance(self):
        """检查性能"""
        import time

        start_time = time.time()
        for batch in self.dataloader:
            _ = self.model(batch)
        elapsed_time = time.time() - start_time

        throughput = len(self.dataloader) / elapsed_time

        return {
            'throughput': throughput,
            'elapsed_time': elapsed_time
        }

    def check_resources(self):
        """检查资源"""
        import psutil
        import GPUtil

        issues = []

        # CPU检查
        cpu_percent = psutil.cpu_percent(interval=1)
        if cpu_percent > 90:
            issues.append(f"CPU使用率过高: {cpu_percent}%")

        # 内存检查
        memory = psutil.virtual_memory()
        if memory.percent > 90:
            issues.append(f"内存使用率过高: {memory.percent}%")

        # GPU检查
        if torch.cuda.is_available():
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                if gpu.memoryUsed > gpu.memoryTotal * 0.9:
                    issues.append(f"GPU {gpu.id} 内存使用率过高")

        return issues
```

---

## 📝 **七、预防措施 / Preventive Measures**

### 7.1 代码质量

- ✅ 使用类型检查
- ✅ 添加单元测试
- ✅ 代码审查
- ✅ 静态分析

### 7.2 监控告警

- ✅ 性能监控
- ✅ 错误监控
- ✅ 资源监控
- ✅ 自动告警

### 7.3 文档完善

- ✅ API文档
- ✅ 故障排查文档
- ✅ 最佳实践文档
- ✅ 运维手册

---

## 📝 **八、总结 / Summary**

### 8.1 关键要点

1. ✅ **快速诊断**: 使用诊断工具快速定位问题
2. ✅ **系统解决**: 从根本解决问题，不只是临时修复
3. ✅ **预防为主**: 通过监控和检查预防问题
4. ✅ **文档完善**: 记录问题和解决方案

### 8.2 故障排查检查清单

- [ ] 收集错误信息和日志
- [ ] 运行诊断工具
- [ ] 检查模型、数据、训练
- [ ] 检查资源使用
- [ ] 应用解决方案
- [ ] 验证修复效果
- [ ] 更新文档

---

**文档版本**: v2.0
**创建时间**: 2025年1月
**最后更新**: 2025年1月 - 整合NSDI 2025最新系统故障排查
**维护者**: GraphNetWorkCommunicate项目组
**状态**: ✅ 完成
