# åº”ç”¨å®è·µæ·±åŒ– - å®éªŒé…ç½®ä¸è¶…å‚æ•°è°ƒä¼˜æŒ‡å— / Application Practice Deepening - Experimental Configuration and Hyperparameter Tuning Guide

## ğŸ“š **æ¦‚è¿° / Overview**

æœ¬æ–‡æ¡£æä¾›PGTã€Emmaã€GraphGPTã€GPSã€Mamba2äº”ä¸ªä¸“é¢˜çš„å®éªŒé…ç½®å’Œè¶…å‚æ•°è°ƒä¼˜æŒ‡å—ï¼ŒåŒ…æ‹¬é…ç½®æ¨¡æ¿ã€è°ƒä¼˜ç­–ç•¥å’Œæœ€ä½³å®è·µã€‚

**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**çŠ¶æ€**: âœ… æŒç»­æ›´æ–°ä¸­
**ä¼˜å…ˆçº§**: ğŸ”´ P0 - æé«˜ä¼˜å…ˆçº§

---

## âš™ï¸ **ä¸€ã€PGTå®éªŒé…ç½® / PGT Experimental Configuration**

### 1.1 æ ‡å‡†é…ç½®æ¨¡æ¿

```yaml
# PGTæ ‡å‡†é…ç½®
pgt_config:
  # æ¨¡å‹é…ç½®
  model:
    input_dim: 768
    hidden_dim: 768
    num_layers: 12
    num_heads: 12
    ffn_dim: 3072
    dropout: 0.1
    max_nodes: 10000

  # é¢„è®­ç»ƒé…ç½®
  pretraining:
    num_epochs: 100
    batch_size: 32
    learning_rate: 1e-4
    weight_decay: 0.01
    warmup_steps: 10000
    gradient_clip: 1.0

    # é¢„è®­ç»ƒä»»åŠ¡æƒé‡
    loss_weights:
      node_reconstruction: 1.0
      edge_prediction: 0.5
      subgraph_contrastive: 0.3

  # æ•°æ®é…ç½®
  data:
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    num_workers: 8
    pin_memory: true

  # ä¼˜åŒ–å™¨é…ç½®
  optimizer:
    type: AdamW
    lr: 1e-4
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.01

  # å­¦ä¹ ç‡è°ƒåº¦
  scheduler:
    type: LambdaLR
    warmup_steps: 10000
    total_steps: 1000000
```

### 1.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**ç½‘æ ¼æœç´¢é…ç½®**:

```python
# PGTè¶…å‚æ•°æœç´¢ç©ºé—´
pgt_search_space = {
    'hidden_dim': [512, 768, 1024],
    'num_layers': [6, 12, 18],
    'num_heads': [8, 12, 16],
    'dropout': [0.1, 0.2, 0.3],
    'learning_rate': [1e-5, 1e-4, 5e-4],
    'batch_size': [16, 32, 64],
    'weight_decay': [0.001, 0.01, 0.1]
}
```

**è´å¶æ–¯ä¼˜åŒ–é…ç½®**:

```python
from optuna import create_study

study = create_study(
    direction='maximize',
    study_name='pgt_hyperparameter_optimization'
)

def objective(trial):
    config = {
        'hidden_dim': trial.suggest_int('hidden_dim', 512, 1024, step=128),
        'num_layers': trial.suggest_int('num_layers', 6, 18, step=6),
        'num_heads': trial.suggest_int('num_heads', 8, 16, step=4),
        'dropout': trial.suggest_float('dropout', 0.1, 0.3),
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True),
        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),
        'weight_decay': trial.suggest_float('weight_decay', 0.001, 0.1, log=True)
    }

    # è®­ç»ƒå’Œè¯„ä¼°
    model = create_pgt_model(config)
    score = train_and_evaluate(model, config)

    return score

study.optimize(objective, n_trials=100)
```

### 1.3 æœ€ä½³å®è·µé…ç½®

**å¤§è§„æ¨¡é¢„è®­ç»ƒé…ç½®**:

```yaml
pgt_large_scale:
  model:
    hidden_dim: 1024
    num_layers: 18
    num_heads: 16
  pretraining:
    batch_size: 64
    learning_rate: 5e-5
    gradient_accumulation_steps: 4
  data:
    num_workers: 16
```

**å¿«é€Ÿå®éªŒé…ç½®**:

```yaml
pgt_fast:
  model:
    hidden_dim: 512
    num_layers: 6
    num_heads: 8
  pretraining:
    batch_size: 128
    learning_rate: 1e-3
    num_epochs: 10
```

---

## ğŸš€ **äºŒã€Emmaå®éªŒé…ç½® / Emma Experimental Configuration**

### 2.1 æ ‡å‡†é…ç½®æ¨¡æ¿

```yaml
# Emmaæ ‡å‡†é…ç½®
emma_config:
  # æ¨¡å‹é…ç½®
  model:
    gnn_type: GCN
    hidden_dim: 512
    num_layers: 3

  # åˆ†å¸ƒå¼é…ç½®
  distributed:
    num_workers: 64
    backend: nccl
    init_method: env://

  # Emmaç‰¹å®šé…ç½®
  emma:
    block_size: 10000
    use_mobile_aggregation: true
    num_aggregation_points: 1000
    use_load_balancing: true
    balancing_strategy: dynamic

  # è®­ç»ƒé…ç½®
  training:
    num_epochs: 100
    batch_size: 32
    learning_rate: 1e-3
    gradient_clip: 1.0

  # é€šä¿¡é…ç½®
  communication:
    allreduce_backend: nccl
    gradient_compression: false
    communication_frequency: 1
```

### 2.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**å…³é”®è¶…å‚æ•°**:

```python
emma_search_space = {
    'block_size': [5000, 10000, 20000],
    'num_aggregation_points': [500, 1000, 2000],
    'num_workers': [32, 64, 128],
    'learning_rate': [1e-4, 1e-3, 5e-3],
    'batch_size': [16, 32, 64]
}
```

### 2.3 ä¸åŒè§„æ¨¡é…ç½®

**å°è§„æ¨¡ï¼ˆ<1000ä¸‡èŠ‚ç‚¹ï¼‰**:

```yaml
emma_small:
  emma:
    block_size: 5000
    num_aggregation_points: 500
  distributed:
    num_workers: 32
```

**å¤§è§„æ¨¡ï¼ˆ1äº¿+èŠ‚ç‚¹ï¼‰**:

```yaml
emma_large:
  emma:
    block_size: 20000
    num_aggregation_points: 2000
  distributed:
    num_workers: 128
```

---

## ğŸ¨ **ä¸‰ã€GraphGPTå®éªŒé…ç½® / GraphGPT Experimental Configuration**

### 3.1 æ ‡å‡†é…ç½®æ¨¡æ¿

```yaml
# GraphGPTæ ‡å‡†é…ç½®
graphgpt_config:
  # æ¨¡å‹é…ç½®
  model:
    vocab_size: 10000
    hidden_dim: 768
    num_layers: 12
    num_heads: 12
    max_length: 1024

  # åºåˆ—åŒ–é…ç½®
  sequencing:
    method: bfs  # bfs, dfs, random_walk
    start_node: 0

  # ç”Ÿæˆé…ç½®
  generation:
    num_graphs: 1000
    max_length: 100
    temperature: 0.8
    top_k: 50
    top_p: 0.95

  # è®­ç»ƒé…ç½®
  training:
    num_epochs: 100
    batch_size: 32
    learning_rate: 1e-4
    gradient_clip: 1.0
```

### 3.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**ç”Ÿæˆè´¨é‡ä¼˜åŒ–**:

```python
graphgpt_generation_space = {
    'temperature': [0.5, 0.7, 0.8, 0.9, 1.0],
    'top_k': [10, 20, 50, 100],
    'top_p': [0.9, 0.95, 0.99],
    'max_length': [50, 100, 200]
}
```

---

## ğŸ¯ **å››ã€GPSå®éªŒé…ç½® / GPS Experimental Configuration**

### 4.1 æ ‡å‡†é…ç½®æ¨¡æ¿

```yaml
# GPSæ ‡å‡†é…ç½®
gps_config:
  # æ¨¡å‹é…ç½®
  model:
    input_dim: 128
    hidden_dim: 512
    num_layers: 6
    num_heads: 8
    dropout: 0.1

  # èåˆé…ç½®
  fusion:
    lambda_local: 0.5
    lambda_global: 0.5

  # è®­ç»ƒé…ç½®
  training:
    num_epochs: 100
    batch_size: 64
    learning_rate: 1e-3
    weight_decay: 0.0001
```

### 4.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**å¤§è§„æ¨¡å›¾ä¼˜åŒ–**:

```python
gps_large_scale_space = {
    'hidden_dim': [256, 512, 1024],
    'num_layers': [4, 6, 8],
    'num_heads': [4, 8, 16],
    'lambda_local': [0.3, 0.5, 0.7],
    'lambda_global': [0.3, 0.5, 0.7]
}
```

---

## âš¡ **äº”ã€Mamba2å®éªŒé…ç½® / Mamba2 Experimental Configuration**

### 5.1 æ ‡å‡†é…ç½®æ¨¡æ¿

```yaml
# Mamba2æ ‡å‡†é…ç½®
mamba2_config:
  # æ¨¡å‹é…ç½®
  model:
    input_dim: 128
    hidden_dim: 512
    num_layers: 12
    num_heads: 16
    s4_state_dim: 128
    dropout: 0.1

  # æ—¶åºé…ç½®
  temporal:
    max_sequence_length: 36500
    prediction_horizon: 100

  # è®­ç»ƒé…ç½®
  training:
    num_epochs: 100
    batch_size: 32
    learning_rate: 1e-4
    gradient_clip: 1.0
```

### 5.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**è¶…é•¿åºåˆ—ä¼˜åŒ–**:

```python
mamba2_long_sequence_space = {
    's4_state_dim': [64, 128, 256],
    'num_layers': [6, 12, 18],
    'hidden_dim': [256, 512, 1024],
    'learning_rate': [1e-5, 1e-4, 5e-4]
}
```

---

## ğŸ“Š **å…­ã€é€šç”¨è°ƒä¼˜ç­–ç•¥ / General Tuning Strategies**

### 6.1 å­¦ä¹ ç‡è°ƒä¼˜

**å­¦ä¹ ç‡æœç´¢ç­–ç•¥**:

```python
# å­¦ä¹ ç‡èŒƒå›´æµ‹è¯•
learning_rates = [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]

def lr_range_test(model, dataloader, lr_range):
    """å­¦ä¹ ç‡èŒƒå›´æµ‹è¯•"""
    results = []

    for lr in lr_range:
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
        loss = train_one_epoch(model, dataloader, optimizer)
        results.append((lr, loss))

    # æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡ï¼ˆæŸå¤±ä¸‹é™æœ€å¿«ï¼‰
    best_lr = min(results, key=lambda x: x[1])[0]
    return best_lr
```

### 6.2 æ‰¹å¤§å°è°ƒä¼˜

**æ‰¹å¤§å°é€‰æ‹©ç­–ç•¥**:

| æ•°æ®è§„æ¨¡ | æ¨èæ‰¹å¤§å° | åŸå›  |
|---------|-----------|------|
| **<100ä¸‡èŠ‚ç‚¹** | 128 | å……åˆ†åˆ©ç”¨GPU |
| **100ä¸‡-1000ä¸‡èŠ‚ç‚¹** | 64 | å¹³è¡¡å†…å­˜å’Œé€Ÿåº¦ |
| **1000ä¸‡-1äº¿èŠ‚ç‚¹** | 32 | å†…å­˜å—é™ |
| **>1äº¿èŠ‚ç‚¹** | 16 | å†…å­˜æåº¦å—é™ |

### 6.3 Dropoutè°ƒä¼˜

**Dropouté€‰æ‹©ç­–ç•¥**:

```python
# Dropoutè°ƒä¼˜
dropout_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

def tune_dropout(model, dataloader, dropout_values):
    """è°ƒä¼˜Dropout"""
    results = []

    for dropout in dropout_values:
        model.dropout = dropout
        train_loss, val_loss = train_and_evaluate(model, dataloader)
        results.append((dropout, train_loss, val_loss))

    # é€‰æ‹©éªŒè¯æŸå¤±æœ€å°çš„dropout
    best_dropout = min(results, key=lambda x: x[2])[0]
    return best_dropout
```

---

## ğŸ”§ **ä¸ƒã€è‡ªåŠ¨åŒ–è°ƒä¼˜å·¥å…· / Automated Tuning Tools**

### 7.1 Optunaé›†æˆ

```python
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

def create_optuna_study(model_name):
    """åˆ›å»ºOptunaç ”ç©¶"""
    study = optuna.create_study(
        direction='maximize',
        study_name=f'{model_name}_hyperparameter_optimization',
        storage=f'sqlite:///{model_name}_optuna.db',
        load_if_exists=True
    )
    return study

def optimize_hyperparameters(model_name, objective_func, n_trials=100):
    """ä¼˜åŒ–è¶…å‚æ•°"""
    study = create_optuna_study(model_name)
    study.optimize(objective_func, n_trials=n_trials)

    # å¯è§†åŒ–
    plot_optimization_history(study).show()
    plot_param_importances(study).show()

    return study.best_params
```

### 7.2 Ray Tuneé›†æˆ

```python
from ray import tune
from ray.tune.schedulers import ASHAScheduler

def ray_tune_hyperparameters(model_name, config_space):
    """Ray Tuneè¶…å‚æ•°ä¼˜åŒ–"""
    scheduler = ASHAScheduler(metric='accuracy', mode='max')

    analysis = tune.run(
        train_function,
        config=config_space,
        scheduler=scheduler,
        num_samples=100,
        resources_per_trial={'gpu': 1}
    )

    best_config = analysis.get_best_config(metric='accuracy', mode='max')
    return best_config
```

---

## ğŸ“ **å…«ã€é…ç½®ç®¡ç†æœ€ä½³å®è·µ / Configuration Management Best Practices**

### 8.1 é…ç½®æ–‡ä»¶ç»„ç»‡

```
configs/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ pgt_base.yaml
â”‚   â”œâ”€â”€ emma_base.yaml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ pgt_large_scale.yaml
â”‚   â”œâ”€â”€ emma_distributed.yaml
â”‚   â””â”€â”€ ...
â””â”€â”€ hyperparameters/
    â”œâ”€â”€ pgt_search_space.yaml
    â””â”€â”€ ...
```

### 8.2 é…ç½®ç‰ˆæœ¬æ§åˆ¶

```python
import yaml
from pathlib import Path

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    def __init__(self, config_dir='configs'):
        self.config_dir = Path(config_dir)
        self.configs = {}

    def load_config(self, config_name):
        """åŠ è½½é…ç½®"""
        config_path = self.config_dir / f'{config_name}.yaml'
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config

    def save_config(self, config_name, config):
        """ä¿å­˜é…ç½®"""
        config_path = self.config_dir / f'{config_name}.yaml'
        with open(config_path, 'w') as f:
            yaml.dump(config, f)

    def merge_configs(self, base_config, override_config):
        """åˆå¹¶é…ç½®"""
        merged = base_config.copy()
        merged.update(override_config)
        return merged
```

---

## ğŸ“ **ä¹ã€æ€»ç»“ / Summary**

### 9.1 å…³é”®é…ç½®å»ºè®®

1. **å­¦ä¹ ç‡**: ä»1e-4å¼€å§‹ï¼Œæ ¹æ®æŸå¤±æ›²çº¿è°ƒæ•´
2. **æ‰¹å¤§å°**: æ ¹æ®GPUå†…å­˜é€‰æ‹©ï¼Œä¼˜å…ˆé€‰æ‹©è¾ƒå¤§çš„æ‰¹å¤§å°
3. **Dropout**: ä»0.1å¼€å§‹ï¼Œè¿‡æ‹Ÿåˆæ—¶å¢åŠ 
4. **å±‚æ•°**: ä»6å±‚å¼€å§‹ï¼Œæ ¹æ®æ€§èƒ½éœ€æ±‚å¢åŠ 

### 9.2 è°ƒä¼˜æµç¨‹

1. **åŸºçº¿é…ç½®**: ä½¿ç”¨æ ‡å‡†é…ç½®å»ºç«‹åŸºçº¿
2. **å•å‚æ•°è°ƒä¼˜**: é€ä¸ªè°ƒä¼˜å…³é”®å‚æ•°
3. **è”åˆè°ƒä¼˜**: ä½¿ç”¨è‡ªåŠ¨åŒ–å·¥å…·è”åˆè°ƒä¼˜
4. **éªŒè¯**: åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³é…ç½®

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´1æœˆ
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: GraphNetWorkCommunicateé¡¹ç›®ç»„
**çŠ¶æ€**: âœ… å®Œæˆ
